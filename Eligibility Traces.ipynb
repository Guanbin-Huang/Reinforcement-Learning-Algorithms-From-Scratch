{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_world(length, width,path_lenght,holes_number,Random_State):\n",
    "    \n",
    "    random.seed(Random_State)\n",
    "    #store all cells in a list\n",
    "    Grid_Cells = []\n",
    "    for row in range(length):\n",
    "        for col in range(width):\n",
    "            Grid_Cells.append([row,col])\n",
    "\n",
    "\n",
    "    #specify the number of holes in the gridworld\n",
    "    \n",
    "    #specify the start point as a random cell\n",
    "    start = [random.randint(0, length), random.randint(0, width)]\n",
    "\n",
    "    #create a path from start point\n",
    "    \"\"\"instead of defining start and goal points,\n",
    "      we define just a start point and a random path with a random lenght to\n",
    "       another point and name it as goal point\"\"\"\n",
    "    \n",
    "    def random_path(Start, Path_Lenght,length, width):\n",
    "        \n",
    "        Path = []\n",
    "        Path.append(Start)\n",
    "        for i in range(Path_Lenght):\n",
    "            \n",
    "            #there are two moves that take us on a random cell named Goal [1,0], [0,1]\n",
    "            \n",
    "            move = random.choice([[1,0], [0,1]])\n",
    "            \n",
    "            #update the start cell/point by the above move\n",
    "            Start = [x + y for x, y in zip(Start, move)]\n",
    "            \n",
    "            #if the movement take us out of our gridworld, we reverse the change in the start point\n",
    "            if Start[0] < 0 or Start[1] < 0 or Start[0] > length-1 or Start[1] > width-1:\n",
    "\n",
    "                Start = [x - y for x, y in zip(Start, move)]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                #create a path history\n",
    "                Path.append(Start)\n",
    "\n",
    "        Goal = Start\n",
    "\n",
    "        return Goal,Path\n",
    "    \n",
    "\n",
    "    GoalPath = random_path(start, path_lenght,length, width)\n",
    "\n",
    "    goal = GoalPath[0]\n",
    "    path = GoalPath[1]\n",
    "\n",
    "    #now we must eliminate the path cells from the Grid_Cells to choose hole cells from remaining cells\n",
    "\n",
    "    FreeCells = [x for x in Grid_Cells if x not in path]\n",
    "\n",
    "    Holes = random.sample(FreeCells, holes_number)\n",
    "\n",
    "    #Also, we can visualize our gridworld in a simple way\n",
    "\n",
    "    def mark_holes(holes):\n",
    "        marked_data = [[\"Hole\" if [row, col] in holes else [row, col] for col in range(width)] for row in range(length)]\n",
    "        return marked_data\n",
    "    \n",
    "    marked_matrix = mark_holes(Holes)\n",
    "\n",
    "    print(tabulate(marked_matrix, tablefmt=\"grid\"))\n",
    "\n",
    "    \n",
    "    return length, width, start, goal, Holes, path,Grid_Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+\n",
      "| Hole   | [0, 1] | [0, 2] | [0, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [1, 0] | [1, 1] | [1, 2] | [1, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [2, 1] | [2, 2] | [2, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [3, 1] | Hole   | [3, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [4, 0] | [4, 1] | [4, 2] | [4, 3] |\n",
      "+--------+--------+--------+--------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 4,\n",
       " [1, 2],\n",
       " [4, 3],\n",
       " [[2, 0], [3, 2], [3, 0], [0, 0]],\n",
       " [[1, 2], [1, 3], [2, 3], [3, 3], [4, 3]],\n",
       " [[0, 0],\n",
       "  [0, 1],\n",
       "  [0, 2],\n",
       "  [0, 3],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [3, 0],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [4, 0],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#environment = generate_grid_world(50, 40,1300,400,39)\n",
    "environment = generate_grid_world(5, 4,4,4,39)\n",
    "\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_distribution(grid_size,randomness):\n",
    "    #random.seed(40)\n",
    "    \n",
    "    #by this function we generate probabilities which their sum is equal to 1\n",
    "    def generate_probabilities(n):\n",
    "\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "        \n",
    "        return scaled_numbers\n",
    "    \n",
    "    cells_prob = {}\n",
    "    if randomness == 'stochastic':\n",
    "        for cell in range(grid_size):\n",
    "            \n",
    "            #we set the number of probs to 4 due to 4 possible action for each cell (go to its neighbors)\n",
    "            probs = generate_probabilities(4)\n",
    "\n",
    "            cells_prob[cell] = probs\n",
    "    elif randomness == 'equal probable':\n",
    "\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.25,0.25,0.25,0.25]\n",
    "    \n",
    "    elif randomness == 'deterministic':\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.03,0.06,0.01,0.9] #[0,0,0,1] ##[0.15,.15,0.1,0.6]\n",
    "\n",
    "\n",
    "    #Note that we consider the correspondence between probabilities and actions as below:\n",
    "    #probs = [p1, p2, p3, p4] ---> [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    return cells_prob\n",
    "\n",
    "def neighbor_cells(cell):\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    Neighbors = []\n",
    "    Actions_Neighbors = []\n",
    "    for action in Actions:\n",
    "\n",
    "        neighbor = [x + y for x, y in zip(cell, action)]\n",
    "        #if neighbor not in environment[4]:\n",
    "        Neighbors.append(neighbor)\n",
    "        Actions_Neighbors.append(action)\n",
    "\n",
    "    return Neighbors, Actions_Neighbors\n",
    "\n",
    "def arbitrary_policy(randomness):\n",
    "\n",
    "        #random.seed(randomness)\n",
    "        \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            neighbors = neighbor_cells(state)[0]\n",
    "            Actions_Neighbors = neighbor_cells(state)[1]\n",
    "\n",
    "            allowed_positions = []\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                \n",
    "                if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                    \n",
    "                    allowed_positions.append(neighbor)\n",
    "            \n",
    "            if len(allowed_positions) > 0:\n",
    "                \n",
    "                next_state = random.choice(allowed_positions)\n",
    "                row = next_state[0] - state[0]\n",
    "                col = next_state[1] - state[1]\n",
    "                PolicyAction = [row, col]\n",
    "\n",
    "                policy['{}'.format(state)] = next_state\n",
    "                policy_action['{}'.format(state)] = PolicyAction\n",
    "\n",
    "\n",
    "\n",
    "    return policy, policy_action\n",
    "\n",
    "def state_reward(next_state):\n",
    "\n",
    "    if next_state in environment[4]:\n",
    "\n",
    "        r = -3\n",
    "    \n",
    "    elif next_state == environment[3]:\n",
    "\n",
    "        r = 10\n",
    "    \n",
    "    elif next_state not in environment[6]:\n",
    "\n",
    "        r = -2\n",
    "    \n",
    "    else:\n",
    "\n",
    "        r = -1\n",
    "    \n",
    "    return r\n",
    "\n",
    "def reverse_dictionary(dict):\n",
    "    reverse_dict = {}\n",
    "    for key in list(dict.keys()):\n",
    "        val = dict[key]\n",
    "        reverse_dict[val] = key\n",
    "    return reverse_dict\n",
    "\n",
    "\n",
    "state_indice_dict = {}\n",
    "counter = 0\n",
    "for state in environment[6]:\n",
    "\n",
    "    state = str(state)\n",
    "    state_indice_dict[state] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "def generate_trajectory(policy,randomness,environment_stochasticity):\n",
    "\n",
    "    policy_action = policy[1]\n",
    "    probs = probability_distribution(environment[0]*environment[1],environment_stochasticity)\n",
    "    start = environment[2]\n",
    "    terminate = start\n",
    "    trajectory = []\n",
    "    pure_trajectory = [start]\n",
    "    c = 0\n",
    "    while terminate != environment[3]:\n",
    "        random.seed(randomness+c)\n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        action = policy_action[str(terminate)]\n",
    "        Actions.remove(action)\n",
    "        sorted_actions = Actions + [action]\n",
    "        state_indice = state_indice_dict[str(terminate)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "\n",
    "        selected_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        current_state = terminate\n",
    "        next_state = [x + y for x, y in zip(terminate, selected_action)]\n",
    "        pure_trajectory.append(next_state)\n",
    "        \n",
    "        #if the agent goes out of the gridworld, it stays in its current state\n",
    "        if next_state not in environment[6]:\n",
    "            next_state = terminate\n",
    "        \n",
    "        #if it drops into the holes, it goes to the start points\n",
    "        elif next_state in environment[4]:\n",
    "            next_state = start  \n",
    "\n",
    "        terminate = next_state\n",
    "        trajectory.append((current_state))\n",
    "        c = c+1\n",
    "    \n",
    "    trajectory.append((environment[3]))\n",
    "    pure_trajectory.append(environment[3])\n",
    "\n",
    "    return trajectory,pure_trajectory\n",
    "\n",
    "def extract_features(state):\n",
    "\n",
    "    goal = environment[3]\n",
    "    max_length = environment[0]\n",
    "    max_width = environment[1]\n",
    "\n",
    "    w1 = (goal[0] - state[0]) / max_width\n",
    "    w2 = (goal[1] - state[1]) / max_length\n",
    "\n",
    "    return abs(w1), abs(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-gradient $TD(\\lambda)$ for Estimating $\\hat{v} \\approx v_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_gradient_TD_lambda(num_trials, gamma, policy, Lambda, alpha, environment_stochasticity):\n",
    "\n",
    "    W = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            W[str(state)] = {}\n",
    "\n",
    "            Features = extract_features(state)\n",
    "\n",
    "            for element in [0,1]:\n",
    "                \n",
    "                W[str(state)][element] = Features[element] + random.uniform(1e-9, 1e-8)\n",
    "\n",
    "\n",
    "    V = {}\n",
    "    state_observed = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            V[str(state)] = 0\n",
    "            state_observed[str(state)] = 0\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        TRAJECTORY = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "        \n",
    "\n",
    "        trajectory = TRAJECTORY[0]\n",
    "\n",
    "        z1 , z2 = 0 , 0\n",
    "\n",
    "        for step_indx in range(len(trajectory[:-1])):\n",
    "\n",
    "            step = trajectory[step_indx]\n",
    "            next_step = trajectory[step_indx+1]\n",
    "\n",
    "            r = state_reward(next_step)\n",
    "            #print(W[str(step)][0],W[str(step)][1])\n",
    "            gradient_w1 = -np.sin(abs(W[str(step)][0]) + abs(W[str(step)][1])) * (W[str(step)][0]/abs(W[str(step)][0]))\n",
    "            gradient_w2 = -np.sin(abs(W[str(step)][1]) + abs(W[str(step)][0])) * (W[str(step)][1]/abs(W[str(step)][1]))\n",
    "            \n",
    "            z1 = gamma * Lambda * z1 + gradient_w1\n",
    "            z2 = gamma * Lambda * z2 + gradient_w2\n",
    "\n",
    "            delta = r + gamma * np.cos(abs(W[str(next_step)][0]) + abs(W[str(next_step)][1]))\\\n",
    "                         - np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "\n",
    "            #alpha = 1/math.log(step_indx+2)\n",
    "            W[str(step)][0] = W[str(step)][0] + alpha * delta * z1\n",
    "\n",
    "            W[str(step)][1] = W[str(step)][1] + alpha * delta * z2\n",
    "\n",
    "            \n",
    "            V[str(step)] = V[str(step)] + np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "            state_observed[str(step)] = state_observed[str(step)] + 1\n",
    "    \n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            if state_observed[str(state)] > 0:\n",
    "\n",
    "                V[str(state)] = V[str(state)] / state_observed[str(state)]\n",
    "\n",
    "\n",
    "    return V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_0 = arbitrary_policy(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:24<00:00, 400.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': -0.9999979959477547,\n",
       " '[0, 2]': -0.9999943864821053,\n",
       " '[0, 3]': -0.9998482418602734,\n",
       " '[1, 0]': -0.9994320720238307,\n",
       " '[1, 1]': -0.9998064299228693,\n",
       " '[1, 2]': -0.9999734493958976,\n",
       " '[1, 3]': -0.9999874784926045,\n",
       " '[2, 1]': -0.9998100019500074,\n",
       " '[2, 2]': -0.9995345080561471,\n",
       " '[2, 3]': -0.9998861917275234,\n",
       " '[3, 1]': -0.9997765349035409,\n",
       " '[3, 3]': -0.9994380990624914,\n",
       " '[4, 0]': -0.997328139417871,\n",
       " '[4, 1]': -0.9979638265415661,\n",
       " '[4, 2]': -0.10095922794665614,\n",
       " '[4, 3]': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_gradient_TD_lambda(10000, 0.9, policy_0, 0.5, 0.2, 'deterministic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True online $TD(\\lambda)$ for Estimating $W^T X \\approx v_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_online_TD_lambda(num_trials, gamma, policy, Lambda, alpha, environment_stochasticity):\n",
    "\n",
    "    W = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Features = extract_features(state)\n",
    "            \n",
    "            W[str(state)] = np.array([0, 0])\n",
    "\n",
    "    V = {}\n",
    "    state_observed = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            V[str(state)] = 0\n",
    "            state_observed[str(state)] = 0\n",
    "    \n",
    "\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        TRAJECTORY = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "\n",
    "        trajectory = TRAJECTORY[0]\n",
    "        z = np.array([0,0])\n",
    "        v_old = 0\n",
    "\n",
    "        for step_indx in range(len(trajectory[:-1])):\n",
    "\n",
    "            step = trajectory[step_indx]\n",
    "            next_step = trajectory[step_indx+1]\n",
    "\n",
    "            r = state_reward(next_step)\n",
    "\n",
    "            x_features = np.array([extract_features(step)[0],extract_features(step)[1]])\n",
    "            xprim_features = np.array([extract_features(next_step)[0],extract_features(next_step)[1]])\n",
    "\n",
    "            v = np.matmul(np.transpose(W[str(step)]) , x_features)\n",
    "            v_prime = np.matmul(np.transpose(W[str(step)]) , xprim_features)\n",
    "\n",
    "            delta = r + gamma * v_prime - v \n",
    "            #print(gamma * Lambda * z)\n",
    "            #print(np.matmul(z , x_features))\n",
    "            #print( np.matmul(1 - alpha * gamma * Lambda * np.matmul(np.transpose(z) , x_features) , x_features))\n",
    "            z = gamma * Lambda * z +\\\n",
    "                  (1 - alpha * gamma * Lambda * np.matmul(np.transpose(z) , x_features)) * x_features\n",
    "            \n",
    "            W[str(step)] = W[str(step)] +\\\n",
    "                            alpha * (delta + v - v_old) * z - alpha * (v - v_old) * x_features\n",
    "            #print(W[str(step)])\n",
    "            v_old = v_prime\n",
    "            V[str(step)] = V[str(step)] + np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "            state_observed[str(step)] = state_observed[str(step)] + 1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return V,W\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 106.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': 9.43536194556366,\n",
       "  '[0, 2]': 28.276216443317836,\n",
       "  '[0, 3]': 13.634662721241174,\n",
       "  '[1, 0]': 36.79849297251622,\n",
       "  '[1, 1]': -433.0221765091339,\n",
       "  '[1, 2]': 1.1925062965601612,\n",
       "  '[1, 3]': -28.51072569144077,\n",
       "  '[2, 1]': 390.2462587758875,\n",
       "  '[2, 2]': 72.00122084088336,\n",
       "  '[2, 3]': 41.42228789851967,\n",
       "  '[3, 1]': -108.14759638409062,\n",
       "  '[3, 3]': 1.999935447527856,\n",
       "  '[4, 0]': -1608.9836268413972,\n",
       "  '[4, 1]': 219.91119694671116,\n",
       "  '[4, 2]': 472.81097515783796,\n",
       "  '[4, 3]': 0},\n",
       " {'[0, 1]': array([-6.79903526e+09, -1.26539568e+09]),\n",
       "  '[0, 2]': array([1.25862686e+12, 3.67348767e+11]),\n",
       "  '[0, 3]': array([-1.46972616e+09, -3.03282254e+08]),\n",
       "  '[1, 0]': array([-6.06875029e+09, -3.16127388e+09]),\n",
       "  '[1, 1]': array([7.85841292e+12, 1.76526918e+12]),\n",
       "  '[1, 2]': array([-1.10314764e+14, -1.45604276e+15]),\n",
       "  '[1, 3]': array([9.21434942e+10, 2.71493775e+10]),\n",
       "  '[2, 1]': array([-7.43474150e+10, -3.66317043e+10]),\n",
       "  '[2, 2]': array([531.02895986,  -1.26522396]),\n",
       "  '[2, 3]': array([-39198291.16052371,   -271414.3186052 ]),\n",
       "  '[3, 1]': array([-8.42470280e+14,  5.96204612e+16]),\n",
       "  '[3, 3]': array([-1.01445647e-02, -3.30491145e-08]),\n",
       "  '[4, 0]': array([3.06315024e+15, 7.50175570e+17]),\n",
       "  '[4, 1]': array([-1.06693334e+17, -2.30093129e+19]),\n",
       "  '[4, 2]': array([4.45161255e+13, 6.55503625e+16]),\n",
       "  '[4, 3]': array([0, 0])})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_online_TD_lambda(500, 0.9, policy_0, 0.03, 0.02, 'deterministic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa($ \\lambda $) with binary features and linear function approximation for estimating $W^T X \\approx q_{\\pi}$ or $q_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_action_nextstate(Q,current_state,epsilon,environment_stochasticity):\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "    #print(probs)\n",
    "\n",
    "    if type(current_state) == str:\n",
    "\n",
    "        state = ast.literal_eval(current_state)\n",
    "    else:\n",
    "        state = current_state\n",
    "    #Choose action using policy derived from Q===================================\n",
    "    value_action_state = reverse_dictionary(Q[str(state)])\n",
    "    Max_val = max(list(value_action_state.keys()))\n",
    "    best_action = value_action_state[Max_val]\n",
    "    best_action = ast.literal_eval(best_action)\n",
    "\n",
    "    #============================================================================\n",
    "    #Epsilon Greedy\n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "        selected_action = best_action\n",
    "    \n",
    "    else:\n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        Actions.remove(best_action)\n",
    "        epsilon_action = random.choice(Actions)\n",
    "\n",
    "        selected_action = epsilon_action\n",
    "    #============================================================================\n",
    "    \n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "    Actions.remove(selected_action)\n",
    "    sorted_actions = Actions + [selected_action]\n",
    "    state_indice = state_indice_dict[str(state)]\n",
    "    #print(probs)\n",
    "    #print(state_indice)\n",
    "    actions_prob = probs[state_indice]\n",
    "    actions_prob.sort()\n",
    "    #due to stochasticity of the environment\n",
    "    Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "    #print(type(state), type(Final_action))\n",
    "    \n",
    "    next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "    if next_state not in environment[6] or next_state in environment[4]:\n",
    "\n",
    "        next_state = current_state\n",
    "    \n",
    "    value_action_state = reverse_dictionary(Q[str(next_state)])\n",
    "    #max Q(s',s)\n",
    "    Max_q_val = max(list(value_action_state.keys()))\n",
    "    best_action = value_action_state[Max_q_val]\n",
    "    best_action = ast.literal_eval(best_action)\n",
    "\n",
    "    return Final_action, next_state, Max_q_val\n",
    "    \n",
    "def active_features(state):\n",
    "\n",
    "    actives = []\n",
    "\n",
    "    feature = extract_features(state)\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        if feature[i] != environment[3][i]:\n",
    "\n",
    "            actives.append(i)\n",
    "    \n",
    "    return actives\n",
    "\n",
    "def sarsa_lambda(num_trials, gamma, Lambda, alpha, environment_stochasticity,epsilon):\n",
    "\n",
    "    W  = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Features = extract_features(state)\n",
    "            W[str(state)] =  {}\n",
    "            \n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                W[str(state)][action] = [Features[0] + random.uniform(1e-9, 1e-8),Features[1] + random.uniform(1e-9, 1e-8)]\n",
    "    \n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        Z = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                Z[str(state)] = {}\n",
    "                \n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    Z[str(state)][action] = [random.uniform(1e-9, 1e-8),random.uniform(1e-9, 1e-8)]\n",
    "\n",
    "\n",
    "        current_state = environment[2] #start\n",
    "\n",
    "        while current_state != environment[3]:\n",
    "\n",
    "            epsilon_policy = state_action_nextstate(Q,current_state,epsilon,environment_stochasticity)\n",
    "            action = epsilon_policy[0]\n",
    "            next_state = epsilon_policy[1]\n",
    "            \n",
    "            r = state_reward(next_state)\n",
    "            delta = r\n",
    "\n",
    "            ActiveFeatures = active_features(current_state)\n",
    "\n",
    "            for i in ActiveFeatures:\n",
    "\n",
    "                delta = delta - W[str(current_state)][str(action)][i]\n",
    "                Z[str(current_state)][str(action)][i] = Z[str(current_state)][str(action)][i] + 1\n",
    "            \n",
    "            if next_state == environment[3]:\n",
    "\n",
    "                for i in range(2):\n",
    "\n",
    "                    W[str(current_state)][str(action)][i] = W[str(current_state)][str(action)][i] +\\\n",
    "                    alpha * alpha * Z[str(current_state)][str(action)][i]\n",
    "            \n",
    "\n",
    "            NextActiveFeatures = active_features(next_state)\n",
    "\n",
    "            for i in NextActiveFeatures:\n",
    "\n",
    "                delta = delta - gamma * W[str(next_state)][str(action)][i]\n",
    "\n",
    "                #print(i,type(W[str(next_state)][i]), type(Z[str(next_state)][i]))\n",
    "                #print( alpha * delta * Z[str(next_state)][i])\n",
    "            for i in NextActiveFeatures:\n",
    "\n",
    "                W[str(next_state)][str(action)][i] = W[str(next_state)][str(action)][i] + alpha * delta * Z[str(next_state)][str(action)][i]\n",
    "\n",
    "                Z[str(next_state)][str(action)][i] = gamma * Lambda * Z[str(next_state)][str(action)][i]\n",
    "\n",
    "        \n",
    "            Q[str(current_state)][str(action)] = np.cos(abs(W[str(current_state)][str(action)][0]) +\\\n",
    "                                                abs(W[str(current_state)][str(action)][1]))\n",
    "            \n",
    "            current_state = next_state\n",
    "    \n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [00:21<32:46,  1.99s/it]C:\\Users\\Taha\\AppData\\Local\\Temp\\ipykernel_12408\\4065402722.py:155: RuntimeWarning: invalid value encountered in cos\n",
      "  Q[str(current_state)][str(action)] = np.cos(abs(W[str(current_state)][str(action)][0]) +\\\n",
      "100%|██████████| 1000/1000 [24:25<00:00,  1.47s/it] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': 0.16996713642209252,\n",
       "  '[-1, 0]': nan,\n",
       "  '[0, 1]': 0.16996713184861328,\n",
       "  '[0, -1]': 0.825335615063446},\n",
       " '[0, 2]': {'[1, 0]': 0.36235774007940674,\n",
       "  '[-1, 0]': nan,\n",
       "  '[0, 1]': -0.8893265739110678,\n",
       "  '[0, -1]': -0.6063209331792516},\n",
       " '[0, 3]': {'[1, 0]': 0.5403023007241335,\n",
       "  '[-1, 0]': 0.693904951076342,\n",
       "  '[0, 1]': 0.5403022785946978,\n",
       "  '[0, -1]': 0.5403022955776547},\n",
       " '[1, 0]': {'[1, 0]': 0.8646637019492132,\n",
       "  '[-1, 0]': 0.8646637019492132,\n",
       "  '[0, 1]': 0.21900667727102666,\n",
       "  '[0, -1]': -0.3045487830204501},\n",
       " '[1, 1]': {'[1, 0]': -0.8893265715530619,\n",
       "  '[-1, 0]': nan,\n",
       "  '[0, 1]': -0.862563540369753,\n",
       "  '[0, -1]': -0.9710976940697086},\n",
       " '[1, 2]': {'[1, 0]': -0.766676259804487,\n",
       "  '[-1, 0]': nan,\n",
       "  '[0, 1]': -0.6191603902293098,\n",
       "  '[0, -1]': -0.3650142742096699},\n",
       " '[1, 3]': {'[1, 0]': -0.6063209277752457,\n",
       "  '[-1, 0]': 0.36535032369881004,\n",
       "  '[0, 1]': 0.08772729918717495,\n",
       "  '[0, -1]': 0.7316888581537256},\n",
       " '[2, 1]': {'[1, 0]': -0.27736992032089147,\n",
       "  '[-1, 0]': 0.8154535672410038,\n",
       "  '[0, 1]': 0.6216099567374571,\n",
       "  '[0, -1]': 0.864653941405023},\n",
       " '[2, 2]': {'[1, 0]': 0.864647283923992,\n",
       "  '[-1, 0]': 0.7648421843729045,\n",
       "  '[0, 1]': -0.5144059734456348,\n",
       "  '[0, -1]': -0.09572355868591283},\n",
       " '[2, 3]': {'[1, 0]': 0.21116983345009904,\n",
       "  '[-1, 0]': 0.1809058569811779,\n",
       "  '[0, 1]': 0.877582539622262,\n",
       "  '[0, -1]': 0.877582557264544},\n",
       " '[3, 1]': {'[1, 0]': -0.9996319597617923,\n",
       "  '[-1, 0]': 0.015240172341330587,\n",
       "  '[0, 1]': 0.8646637019492132,\n",
       "  '[0, -1]': 0.8646637019492132},\n",
       " '[3, 3]': {'[1, 0]': -0.35075427290933475,\n",
       "  '[-1, 0]': 0.9689124193613886,\n",
       "  '[0, 1]': 0.8646637019492132,\n",
       "  '[0, -1]': 0.8646637019492132},\n",
       " '[4, 0]': {'[1, 0]': 0.8253356127337367,\n",
       "  '[-1, 0]': 0.8253356152248872,\n",
       "  '[0, 1]': 0.8253356085498306,\n",
       "  '[0, -1]': 0.8253356135795026},\n",
       " '[4, 1]': {'[1, 0]': 0.8596923323711294,\n",
       "  '[-1, 0]': 0.9210609908899446,\n",
       "  '[0, 1]': -0.20550673261602842,\n",
       "  '[0, -1]': 0.2352375646476296},\n",
       " '[4, 2]': {'[1, 0]': 0.8646637019492132,\n",
       "  '[-1, 0]': 0.8646637019492132,\n",
       "  '[0, 1]': 0.766358188097808,\n",
       "  '[0, -1]': 0.9800665762489598},\n",
       " '[4, 3]': {'[1, 0]': 4.809955581185084e-09,\n",
       "  '[-1, 0]': 5.887185994451507e-09,\n",
       "  '[0, 1]': 2.0702578770561952e-09,\n",
       "  '[0, -1]': 5.666630275552348e-09}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa_lambda(100, 0.9, 0.5, 0.2, 'deterministic',0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True online Sarsa($ \\lambda $) for estimating $W^T X \\approx q_{\\pi}$ or $q_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_w(W,current_state,epsilon,environment_stochasticity):\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "    new_w = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            new_w[str(state)] =  {}\n",
    "            \n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                new_w[str(state)][action] = np.cos(abs(W[str(state)][str(action)][0]) +\\\n",
    "                                                abs(W[str(state)][str(action)][1]))\n",
    "    #print(probs)\n",
    "\n",
    "    if type(current_state) == str:\n",
    "\n",
    "        state = ast.literal_eval(current_state)\n",
    "    else:\n",
    "        state = current_state\n",
    "    \n",
    "    #Choose action using policy derived from Q===================================\n",
    "    value_action_state = reverse_dictionary(new_w[str(state)])\n",
    "    Max_val = max(list(value_action_state.keys()))\n",
    "    best_action = value_action_state[Max_val]\n",
    "    best_action = ast.literal_eval(best_action)\n",
    "\n",
    "    #============================================================================\n",
    "    #Epsilon Greedy\n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "        selected_action = best_action\n",
    "    \n",
    "    else:\n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        Actions.remove(best_action)\n",
    "        epsilon_action = random.choice(Actions)\n",
    "\n",
    "        selected_action = epsilon_action\n",
    "    #============================================================================\n",
    "    \n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "    Actions.remove(selected_action)\n",
    "    sorted_actions = Actions + [selected_action]\n",
    "    state_indice = state_indice_dict[str(state)]\n",
    "    #print(probs)\n",
    "    #print(state_indice)\n",
    "    actions_prob = probs[state_indice]\n",
    "    actions_prob.sort()\n",
    "    #due to stochasticity of the environment\n",
    "    Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "    #print(type(state), type(Final_action))\n",
    "    \n",
    "    next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "    if next_state not in environment[6] or next_state in environment[4]:\n",
    "\n",
    "        next_state = current_state\n",
    "    \n",
    "    value_action_state = reverse_dictionary(new_w[str(state)])\n",
    "    #max Q(s',s)\n",
    "    Max_q_val = max(list(value_action_state.keys()))\n",
    "    best_action = value_action_state[Max_q_val]\n",
    "    best_action = ast.literal_eval(best_action)\n",
    "\n",
    "    return Final_action, next_state, Max_q_val\n",
    "\n",
    "\n",
    "def true_online_sarsa(num_trials, gamma, Lambda, alpha, environment_stochasticity,epsilon):\n",
    "\n",
    "    W  = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Features = extract_features(state)\n",
    "            W[str(state)] =  {}\n",
    "            \n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                W[str(state)][action] = np.array([Features[0] + random.uniform(1e-9, 1e-8),Features[1] + random.uniform(1e-9, 1e-8)])\n",
    "    \n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        \n",
    "\n",
    "        Z = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                Z[str(state)] = {}\n",
    "                \n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    Z[str(state)][action] = np.array([0,0])\n",
    "        \n",
    "        Q_old = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                Q_old[str(state)] = {}\n",
    "                \n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    Q_old[str(state)][action] = 0\n",
    "        \n",
    "        Q_prime = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                Q_prime[str(state)] = {}\n",
    "                \n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    Q_prime[str(state)][action] = 0\n",
    "        \n",
    "        current_state = environment[2] #start\n",
    "        \n",
    "        while current_state != environment[3]:\n",
    "\n",
    "            \n",
    "\n",
    "            greedy_policy = greedy_w(W,current_state,epsilon,environment_stochasticity)\n",
    "            action = greedy_policy[0]\n",
    "            next_state = greedy_policy[1]\n",
    "            #print(current_state,action)\n",
    "\n",
    "            x = np.array([extract_features(next_state)[0],extract_features(next_state)[1]])\n",
    "\n",
    "            next_next_state = greedy_w(W,next_state,epsilon,environment_stochasticity)[1]\n",
    "\n",
    "            x_prime = np.array([extract_features(next_next_state)[0],extract_features(next_next_state)[1]])\n",
    "\n",
    "            Q[str(current_state)][str(action)] = np.matmul(np.transpose(W[str(current_state)][str(action)]),x)\n",
    "            Q_prime[str(current_state)][str(action)] = np.matmul(np.transpose(W[str(current_state)][str(action)]),x_prime)\n",
    "            \n",
    "            r = state_reward(next_state)\n",
    "            delta = r + gamma * Q_prime[str(current_state)][str(action)] - Q[str(current_state)][str(action)]\n",
    "\n",
    "            Z[str(current_state)][str(action)] = gamma * Lambda * Z[str(current_state)][str(action)] +\\\n",
    "                ((1 - alpha * gamma * Lambda * np.matmul(np.transpose(Z[str(current_state)][str(action)]) , x)) * x)\n",
    "\n",
    "            W[str(current_state)][str(action)] = W[str(current_state)][str(action)] +\\\n",
    "                alpha * (delta + Q[str(current_state)][str(action)] - Q_old[str(current_state)][str(action)]) * Z[str(current_state)][str(action)]\\\n",
    "                     - alpha * (Q[str(current_state)][str(action)] - Q_old[str(current_state)][str(action)]) * x\n",
    "\n",
    "            Q_old[str(current_state)][str(action)] = Q_prime[str(current_state)][str(action)]\n",
    "\n",
    "            current_state = next_state\n",
    "            x = x_prime\n",
    "            #print(current_state)\n",
    "\n",
    "    \n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:03<00:00, 78.57it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -76.89262328227105,\n",
       "  '[-1, 0]': -7.561396121660657,\n",
       "  '[0, 1]': -7.903332838177607,\n",
       "  '[0, -1]': -7.628312257172707},\n",
       " '[0, 2]': {'[1, 0]': -9.974782542185366,\n",
       "  '[-1, 0]': -7.944849854234295,\n",
       "  '[0, 1]': -4.633836947346454,\n",
       "  '[0, -1]': -6.895198670678157},\n",
       " '[0, 3]': {'[1, 0]': -5.304753784834761,\n",
       "  '[-1, 0]': -7.626240113567025,\n",
       "  '[0, 1]': -7.499625973759261,\n",
       "  '[0, -1]': -8.363630903980031},\n",
       " '[1, 0]': {'[1, 0]': -9.635010951461,\n",
       "  '[-1, 0]': -9.431716817872601,\n",
       "  '[0, 1]': -20.288480998326,\n",
       "  '[0, -1]': -9.352078423176987},\n",
       " '[1, 1]': {'[1, 0]': -10.280961714230699,\n",
       "  '[-1, 0]': -6.706821101653243,\n",
       "  '[0, 1]': -10.582696582646017,\n",
       "  '[0, -1]': -7.7027253978061285},\n",
       " '[1, 2]': {'[1, 0]': -6.14846605195029,\n",
       "  '[-1, 0]': -7.315489855670303,\n",
       "  '[0, 1]': -5.044813393371052,\n",
       "  '[0, -1]': -17.900661163748822},\n",
       " '[1, 3]': {'[1, 0]': -2.483009231300176,\n",
       "  '[-1, 0]': -4.64324189342688,\n",
       "  '[0, 1]': -10.542516465922022,\n",
       "  '[0, -1]': -8.885592278498086},\n",
       " '[2, 1]': {'[1, 0]': -20.527496706222628,\n",
       "  '[-1, 0]': -13.4595356183057,\n",
       "  '[0, 1]': -9.225140962341804,\n",
       "  '[0, -1]': -7.267348572546637},\n",
       " '[2, 2]': {'[1, 0]': -11.046742196376066,\n",
       "  '[-1, 0]': -5.870855454865903,\n",
       "  '[0, 1]': -2.699106535145733,\n",
       "  '[0, -1]': -7.62895659228807},\n",
       " '[2, 3]': {'[1, 0]': -1.461257906793699,\n",
       "  '[-1, 0]': -3.5846915382269766,\n",
       "  '[0, 1]': -3.49166719410254,\n",
       "  '[0, -1]': -6.583659984894905},\n",
       " '[3, 1]': {'[1, 0]': -3.295232450483244,\n",
       "  '[-1, 0]': -6.530274140803386,\n",
       "  '[0, 1]': -10.454687099447925,\n",
       "  '[0, -1]': -10.191544946148202},\n",
       " '[3, 3]': {'[1, 0]': 0.0,\n",
       "  '[-1, 0]': -2.51870782972586,\n",
       "  '[0, 1]': -1.6752291178104044,\n",
       "  '[0, -1]': -1.6561874600385205},\n",
       " '[4, 0]': {'[1, 0]': -8.556869872436824,\n",
       "  '[-1, 0]': -6.2929475683347675,\n",
       "  '[0, 1]': -9.671522304675113,\n",
       "  '[0, -1]': -8.367114725130694},\n",
       " '[4, 1]': {'[1, 0]': -3.4251705491193785,\n",
       "  '[-1, 0]': -3.3774214977330255,\n",
       "  '[0, 1]': -1.334343739623939,\n",
       "  '[0, -1]': -4.918503988600582},\n",
       " '[4, 2]': {'[1, 0]': -0.26773269289270896,\n",
       "  '[-1, 0]': -0.35348822880398995,\n",
       "  '[0, 1]': 0.0,\n",
       "  '[0, -1]': -1.2288314383848236},\n",
       " '[4, 3]': {'[1, 0]': 9.212171246978418e-09,\n",
       "  '[-1, 0]': 6.024075547797405e-09,\n",
       "  '[0, 1]': 1.2269101618918497e-09,\n",
       "  '[0, -1]': 4.312271088822588e-09}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_online_sarsa(5000, 0.9, 0.5, 0.2, 'deterministic',0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
