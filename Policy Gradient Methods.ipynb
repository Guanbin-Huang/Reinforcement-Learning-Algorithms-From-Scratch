{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_world(length, width,path_lenght,holes_number,Random_State):\n",
    "    \n",
    "    random.seed(Random_State)\n",
    "    #store all cells in a list\n",
    "    Grid_Cells = []\n",
    "    for row in range(length):\n",
    "        for col in range(width):\n",
    "            Grid_Cells.append([row,col])\n",
    "\n",
    "\n",
    "    #specify the number of holes in the gridworld\n",
    "    \n",
    "    #specify the start point as a random cell\n",
    "    start = [random.randint(0, length), random.randint(0, width)]\n",
    "\n",
    "    #create a path from start point\n",
    "    \"\"\"instead of defining start and goal points,\n",
    "      we define just a start point and a random path with a random lenght to\n",
    "       another point and name it as goal point\"\"\"\n",
    "    \n",
    "    def random_path(Start, Path_Lenght,length, width):\n",
    "        \n",
    "        Path = []\n",
    "        Path.append(Start)\n",
    "        for i in range(Path_Lenght):\n",
    "            \n",
    "            #there are two moves that take us on a random cell named Goal [1,0], [0,1]\n",
    "            \n",
    "            move = random.choice([[1,0], [0,1]])\n",
    "            \n",
    "            #update the start cell/point by the above move\n",
    "            Start = [x + y for x, y in zip(Start, move)]\n",
    "            \n",
    "            #if the movement take us out of our gridworld, we reverse the change in the start point\n",
    "            if Start[0] < 0 or Start[1] < 0 or Start[0] > length-1 or Start[1] > width-1:\n",
    "\n",
    "                Start = [x - y for x, y in zip(Start, move)]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                #create a path history\n",
    "                Path.append(Start)\n",
    "\n",
    "        Goal = Start\n",
    "\n",
    "        return Goal,Path\n",
    "    \n",
    "\n",
    "    GoalPath = random_path(start, path_lenght,length, width)\n",
    "\n",
    "    goal = GoalPath[0]\n",
    "    path = GoalPath[1]\n",
    "\n",
    "    #now we must eliminate the path cells from the Grid_Cells to choose hole cells from remaining cells\n",
    "\n",
    "    FreeCells = [x for x in Grid_Cells if x not in path]\n",
    "\n",
    "    Holes = random.sample(FreeCells, holes_number)\n",
    "\n",
    "    #Also, we can visualize our gridworld in a simple way\n",
    "\n",
    "    def mark_holes(holes):\n",
    "        marked_data = [[\"Hole\" if [row, col] in holes else [row, col] for col in range(width)] for row in range(length)]\n",
    "        return marked_data\n",
    "    \n",
    "    marked_matrix = mark_holes(Holes)\n",
    "\n",
    "    print(tabulate(marked_matrix, tablefmt=\"grid\"))\n",
    "\n",
    "    \n",
    "    return length, width, start, goal, Holes, path,Grid_Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+\n",
      "| Hole   | [0, 1] | [0, 2] | [0, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [1, 0] | [1, 1] | [1, 2] | [1, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [2, 1] | [2, 2] | [2, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [3, 1] | Hole   | [3, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [4, 0] | [4, 1] | [4, 2] | [4, 3] |\n",
      "+--------+--------+--------+--------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 4,\n",
       " [1, 2],\n",
       " [4, 3],\n",
       " [[2, 0], [3, 2], [3, 0], [0, 0]],\n",
       " [[1, 2], [1, 3], [2, 3], [3, 3], [4, 3]],\n",
       " [[0, 0],\n",
       "  [0, 1],\n",
       "  [0, 2],\n",
       "  [0, 3],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [3, 0],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [4, 0],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#environment = generate_grid_world(50, 40,1300,400,39)\n",
    "environment = generate_grid_world(5, 4,4,4,39)\n",
    "\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_distribution(grid_size,randomness):\n",
    "    #random.seed(40)\n",
    "    \n",
    "    #by this function we generate probabilities which their sum is equal to 1\n",
    "    def generate_probabilities(n):\n",
    "\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "        \n",
    "        return scaled_numbers\n",
    "    \n",
    "    cells_prob = {}\n",
    "    if randomness == 'stochastic':\n",
    "        for cell in range(grid_size):\n",
    "            \n",
    "            #we set the number of probs to 4 due to 4 possible action for each cell (go to its neighbors)\n",
    "            probs = generate_probabilities(4)\n",
    "\n",
    "            cells_prob[cell] = probs\n",
    "    elif randomness == 'equal probable':\n",
    "\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.25,0.25,0.25,0.25]\n",
    "    \n",
    "    elif randomness == 'deterministic':\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.07,0.09,0.04,0.8] #[0,0,0,1] ##[0.15,.15,0.1,0.6]\n",
    "\n",
    "\n",
    "    #Note that we consider the correspondence between probabilities and actions as below:\n",
    "    #probs = [p1, p2, p3, p4] ---> [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    return cells_prob\n",
    "\n",
    "def neighbor_cells(cell):\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    Neighbors = []\n",
    "    Actions_Neighbors = []\n",
    "    for action in Actions:\n",
    "\n",
    "        neighbor = [x + y for x, y in zip(cell, action)]\n",
    "        #if neighbor not in environment[4]:\n",
    "        Neighbors.append(neighbor)\n",
    "        Actions_Neighbors.append(action)\n",
    "\n",
    "    return Neighbors, Actions_Neighbors\n",
    "\n",
    "def arbitrary_policy(randomness):\n",
    "\n",
    "        #random.seed(randomness)\n",
    "        \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            neighbors = neighbor_cells(state)[0]\n",
    "            Actions_Neighbors = neighbor_cells(state)[1]\n",
    "\n",
    "            allowed_positions = []\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                \n",
    "                if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                    \n",
    "                    allowed_positions.append(neighbor)\n",
    "            \n",
    "            if len(allowed_positions) > 0:\n",
    "                \n",
    "                next_state = random.choice(allowed_positions)\n",
    "                row = next_state[0] - state[0]\n",
    "                col = next_state[1] - state[1]\n",
    "                PolicyAction = [row, col]\n",
    "\n",
    "                policy['{}'.format(state)] = next_state\n",
    "                policy_action['{}'.format(state)] = PolicyAction\n",
    "\n",
    "\n",
    "\n",
    "    return policy, policy_action\n",
    "\n",
    "def state_reward(next_state):\n",
    "\n",
    "    if next_state in environment[4]:\n",
    "\n",
    "        r = -3\n",
    "    \n",
    "    elif next_state == environment[3]:\n",
    "\n",
    "        r = 10\n",
    "    \n",
    "    elif next_state not in environment[6]:\n",
    "\n",
    "        r = -2\n",
    "    \n",
    "    else:\n",
    "\n",
    "        r = -1\n",
    "    \n",
    "    return r\n",
    "\n",
    "def reverse_dictionary(dict):\n",
    "    reverse_dict = {}\n",
    "    for key in list(dict.keys()):\n",
    "        val = dict[key]\n",
    "        reverse_dict[val] = key\n",
    "    return reverse_dict\n",
    "\n",
    "\n",
    "state_indice_dict = {}\n",
    "counter = 0\n",
    "for state in environment[6]:\n",
    "\n",
    "    state = str(state)\n",
    "    state_indice_dict[state] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "def generate_trajectory(policy,randomness,environment_stochasticity):\n",
    "\n",
    "    policy_action = policy[1]\n",
    "    probs = probability_distribution(environment[0]*environment[1],environment_stochasticity)\n",
    "    start = environment[2]\n",
    "    terminate = start\n",
    "    trajectory = []\n",
    "    trajectory_actions = []\n",
    "    pure_trajectory = [start]\n",
    "    c = 0\n",
    "    while terminate != environment[3]:\n",
    "        random.seed(randomness+c)\n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        action = policy_action[str(terminate)]\n",
    "        Actions.remove(action)\n",
    "        sorted_actions = Actions + [action]\n",
    "        state_indice = state_indice_dict[str(terminate)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "\n",
    "        selected_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "\n",
    "        trajectory_actions.append(selected_action)\n",
    "        current_state = terminate\n",
    "        next_state = [x + y for x, y in zip(terminate, selected_action)]\n",
    "        pure_trajectory.append(next_state)\n",
    "        \n",
    "        #if the agent goes out of the gridworld, it stays in its current state\n",
    "        if next_state not in environment[6]:\n",
    "            next_state = terminate\n",
    "        \n",
    "        #if it drops into the holes, it goes to the start points\n",
    "        elif next_state in environment[4]:\n",
    "            next_state = start  \n",
    "\n",
    "        terminate = next_state\n",
    "        trajectory.append((current_state))\n",
    "        c = c+1\n",
    "    \n",
    "    trajectory.append((environment[3]))\n",
    "    pure_trajectory.append(environment[3])\n",
    "\n",
    "    return trajectory,trajectory_actions\n",
    "\n",
    "def extract_features(state):\n",
    "\n",
    "    goal = environment[3]\n",
    "    max_length = environment[0]\n",
    "    max_width = environment[1]\n",
    "\n",
    "    w1 = (goal[0] - state[0]) / max_width\n",
    "    w2 = (goal[1] - state[1]) / max_length\n",
    "\n",
    "    return abs(w1), abs(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_theta(environment):\n",
    "    \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state != environment[3] and state not in environment[4]:\n",
    "\n",
    "            Neighbors = neighbor_cells(state)\n",
    "\n",
    "            Distances = {}\n",
    "\n",
    "            for neighbor in Neighbors[0]:\n",
    "\n",
    "                if neighbor not in environment[4] and neighbor in environment[6]:\n",
    "\n",
    "                    distance = np.cos(extract_features(neighbor)[0]+extract_features(neighbor)[1])\n",
    "                    Distances[distance] = neighbor\n",
    "            \n",
    "            #closest to the terminate state\n",
    "            if list(Distances.keys()) != []:\n",
    "\n",
    "                best_neighbor = Distances[max(list(Distances.keys()))]\n",
    "            \n",
    "            else:\n",
    "                best_neighbor = state\n",
    "\n",
    "            policy[str(state)] = best_neighbor\n",
    "\n",
    "            row = best_neighbor[0] - state[0]\n",
    "            col = best_neighbor[1] - state[1]\n",
    "            PolicyAction = [row,col]\n",
    "            policy_action[str(state)] = PolicyAction\n",
    "    \n",
    "\n",
    "    return policy, policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': [1, 1],\n",
       "  '[0, 2]': [1, 2],\n",
       "  '[0, 3]': [1, 3],\n",
       "  '[1, 0]': [1, 1],\n",
       "  '[1, 1]': [2, 1],\n",
       "  '[1, 2]': [2, 2],\n",
       "  '[1, 3]': [2, 3],\n",
       "  '[2, 1]': [3, 1],\n",
       "  '[2, 2]': [2, 3],\n",
       "  '[2, 3]': [3, 3],\n",
       "  '[3, 1]': [4, 1],\n",
       "  '[3, 3]': [4, 3],\n",
       "  '[4, 0]': [4, 1],\n",
       "  '[4, 1]': [4, 2],\n",
       "  '[4, 2]': [4, 3]},\n",
       " {'[0, 1]': [1, 0],\n",
       "  '[0, 2]': [1, 0],\n",
       "  '[0, 3]': [1, 0],\n",
       "  '[1, 0]': [0, 1],\n",
       "  '[1, 1]': [1, 0],\n",
       "  '[1, 2]': [1, 0],\n",
       "  '[1, 3]': [1, 0],\n",
       "  '[2, 1]': [1, 0],\n",
       "  '[2, 2]': [0, 1],\n",
       "  '[2, 3]': [1, 0],\n",
       "  '[3, 1]': [1, 0],\n",
       "  '[3, 3]': [1, 0],\n",
       "  '[4, 0]': [0, 1],\n",
       "  '[4, 1]': [0, 1],\n",
       "  '[4, 2]': [0, 1]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_theta(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCCE: Monte-Carlo Policy-Gradient Control (episodic) for $\\pi_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_theta(environment,Theta):\n",
    "\n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "\n",
    "    Actions = [[1, 0],[-1, 0],[0, 1],[0, -1]]\n",
    "\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state != environment[3] and state not in environment[4]:\n",
    "\n",
    "            Distances = {}\n",
    "\n",
    "            for action in Actions:\n",
    "\n",
    "                distance = np.cos(abs(Theta[str(state)][str(action)][0]) + abs(Theta[str(state)][str(action)][1]))\n",
    "                #print(distance)\n",
    "\n",
    "                Distances[distance] = action\n",
    "            \n",
    "            #closest to the terminate state\n",
    "            if list(Distances.keys()) != []:\n",
    "\n",
    "                best_action = Distances[max(list(Distances.keys()))]\n",
    "                next_state = [x + y for x, y in zip(state, best_action)]\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                next_state = state\n",
    "                best_action = random.choices(Actions)\n",
    "    \n",
    "            policy[str(state)] = next_state\n",
    "            policy_action[str(state)] = best_action\n",
    "    \n",
    "\n",
    "    return policy, policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': [1, 1],\n",
       "  '[0, 2]': [1, 2],\n",
       "  '[0, 3]': [1, 3],\n",
       "  '[1, 0]': [2, 0],\n",
       "  '[1, 1]': [2, 1],\n",
       "  '[1, 2]': [2, 2],\n",
       "  '[1, 3]': [2, 3],\n",
       "  '[2, 1]': [3, 1],\n",
       "  '[2, 2]': [3, 2],\n",
       "  '[2, 3]': [3, 3],\n",
       "  '[3, 1]': [4, 1],\n",
       "  '[3, 3]': [4, 3],\n",
       "  '[4, 0]': [4, 1],\n",
       "  '[4, 1]': [4, 2],\n",
       "  '[4, 2]': [4, 3]},\n",
       " {'[0, 1]': [1, 0],\n",
       "  '[0, 2]': [1, 0],\n",
       "  '[0, 3]': [1, 0],\n",
       "  '[1, 0]': [1, 0],\n",
       "  '[1, 1]': [1, 0],\n",
       "  '[1, 2]': [1, 0],\n",
       "  '[1, 3]': [1, 0],\n",
       "  '[2, 1]': [1, 0],\n",
       "  '[2, 2]': [1, 0],\n",
       "  '[2, 3]': [1, 0],\n",
       "  '[3, 1]': [1, 0],\n",
       "  '[3, 3]': [1, 0],\n",
       "  '[4, 0]': [0, 1],\n",
       "  '[4, 1]': [0, 1],\n",
       "  '[4, 2]': [0, 1]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta = {}\n",
    "for state in environment[6]:\n",
    "\n",
    "    if state not in environment[4]:\n",
    "\n",
    "        Theta[str(state)] = {} \n",
    "\n",
    "        for action in [[1, 0],[-1, 0],[0, 1],[0, -1]]:\n",
    "\n",
    "            next_state = [x + y for x, y in zip(state, action)]\n",
    "            Features = extract_features(next_state)\n",
    "            \n",
    "            Theta[str(state)][str(action)] = [Features[0],Features[1]]\n",
    "\n",
    "pi_theta(environment,Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_gradient(num_trials, gamma, alpha, environment_stochasticity):\n",
    "    \n",
    "    Theta = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Theta[str(state)] = {} \n",
    "\n",
    "            for action in [[1, 0],[-1, 0],[0, 1],[0, -1]]:\n",
    "\n",
    "                next_state = [x + y for x, y in zip(state, action)]\n",
    "                Features = extract_features(next_state)\n",
    "                \n",
    "                Theta[str(state)][str(action)] = [Features[0],Features[1]] #Features[element] + random.uniform(1e-9, 1e-8)\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    Optimal_Policy = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Optimal_Policy[str(state)] = state\n",
    "\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        policy = pi_theta(environment,Theta)\n",
    "\n",
    "        TRAJECTORY = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "\n",
    "        trajectory = TRAJECTORY[0]\n",
    "        #print(len(trajectory))\n",
    "        #print(trajectory[-100:])\n",
    "        actions = TRAJECTORY[1]\n",
    "        \n",
    "\n",
    "        G = 0\n",
    "\n",
    "        for step_indx in range(len(trajectory[:-1])):\n",
    "\n",
    "            step = trajectory[step_indx]\n",
    "            \n",
    "            next_step = trajectory[step_indx+1]\n",
    "\n",
    "            done_action = actions[step_indx]\n",
    "\n",
    "            for k in range(step_indx+1,len(trajectory)):\n",
    "\n",
    "                \n",
    "                step_k = trajectory[k]\n",
    "\n",
    "                #next_step_k = trajectory[k+1]\n",
    "            \n",
    "                r = state_reward(step_k)\n",
    "\n",
    "                G = G + gamma ** (k - step_indx - 1) * r\n",
    "        \n",
    "            softmax_denominator = 0.0001\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                softmax_denominator = softmax_denominator +\\\n",
    "                    -np.sin(Theta[str(step)][action][0] + Theta[str(step)][action][1]) *\\\n",
    "                math.exp(np.cos(Theta[str(step)][action][0] + Theta[str(step)][action][1]))\n",
    "\n",
    "            #print('softmax_denominators',softmax_denominator)\n",
    "            gradient = (-np.sin(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]) *\\\n",
    "            math.exp(np.cos(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]))) - softmax_denominator\n",
    "\n",
    "            #print('gradient',gradient)\n",
    "                \n",
    "            t1 = Theta[str(step)][str(done_action)][0] +\\\n",
    "                alpha * (gamma ** step_indx) * G * gradient\n",
    "            \n",
    "            t2 = Theta[str(step)][str(done_action)][1] +\\\n",
    "                alpha * (gamma ** step_indx) * G * gradient\n",
    "\n",
    "            Theta[str(step)][str(done_action)] = [t1,t2]\n",
    "\n",
    "            Q[str(step)][str(done_action)] = np.cos(abs(t1)+abs(t2))\n",
    "    \n",
    "    for state in environment[6]:\n",
    "\n",
    "        if str(state) in list(Q.keys()):\n",
    "\n",
    "            value_action_state = reverse_dictionary(Q[str(state)])\n",
    "            Max_val = max(list(value_action_state.keys()))\n",
    "            best_action = value_action_state[Max_val]\n",
    "            best_action = ast.literal_eval(best_action)\n",
    "            next_state = [x + y for x, y in zip(state, best_action)]\n",
    "\n",
    "            if next_state not in environment[4] and next_state in environment[6]:\n",
    "\n",
    "                Optimal_Policy[str(state)] = next_state\n",
    "\n",
    "            else:\n",
    "\n",
    "                Optimal_Policy[str(state)] = state\n",
    "\n",
    "\n",
    "\n",
    "    return Q, Optimal_Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 283.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': {'[1, 0]': 0.7241384675723698,\n",
       "   '[-1, 0]': -0.049247033421538536,\n",
       "   '[0, 1]': 0.38252184475653783,\n",
       "   '[0, -1]': 0.04122590484344676},\n",
       "  '[0, 2]': {'[1, 0]': -0.3220543517165513,\n",
       "   '[-1, 0]': -0.3255366687491661,\n",
       "   '[0, 1]': -0.3323150246661916,\n",
       "   '[0, -1]': -0.33351601522310276},\n",
       "  '[0, 3]': {'[1, 0]': 0.7316888688738197,\n",
       "   '[-1, 0]': 0.3153223623952687,\n",
       "   '[0, 1]': 0.4473784650569547,\n",
       "   '[0, -1]': 0.43470464016508953},\n",
       "  '[1, 0]': {'[1, 0]': 0.6506196690364913,\n",
       "   '[-1, 0]': 0.0016581378741039362,\n",
       "   '[0, 1]': 0.4184760470827733,\n",
       "   '[0, -1]': 0.044824492491251526},\n",
       "  '[1, 1]': {'[1, 0]': -0.5275736590079866,\n",
       "   '[-1, 0]': -0.5280258785296158,\n",
       "   '[0, 1]': -0.525051468583322,\n",
       "   '[0, -1]': -0.5335159921654302},\n",
       "  '[1, 2]': {'[1, 0]': -0.9995943954566024,\n",
       "   '[-1, 0]': -0.9388472248098347,\n",
       "   '[0, 1]': -0.7644298947571735,\n",
       "   '[0, -1]': -0.8250009455325726},\n",
       "  '[1, 3]': {'[1, 0]': -0.33185118551951004,\n",
       "   '[-1, 0]': -0.3341941158153252,\n",
       "   '[0, 1]': -0.34011277597461276,\n",
       "   '[0, -1]': -0.36965716063458964},\n",
       "  '[2, 1]': {'[1, 0]': 0.9887710779360424,\n",
       "   '[-1, 0]': 0.41863282815999986,\n",
       "   '[0, 1]': 0.8036725654287635,\n",
       "   '[0, -1]': 0.49388934483704405},\n",
       "  '[2, 2]': {'[1, 0]': 0.31352057900171504,\n",
       "   '[-1, 0]': 0.2990757491699296,\n",
       "   '[0, 1]': 0.3179683004392141,\n",
       "   '[0, -1]': 0.2649362931732602},\n",
       "  '[2, 3]': {'[1, 0]': 0.9689124217106448,\n",
       "   '[-1, 0]': 0.7316888688738209,\n",
       "   '[0, 1]': 0.7947537789316579,\n",
       "   '[0, -1]': 0.8010684739089817},\n",
       "  '[3, 1]': {'[1, 0]': 0.9210609940028848,\n",
       "   '[-1, 0]': 0.6227552794293422,\n",
       "   '[0, 1]': 0.9015064394360474,\n",
       "   '[0, -1]': 0.6619066438770387},\n",
       "  '[3, 3]': {'[1, 0]': 0.9989893196947007,\n",
       "   '[-1, 0]': 0.8775825618903728,\n",
       "   '[0, 1]': 0.9008418136279148,\n",
       "   '[0, -1]': 0.9009772156934593},\n",
       "  '[4, 0]': {'[1, 0]': 0.6599842939810263,\n",
       "   '[-1, 0]': 0.6599831458849843,\n",
       "   '[0, 1]': 0.921060994002885,\n",
       "   '[0, -1]': 0.6967067093471654},\n",
       "  '[4, 1]': {'[1, 0]': 0.7960838428156204,\n",
       "   '[-1, 0]': 0.7961468380246371,\n",
       "   '[0, 1]': 0.9800665778412416,\n",
       "   '[0, -1]': 0.8253356149096783},\n",
       "  '[4, 2]': {'[1, 0]': 0.9004481112932393,\n",
       "   '[-1, 0]': 0.900448863361042,\n",
       "   '[0, 1]': 0.9999999987553851,\n",
       "   '[0, -1]': 0.9210609940028851},\n",
       "  '[4, 3]': {'[1, 0]': 2.7371086867500235e-09,\n",
       "   '[-1, 0]': 2.310489129624094e-09,\n",
       "   '[0, 1]': 5.87277430713536e-09,\n",
       "   '[0, -1]': 4.9830913593846685e-09}},\n",
       " {'[0, 1]': [1, 1],\n",
       "  '[0, 2]': [1, 2],\n",
       "  '[0, 3]': [1, 3],\n",
       "  '[1, 0]': [1, 0],\n",
       "  '[1, 1]': [1, 2],\n",
       "  '[1, 2]': [1, 3],\n",
       "  '[1, 3]': [2, 3],\n",
       "  '[2, 1]': [3, 1],\n",
       "  '[2, 2]': [2, 3],\n",
       "  '[2, 3]': [3, 3],\n",
       "  '[3, 1]': [4, 1],\n",
       "  '[3, 3]': [4, 3],\n",
       "  '[4, 0]': [4, 1],\n",
       "  '[4, 1]': [4, 2],\n",
       "  '[4, 2]': [4, 3],\n",
       "  '[4, 3]': [4, 3]})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#My trial and error shows that we should consider the gamma lower than 0.9\n",
    "monte_carlo_policy_gradient(1000, 0.05, 0.1, 'deterministic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCCE with Baseline (episodic) for estimating $\\pi_{\\theta} \\approx \\pi_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(num_trials, gamma, w_alpha, t_alpha, environment_stochasticity):\n",
    "\n",
    "    Theta = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Theta[str(state)] = {}\n",
    "\n",
    "            for action in [[1, 0],[-1, 0],[0, 1],[0, -1]]:\n",
    "\n",
    "                next_state = [x + y for x, y in zip(state, action)]\n",
    "                Features = extract_features(next_state)\n",
    "                \n",
    "                Theta[str(state)][str(action)] = [Features[0],Features[1]]\n",
    "    \n",
    "    W = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Features = extract_features(state)\n",
    "            \n",
    "            W[str(state)] = [Features[0]+random.uniform(1e-9, 1e-8),Features[1]+random.uniform(1e-9, 1e-8)]\n",
    "    \n",
    "    V = {}\n",
    "    state_observed = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            V[str(state)] = 0\n",
    "            state_observed[str(state)] = 0\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    Optimal_Policy = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Optimal_Policy[str(state)] = state\n",
    "\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        policy = pi_theta(environment,Theta)\n",
    "\n",
    "        TRAJECTORY = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "\n",
    "        trajectory = TRAJECTORY[0]\n",
    "\n",
    "        actions = TRAJECTORY[1]\n",
    "        \n",
    "\n",
    "        G = 0\n",
    "\n",
    "        for step_indx in range(len(trajectory[:-1])):\n",
    "\n",
    "            step = trajectory[step_indx]\n",
    "            \n",
    "            next_step = trajectory[step_indx+1]\n",
    "\n",
    "            done_action = actions[step_indx]\n",
    "\n",
    "            for k in range(step_indx+1,len(trajectory)):\n",
    "\n",
    "                \n",
    "                step_k = trajectory[k]\n",
    "\n",
    "                #next_step_k = trajectory[k+1]\n",
    "            \n",
    "                r = state_reward(step_k)\n",
    "\n",
    "                G = G + gamma ** (k - step_indx - 1) * r\n",
    "            \n",
    "            v_hat = np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "\n",
    "            delta = G - v_hat\n",
    "\n",
    "            gradient_w1 = -np.sin(abs(W[str(step)][0]) + abs(W[str(step)][1])) * (W[str(step)][0]/abs(W[str(step)][0]))\n",
    "            gradient_w2 = -np.sin(abs(W[str(step)][1]) + abs(W[str(step)][0])) * (W[str(step)][1]/abs(W[str(step)][1]))\n",
    "\n",
    "            W[str(step)][0] = W[str(step)][0] + w_alpha * delta * gradient_w1\n",
    "\n",
    "            W[str(step)][1] = W[str(step)][1] + w_alpha * delta * gradient_w2\n",
    "\n",
    "            \n",
    "            V[str(step)] = V[str(step)] + np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "\n",
    "\n",
    "            softmax_denominator = 0.0001\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                softmax_denominator = softmax_denominator +\\\n",
    "                    -np.sin(Theta[str(step)][action][0] + Theta[str(step)][action][1]) *\\\n",
    "                math.exp(np.cos(Theta[str(step)][action][0] + Theta[str(step)][action][1]))\n",
    "\n",
    "            #print('softmax_denominators',softmax_denominator)\n",
    "            gradient = (-np.sin(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]) *\\\n",
    "            math.exp(np.cos(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]))) - softmax_denominator\n",
    "\n",
    "            #print('gradient',gradient)\n",
    "                \n",
    "            t1 = Theta[str(step)][str(done_action)][0] +\\\n",
    "                t_alpha * (gamma ** step_indx) * delta * gradient\n",
    "            \n",
    "            t2 = Theta[str(step)][str(done_action)][1] +\\\n",
    "                t_alpha * (gamma ** step_indx) * delta * gradient\n",
    "\n",
    "            Theta[str(step)][str(done_action)] = [t1,t2]\n",
    "\n",
    "            Q[str(step)][str(done_action)] = np.cos(abs(t1)+abs(t2))\n",
    "    \n",
    "    for state in environment[6]:\n",
    "\n",
    "        if str(state) in list(Q.keys()):\n",
    "\n",
    "            value_action_state = reverse_dictionary(Q[str(state)])\n",
    "            Max_val = max(list(value_action_state.keys()))\n",
    "            best_action = value_action_state[Max_val]\n",
    "            best_action = ast.literal_eval(best_action)\n",
    "            next_state = [x + y for x, y in zip(state, best_action)]\n",
    "\n",
    "            if next_state not in environment[4] and next_state in environment[6]:\n",
    "\n",
    "                Optimal_Policy[str(state)] = next_state\n",
    "\n",
    "            else:\n",
    "\n",
    "                Optimal_Policy[str(state)] = state\n",
    "\n",
    "\n",
    "\n",
    "    return V, Q, Optimal_Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 892.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': -95.04639579379811,\n",
       "  '[0, 2]': -207.58500704757856,\n",
       "  '[0, 3]': -42.5969616575847,\n",
       "  '[1, 0]': -164.2549703346718,\n",
       "  '[1, 1]': -1348.2026981109395,\n",
       "  '[1, 2]': -2010.9988097002827,\n",
       "  '[1, 3]': -629.8141142535152,\n",
       "  '[2, 1]': -1058.1237556579745,\n",
       "  '[2, 2]': -207.10434582756073,\n",
       "  '[2, 3]': -564.1032737270275,\n",
       "  '[3, 1]': -917.0104795138152,\n",
       "  '[3, 3]': 494.8171595699261,\n",
       "  '[4, 0]': -74.55085932384843,\n",
       "  '[4, 1]': -932.3273933671558,\n",
       "  '[4, 2]': -722.1284663003171,\n",
       "  '[4, 3]': 0},\n",
       " {'[0, 1]': {'[1, 0]': 0.4095107246606044,\n",
       "   '[-1, 0]': -0.079045574106728,\n",
       "   '[0, 1]': 0.3624368801630644,\n",
       "   '[0, -1]': -0.02912633039806283},\n",
       "  '[0, 2]': {'[1, 0]': 0.747224915222459,\n",
       "   '[-1, 0]': 0.1409596020356665,\n",
       "   '[0, 1]': 0.5403023058681398,\n",
       "   '[0, -1]': 0.21778030944790022},\n",
       "  '[0, 3]': {'[1, 0]': 0.7316888688738211,\n",
       "   '[-1, 0]': 8.947011370440381e-09,\n",
       "   '[0, 1]': 0.36235775447676305,\n",
       "   '[0, -1]': 0.3624339375521967},\n",
       "  '[1, 0]': {'[1, 0]': 0.454870946519627,\n",
       "   '[-1, 0]': -0.02908479418704155,\n",
       "   '[0, 1]': 0.40861232127555414,\n",
       "   '[0, -1]': 0.021021309633226463},\n",
       "  '[1, 1]': {'[1, 0]': 0.6808638591335971,\n",
       "   '[-1, 0]': 0.23845324242289348,\n",
       "   '[0, 1]': 0.7105935141222262,\n",
       "   '[0, -1]': 0.3584039214323034},\n",
       "  '[1, 2]': {'[1, 0]': -0.09403345126375945,\n",
       "   '[-1, 0]': 0.35288058890718194,\n",
       "   '[0, 1]': 0.6055478332291827,\n",
       "   '[0, -1]': 0.34326005023479306},\n",
       "  '[1, 3]': {'[1, 0]': 0.8636579787024938,\n",
       "   '[-1, 0]': 0.54030230586814,\n",
       "   '[0, 1]': 0.6331529746675787,\n",
       "   '[0, -1]': 0.6384751262506391},\n",
       "  '[2, 1]': {'[1, 0]': 0.8046457182779942,\n",
       "   '[-1, 0]': 0.4092074305163857,\n",
       "   '[0, 1]': 0.7657221468528126,\n",
       "   '[0, -1]': 0.4552119257321324},\n",
       "  '[2, 2]': {'[1, 0]': 0.955192491893756,\n",
       "   '[-1, 0]': 0.5874925083105699,\n",
       "   '[0, 1]': 0.8775825618903728,\n",
       "   '[0, -1]': 0.6431368849867375},\n",
       "  '[2, 3]': {'[1, 0]': 0.968912421710645,\n",
       "   '[-1, 0]': 0.7316888688738207,\n",
       "   '[0, 1]': 0.7652232374340313,\n",
       "   '[0, -1]': 0.7653788781176084},\n",
       "  '[3, 1]': {'[1, 0]': 0.9210609940028847,\n",
       "   '[-1, 0]': 0.6216134903853714,\n",
       "   '[0, 1]': 0.9004506794749948,\n",
       "   '[0, -1]': 0.6599897456014777},\n",
       "  '[3, 3]': {'[1, 0]': 0.9999999973586172,\n",
       "   '[-1, 0]': 0.8775825618903728,\n",
       "   '[0, 1]': 0.900448791195656,\n",
       "   '[0, -1]': 0.9004496518996394},\n",
       "  '[4, 0]': {'[1, 0]': 0.6599831458941263,\n",
       "   '[-1, 0]': 0.6599831459032703,\n",
       "   '[0, 1]': 0.9210609940028849,\n",
       "   '[0, -1]': 0.6967067093471654},\n",
       "  '[4, 1]': {'[1, 0]': 0.7960838101003325,\n",
       "   '[-1, 0]': 0.7960838226087091,\n",
       "   '[0, 1]': 0.9800665778412415,\n",
       "   '[0, -1]': 0.8253356149096784},\n",
       "  '[4, 2]': {'[1, 0]': 0.9004471023760154,\n",
       "   '[-1, 0]': 0.9004471024126464,\n",
       "   '[0, 1]': 1.0,\n",
       "   '[0, -1]': 0.921060994002885},\n",
       "  '[4, 3]': {'[1, 0]': 2.8199469736564215e-09,\n",
       "   '[-1, 0]': 3.1813373463539836e-09,\n",
       "   '[0, 1]': 2.5985225397608124e-09,\n",
       "   '[0, -1]': 7.932924195698498e-09}},\n",
       " {'[0, 1]': [1, 1],\n",
       "  '[0, 2]': [1, 2],\n",
       "  '[0, 3]': [1, 3],\n",
       "  '[1, 0]': [1, 0],\n",
       "  '[1, 1]': [1, 2],\n",
       "  '[1, 2]': [1, 3],\n",
       "  '[1, 3]': [2, 3],\n",
       "  '[2, 1]': [3, 1],\n",
       "  '[2, 2]': [2, 2],\n",
       "  '[2, 3]': [3, 3],\n",
       "  '[3, 1]': [4, 1],\n",
       "  '[3, 3]': [4, 3],\n",
       "  '[4, 0]': [4, 1],\n",
       "  '[4, 1]': [4, 2],\n",
       "  '[4, 2]': [4, 3],\n",
       "  '[4, 3]': [4, 2]})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline(1000, 0.005,0.1, 0.1, 'deterministic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-step Actor-Critic (episodic), for estimating $\\pi_{\\theta} \\approx \\pi_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_actor_critic(num_trials, gamma, w_alpha, t_alpha, environment_stochasticity):\n",
    "\n",
    "    Theta = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Theta[str(state)] = {}\n",
    "\n",
    "            for action in [[1, 0],[-1, 0],[0, 1],[0, -1]]:\n",
    "\n",
    "                next_state = [x + y for x, y in zip(state, action)]\n",
    "                Features = extract_features(next_state)\n",
    "                \n",
    "                Theta[str(state)][str(action)] = [Features[0],Features[1]]\n",
    "    \n",
    "    W = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Features = extract_features(state)\n",
    "            \n",
    "            W[str(state)] = [Features[0]+random.uniform(1e-9, 1e-8),Features[1]+random.uniform(1e-9, 1e-8)]\n",
    "    \n",
    "    V = {}\n",
    "    state_observed = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            V[str(state)] = 0\n",
    "            state_observed[str(state)] = 0\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    Optimal_Policy = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Optimal_Policy[str(state)] = state\n",
    "\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        policy = pi_theta(environment,Theta)\n",
    "\n",
    "        TRAJECTORY = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "\n",
    "        trajectory = TRAJECTORY[0]\n",
    "\n",
    "        actions = TRAJECTORY[1]\n",
    "        \n",
    "\n",
    "        G = 0\n",
    "        I = 1\n",
    "\n",
    "        for step_indx in range(len(trajectory[:-1])):\n",
    "\n",
    "            step = trajectory[step_indx]\n",
    "            \n",
    "            next_step = trajectory[step_indx+1]\n",
    "\n",
    "            done_action = actions[step_indx]\n",
    "            \n",
    "            v_hat_step = np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "            v_hat_next_step = np.cos(abs(W[str(next_step)][0]) + abs(W[str(next_step)][1]))\n",
    "            r = state_reward(next_step)\n",
    "            delta = r + gamma * v_hat_next_step - v_hat_step\n",
    "\n",
    "            gradient_w1 = -np.sin(abs(W[str(step)][0]) + abs(W[str(step)][1])) * (W[str(step)][0]/abs(W[str(step)][0]))\n",
    "            gradient_w2 = -np.sin(abs(W[str(step)][1]) + abs(W[str(step)][0])) * (W[str(step)][1]/abs(W[str(step)][1]))\n",
    "\n",
    "            W[str(step)][0] = W[str(step)][0] + w_alpha * delta * gradient_w1\n",
    "\n",
    "            W[str(step)][1] = W[str(step)][1] + w_alpha * delta * gradient_w2\n",
    "\n",
    "            \n",
    "            V[str(step)] = V[str(step)] + np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "\n",
    "\n",
    "            softmax_denominator = 0.0001\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                softmax_denominator = softmax_denominator +\\\n",
    "                    -np.sin(Theta[str(step)][action][0] + Theta[str(step)][action][1]) *\\\n",
    "                math.exp(np.cos(Theta[str(step)][action][0] + Theta[str(step)][action][1]))\n",
    "\n",
    "            #print('softmax_denominators',softmax_denominator)\n",
    "            gradient = (-np.sin(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]) *\\\n",
    "            math.exp(np.cos(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]))) - softmax_denominator\n",
    "\n",
    "            #print('gradient',gradient)\n",
    "                \n",
    "            t1 = Theta[str(step)][str(done_action)][0] +\\\n",
    "                t_alpha * I * delta * gradient\n",
    "            \n",
    "            t2 = Theta[str(step)][str(done_action)][1] +\\\n",
    "                t_alpha * I * delta * gradient\n",
    "\n",
    "            Theta[str(step)][str(done_action)] = [t1,t2]\n",
    "\n",
    "            Q[str(step)][str(done_action)] = np.cos(abs(t1)+abs(t2))\n",
    "            I = gamma * I\n",
    "    \n",
    "    for state in environment[6]:\n",
    "\n",
    "        if str(state) in list(Q.keys()):\n",
    "\n",
    "            value_action_state = reverse_dictionary(Q[str(state)])\n",
    "            Max_val = max(list(value_action_state.keys()))\n",
    "            best_action = value_action_state[Max_val]\n",
    "            best_action = ast.literal_eval(best_action)\n",
    "            next_state = [x + y for x, y in zip(state, best_action)]\n",
    "\n",
    "            if next_state not in environment[4] and next_state in environment[6]:\n",
    "\n",
    "                Optimal_Policy[str(state)] = next_state\n",
    "\n",
    "            else:\n",
    "\n",
    "                Optimal_Policy[str(state)] = state\n",
    "\n",
    "\n",
    "\n",
    "    return V, Q, Optimal_Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 379.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': -860.3076210582119,\n",
       "  '[0, 2]': -4180.09966920436,\n",
       "  '[0, 3]': -2765.974007313246,\n",
       "  '[1, 0]': -406.2906355155226,\n",
       "  '[1, 1]': -3784.105547676202,\n",
       "  '[1, 2]': -10520.660373814468,\n",
       "  '[1, 3]': -5153.514297126877,\n",
       "  '[2, 1]': -1301.722967067913,\n",
       "  '[2, 2]': -1512.5480462175228,\n",
       "  '[2, 3]': -491.18959751862985,\n",
       "  '[3, 1]': -1034.3398536782017,\n",
       "  '[3, 3]': -15.676458542581848,\n",
       "  '[4, 0]': -102.41434506539863,\n",
       "  '[4, 1]': -937.0345853798509,\n",
       "  '[4, 2]': -92.13313501093741,\n",
       "  '[4, 3]': 0},\n",
       " {'[0, 1]': {'[1, 0]': 0.4272020238645434,\n",
       "   '[-1, 0]': -0.07654376852531099,\n",
       "   '[0, 1]': 0.36289728262255055,\n",
       "   '[0, -1]': -0.025365826770949246},\n",
       "  '[0, 2]': {'[1, 0]': 0.46947105847960385,\n",
       "   '[-1, 0]': 0.22622475853815943,\n",
       "   '[0, 1]': 0.5403023058681399,\n",
       "   '[0, -1]': 0.31734085450485133},\n",
       "  '[0, 3]': {'[1, 0]': 0.7316888688738201,\n",
       "   '[-1, 0]': 0.315322362395268,\n",
       "   '[0, 1]': 0.37420844144266896,\n",
       "   '[0, -1]': 0.36635506376672133},\n",
       "  '[1, 0]': {'[1, 0]': 0.4607614513807729,\n",
       "   '[-1, 0]': -0.027869482006130708,\n",
       "   '[0, 1]': 0.4090317192923561,\n",
       "   '[0, -1]': 0.0214693477587142},\n",
       "  '[1, 1]': {'[1, 0]': 0.5918091520242087,\n",
       "   '[-1, 0]': 0.24615455786984378,\n",
       "   '[0, 1]': 0.8525245220595064,\n",
       "   '[0, -1]': 0.2800998067025553},\n",
       "  '[1, 2]': {'[1, 0]': -0.9999972375693531,\n",
       "   '[-1, 0]': -0.9999972804826439,\n",
       "   '[0, 1]': -0.9999973678194318,\n",
       "   '[0, -1]': -0.9999972888318652},\n",
       "  '[1, 3]': {'[1, 0]': 0.5466574012531948,\n",
       "   '[-1, 0]': 0.5403023058681393,\n",
       "   '[0, 1]': 0.6862410748097564,\n",
       "   '[0, -1]': 0.8525245220595051},\n",
       "  '[2, 1]': {'[1, 0]': 0.8786258881077097,\n",
       "   '[-1, 0]': 0.4174376386575278,\n",
       "   '[0, 1]': 0.7751653872832344,\n",
       "   '[0, -1]': 0.4703157308182445},\n",
       "  '[2, 2]': {'[1, 0]': 0.88809769864351,\n",
       "   '[-1, 0]': 0.6256403892024585,\n",
       "   '[0, 1]': 0.8775825618903726,\n",
       "   '[0, -1]': 0.9922396719653959},\n",
       "  '[2, 3]': {'[1, 0]': 0.9689124217106447,\n",
       "   '[-1, 0]': 0.7316888688738209,\n",
       "   '[0, 1]': 0.7684473848441083,\n",
       "   '[0, -1]': 0.7671381085766711},\n",
       "  '[3, 1]': {'[1, 0]': 0.9210609940028851,\n",
       "   '[-1, 0]': 0.6217820647233208,\n",
       "   '[0, 1]': 0.9007300819937563,\n",
       "   '[0, -1]': 0.660293456866352},\n",
       "  '[3, 3]': {'[1, 0]': 0.9664261433639576,\n",
       "   '[-1, 0]': 0.8775825618903728,\n",
       "   '[0, 1]': 0.9009491815474716,\n",
       "   '[0, -1]': 0.9011011093645542},\n",
       "  '[4, 0]': {'[1, 0]': 0.6599831458849832,\n",
       "   '[-1, 0]': 0.6599832733192074,\n",
       "   '[0, 1]': 0.9210609940028851,\n",
       "   '[0, -1]': 0.6967067093471655},\n",
       "  '[4, 1]': {'[1, 0]': 0.7960879855024484,\n",
       "   '[-1, 0]': 0.796088883998717,\n",
       "   '[0, 1]': 0.9800665778412416,\n",
       "   '[0, -1]': 0.8253356149096782},\n",
       "  '[4, 2]': {'[1, 0]': 0.9004480906558092,\n",
       "   '[-1, 0]': 0.9004503377904569,\n",
       "   '[0, 1]': 0.9999993664871746,\n",
       "   '[0, -1]': 0.9210609940028851},\n",
       "  '[4, 3]': {'[1, 0]': 5.540574513523441e-09,\n",
       "   '[-1, 0]': 4.157353811294666e-09,\n",
       "   '[0, 1]': 2.6591448443907306e-09,\n",
       "   '[0, -1]': 3.897777761001145e-09}},\n",
       " {'[0, 1]': [1, 1],\n",
       "  '[0, 2]': [0, 3],\n",
       "  '[0, 3]': [1, 3],\n",
       "  '[1, 0]': [1, 0],\n",
       "  '[1, 1]': [1, 2],\n",
       "  '[1, 2]': [2, 2],\n",
       "  '[1, 3]': [1, 2],\n",
       "  '[2, 1]': [3, 1],\n",
       "  '[2, 2]': [2, 1],\n",
       "  '[2, 3]': [3, 3],\n",
       "  '[3, 1]': [4, 1],\n",
       "  '[3, 3]': [4, 3],\n",
       "  '[4, 0]': [4, 1],\n",
       "  '[4, 1]': [4, 2],\n",
       "  '[4, 2]': [4, 3],\n",
       "  '[4, 3]': [4, 3]})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_actor_critic(1000, 0.05,0.5, 0.7, 'deterministic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic with Eligibility Traces (episodic), for estimating $\\pi_{\\theta} \\approx \\pi_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eligibility_traces_actor_critic_episodic(num_trials, gamma, w_alpha, t_alpha, environment_stochasticity,w_lambda,t_lambda):\n",
    "\n",
    "    Theta = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Theta[str(state)] = {}\n",
    "\n",
    "            for action in [[1, 0],[-1, 0],[0, 1],[0, -1]]:\n",
    "\n",
    "                next_state = [x + y for x, y in zip(state, action)]\n",
    "                Features = extract_features(next_state)\n",
    "                \n",
    "                Theta[str(state)][str(action)] = [Features[0]+random.uniform(1e-9, 1e-8),Features[1]+random.uniform(1e-9, 1e-8)]\n",
    "    \n",
    "    \n",
    "    W = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Features = extract_features(state)\n",
    "            \n",
    "            W[str(state)] = [Features[0]+random.uniform(1e-9, 1e-8),Features[1]+random.uniform(1e-9, 1e-8)]\n",
    "    \n",
    "    V = {}\n",
    "    state_observed = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            V[str(state)] = 0\n",
    "            state_observed[str(state)] = 0\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    Optimal_Policy = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Optimal_Policy[str(state)] = state\n",
    "\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        policy = pi_theta(environment,Theta)\n",
    "\n",
    "        TRAJECTORY = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "\n",
    "        trajectory = TRAJECTORY[0]\n",
    "\n",
    "        actions = TRAJECTORY[1]\n",
    "        t_Z = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                t_Z[str(state)] = {}\n",
    "                \n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    t_Z[str(state)][action] = [random.uniform(1e-9, 1e-8),random.uniform(1e-9, 1e-8)]\n",
    "        \n",
    "        w_Z = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                w_Z[str(state)] = {}\n",
    "                \n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    w_Z[str(state)][action] = [random.uniform(1e-9, 1e-8),random.uniform(1e-9, 1e-8)]\n",
    "\n",
    "        I = 1\n",
    "\n",
    "        for step_indx in range(len(trajectory[:-1])):\n",
    "\n",
    "            step = trajectory[step_indx]\n",
    "            \n",
    "            next_step = trajectory[step_indx+1]\n",
    "\n",
    "            done_action = actions[step_indx]\n",
    "\n",
    "\n",
    "            v_hat_step = np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "            v_hat_next_step = np.cos(abs(W[str(next_step)][0]) + abs(W[str(next_step)][1]))\n",
    "            r = state_reward(next_step)\n",
    "            delta = r + gamma * v_hat_next_step - v_hat_step\n",
    "\n",
    "\n",
    "            gradient_w1 = -np.sin(abs(W[str(step)][0]) + abs(W[str(step)][1])) * (W[str(step)][0]/abs(W[str(step)][0]))\n",
    "            gradient_w2 = -np.sin(abs(W[str(step)][1]) + abs(W[str(step)][0])) * (W[str(step)][1]/abs(W[str(step)][1]))\n",
    "\n",
    "            w_Z[str(step)][str(done_action)][0] = gamma * w_lambda * w_Z[str(step)][str(done_action)][0] + gradient_w1\n",
    "            w_Z[str(step)][str(done_action)][1] = gamma * w_lambda * w_Z[str(step)][str(done_action)][1] + gradient_w2\n",
    "            \n",
    "\n",
    "            softmax_denominator = 0.0001\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                softmax_denominator = softmax_denominator +\\\n",
    "                    -np.sin(Theta[str(step)][action][0] + Theta[str(step)][action][1]) *\\\n",
    "                math.exp(np.cos(Theta[str(step)][action][0] + Theta[str(step)][action][1]))\n",
    "\n",
    "            #print('softmax_denominators',softmax_denominator)\n",
    "            gradient_t1 = (-np.sin(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]) * (Theta[str(step)][str(done_action)][0]/abs(Theta[str(step)][str(done_action)][0])) *\\\n",
    "            math.exp(np.cos(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]))) - softmax_denominator\n",
    "\n",
    "            gradient_t2 = (-np.sin(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]) * (Theta[str(step)][str(done_action)][1]/abs(Theta[str(step)][str(done_action)][1])) *\\\n",
    "            math.exp(np.cos(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]))) - softmax_denominator\n",
    "\n",
    "            t_Z[str(step)][str(done_action)][0] = gamma * t_lambda * t_Z[str(step)][str(done_action)][0] + I * gradient_t1\n",
    "            t_Z[str(step)][str(done_action)][1] = gamma * t_lambda * t_Z[str(step)][str(done_action)][1] + I * gradient_t2\n",
    "\n",
    "\n",
    "            W[str(step)][0] = W[str(step)][0] + w_alpha * delta * w_Z[str(step)][str(done_action)][0]\n",
    "\n",
    "            W[str(step)][1] = W[str(step)][1] + w_alpha * delta * w_Z[str(step)][str(done_action)][1]\n",
    "\n",
    "            \n",
    "            V[str(step)] = V[str(step)] + np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #print('gradient',gradient)\n",
    "                \n",
    "            t1 = Theta[str(step)][str(done_action)][0] +\\\n",
    "                t_alpha * delta * t_Z[str(step)][str(done_action)][0]\n",
    "            \n",
    "            t2 = Theta[str(step)][str(done_action)][1] +\\\n",
    "                t_alpha * delta * t_Z[str(step)][str(done_action)][1]\n",
    "\n",
    "            Theta[str(step)][str(done_action)] = [t1,t2]\n",
    "\n",
    "            Q[str(step)][str(done_action)] = np.cos(abs(t1)+abs(t2))\n",
    "            I = gamma * I\n",
    "    \n",
    "    for state in environment[6]:\n",
    "\n",
    "        if str(state) in list(Q.keys()):\n",
    "\n",
    "            value_action_state = reverse_dictionary(Q[str(state)])\n",
    "            Max_val = max(list(value_action_state.keys()))\n",
    "            best_action = value_action_state[Max_val]\n",
    "            best_action = ast.literal_eval(best_action)\n",
    "            next_state = [x + y for x, y in zip(state, best_action)]\n",
    "\n",
    "            if next_state not in environment[4] and next_state in environment[6]:\n",
    "\n",
    "                Optimal_Policy[str(state)] = next_state\n",
    "\n",
    "            else:\n",
    "\n",
    "                Optimal_Policy[str(state)] = state\n",
    "\n",
    "\n",
    "\n",
    "    return V, Q, Optimal_Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 636.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': -14.39216157934931,\n",
       "  '[0, 2]': -140.09687714983772,\n",
       "  '[0, 3]': -23.02240390315338,\n",
       "  '[1, 0]': -12.390073257275487,\n",
       "  '[1, 1]': -326.11383275593835,\n",
       "  '[1, 2]': -2449.719999354992,\n",
       "  '[1, 3]': -242.61224376397573,\n",
       "  '[2, 1]': -1302.738746048939,\n",
       "  '[2, 2]': -1977.4942454828454,\n",
       "  '[2, 3]': -434.1199101146315,\n",
       "  '[3, 1]': -1158.3426521579377,\n",
       "  '[3, 3]': -43.119299527347636,\n",
       "  '[4, 0]': -114.40912538945888,\n",
       "  '[4, 1]': -1075.1407400691994,\n",
       "  '[4, 2]': -90.62618658786363,\n",
       "  '[4, 3]': 0},\n",
       " {'[0, 1]': {'[1, 0]': 0.4227127204380705,\n",
       "   '[-1, 0]': 7.772026331095774e-09,\n",
       "   '[0, 1]': 5.1586720274743505e-09,\n",
       "   '[0, -1]': -0.027304196578897228},\n",
       "  '[0, 2]': {'[1, 0]': 0.8161741550199557,\n",
       "   '[-1, 0]': 0.1407669568008603,\n",
       "   '[0, 1]': 0.5208312380812664,\n",
       "   '[0, -1]': 0.21585772322029495},\n",
       "  '[0, 3]': {'[1, 0]': 0.7302858422890514,\n",
       "   '[-1, 0]': 0.3153223689217506,\n",
       "   '[0, 1]': 4.0423561430237565e-09,\n",
       "   '[0, -1]': 0.36291320935694865},\n",
       "  '[1, 0]': {'[1, 0]': 0.46747128664727994,\n",
       "   '[-1, 0]': 1.6925917634082464e-09,\n",
       "   '[0, 1]': 8.681099027558277e-09,\n",
       "   '[0, -1]': 0.020799313172871845},\n",
       "  '[1, 1]': {'[1, 0]': 0.9801409154836896,\n",
       "   '[-1, 0]': 0.19699388761764178,\n",
       "   '[0, 1]': 0.6094716835116866,\n",
       "   '[0, -1]': 0.26801959155028404},\n",
       "  '[1, 2]': {'[1, 0]': 0.9919697631534611,\n",
       "   '[-1, 0]': -0.44294309021081696,\n",
       "   '[0, 1]': -0.5689304126327576,\n",
       "   '[0, -1]': 0.9912020145636584},\n",
       "  '[1, 3]': {'[1, 0]': 0.8527090453148938,\n",
       "   '[-1, 0]': 0.5250635248691012,\n",
       "   '[0, 1]': 0.6256378379717211,\n",
       "   '[0, -1]': 0.6509420228134009},\n",
       "  '[2, 1]': {'[1, 0]': 0.9260010668764562,\n",
       "   '[-1, 0]': 0.41929963244050955,\n",
       "   '[0, 1]': 0.7780424718579739,\n",
       "   '[0, -1]': 0.4774818400857259},\n",
       "  '[2, 2]': {'[1, 0]': 0.876898075976898,\n",
       "   '[-1, 0]': 0.546880817536112,\n",
       "   '[0, 1]': 0.7985831627480546,\n",
       "   '[0, -1]': 0.9881031318135242},\n",
       "  '[2, 3]': {'[1, 0]': 0.9687198010930829,\n",
       "   '[-1, 0]': 0.7310782188198923,\n",
       "   '[0, 1]': 0.7696855579127109,\n",
       "   '[0, -1]': 0.7679144558094182},\n",
       "  '[3, 1]': {'[1, 0]': 0.9201696722891327,\n",
       "   '[-1, 0]': 0.6219031826508176,\n",
       "   '[0, 1]': 0.9008172413494997,\n",
       "   '[0, -1]': 0.6608044011686383},\n",
       "  '[3, 3]': {'[1, 0]': 0.9441373537909535,\n",
       "   '[-1, 0]': 0.8770700124205341,\n",
       "   '[0, 1]': 0.9015474832707498,\n",
       "   '[0, -1]': 0.9009259275383324},\n",
       "  '[4, 0]': {'[1, 0]': 0.6599832315853268,\n",
       "   '[-1, 0]': 0.6599832730327188,\n",
       "   '[0, 1]': 0.9210606855882755,\n",
       "   '[0, -1]': 0.6967066616237196},\n",
       "  '[4, 1]': {'[1, 0]': 0.7960924762491248,\n",
       "   '[-1, 0]': 0.7961012797739687,\n",
       "   '[0, 1]': 0.9800659606587145,\n",
       "   '[0, -1]': 0.8253271220098451},\n",
       "  '[4, 2]': {'[1, 0]': 0.9004510174501237,\n",
       "   '[-1, 0]': 0.900453831717151,\n",
       "   '[0, 1]': 0.9999978710151296,\n",
       "   '[0, -1]': 0.9210572407272763},\n",
       "  '[4, 3]': {'[1, 0]': 6.436494721046623e-09,\n",
       "   '[-1, 0]': 4.211933711155014e-09,\n",
       "   '[0, 1]': 2.3376445456605923e-09,\n",
       "   '[0, -1]': 6.7233471054301924e-09}},\n",
       " {'[0, 1]': [1, 1],\n",
       "  '[0, 2]': [1, 2],\n",
       "  '[0, 3]': [1, 3],\n",
       "  '[1, 0]': [1, 0],\n",
       "  '[1, 1]': [2, 1],\n",
       "  '[1, 2]': [2, 2],\n",
       "  '[1, 3]': [2, 3],\n",
       "  '[2, 1]': [3, 1],\n",
       "  '[2, 2]': [2, 1],\n",
       "  '[2, 3]': [3, 3],\n",
       "  '[3, 1]': [4, 1],\n",
       "  '[3, 3]': [4, 3],\n",
       "  '[4, 0]': [4, 1],\n",
       "  '[4, 1]': [4, 2],\n",
       "  '[4, 2]': [4, 3],\n",
       "  '[4, 3]': [4, 2]})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eligibility_traces_actor_critic_episodic(1000, 0.05, 0.5, 0.5, 'deterministic',0.2,0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic with Eligibility Traces (continuing), for estimating $\\pi_{\\theta} \\approx \\pi_{*}$\n",
    "\n",
    "### Note that our task is not continuing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eligibility_traces_actor_critic_continuing(num_trials, w_alpha, t_alpha, r_alpha, environment_stochasticity,w_lambda,t_lambda):\n",
    "\n",
    "    Theta = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Theta[str(state)] = {}\n",
    "\n",
    "            for action in [[1, 0],[-1, 0],[0, 1],[0, -1]]:\n",
    "\n",
    "                next_state = [x + y for x, y in zip(state, action)]\n",
    "                Features = extract_features(next_state)\n",
    "                \n",
    "                Theta[str(state)][str(action)] = [Features[0]+random.uniform(1e-9, 1e-8),Features[1]+random.uniform(1e-9, 1e-8)]\n",
    "    \n",
    "    \n",
    "    W = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Features = extract_features(state)\n",
    "            \n",
    "            W[str(state)] = [Features[0]+random.uniform(1e-9, 1e-8),Features[1]+random.uniform(1e-9, 1e-8)]\n",
    "    \n",
    "    V = {}\n",
    "    state_observed = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            V[str(state)] = 0\n",
    "            state_observed[str(state)] = 0\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    Optimal_Policy = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Optimal_Policy[str(state)] = state\n",
    "\n",
    "    R_bar = 0\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        policy = pi_theta(environment,Theta)\n",
    "\n",
    "        TRAJECTORY = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "\n",
    "        trajectory = TRAJECTORY[0]\n",
    "\n",
    "        actions = TRAJECTORY[1]\n",
    "        t_Z = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                t_Z[str(state)] = {}\n",
    "                \n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    t_Z[str(state)][action] = [random.uniform(1e-9, 1e-8),random.uniform(1e-9, 1e-8)]\n",
    "        \n",
    "        w_Z = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                w_Z[str(state)] = {}\n",
    "                \n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    w_Z[str(state)][action] = [random.uniform(1e-9, 1e-8),random.uniform(1e-9, 1e-8)]\n",
    "\n",
    "\n",
    "        for step_indx in range(len(trajectory[:-1])):\n",
    "\n",
    "            step = trajectory[step_indx]\n",
    "            \n",
    "            next_step = trajectory[step_indx+1]\n",
    "\n",
    "            done_action = actions[step_indx]\n",
    "\n",
    "\n",
    "            v_hat_step = np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "            v_hat_next_step = np.cos(abs(W[str(next_step)][0]) + abs(W[str(next_step)][1]))\n",
    "            r = state_reward(next_step)\n",
    "            delta = r - R_bar + v_hat_next_step - v_hat_step\n",
    "            R_bar = R_bar + r_alpha * delta\n",
    "\n",
    "\n",
    "            gradient_w1 = -np.sin(abs(W[str(step)][0]) + abs(W[str(step)][1])) * (W[str(step)][0]/abs(W[str(step)][0]))\n",
    "            gradient_w2 = -np.sin(abs(W[str(step)][1]) + abs(W[str(step)][0])) * (W[str(step)][1]/abs(W[str(step)][1]))\n",
    "\n",
    "            w_Z[str(step)][str(done_action)][0] =  w_lambda * w_Z[str(step)][str(done_action)][0] + gradient_w1\n",
    "            w_Z[str(step)][str(done_action)][1] =  w_lambda * w_Z[str(step)][str(done_action)][1] + gradient_w2\n",
    "            \n",
    "\n",
    "            softmax_denominator = 0.0001\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                softmax_denominator = softmax_denominator +\\\n",
    "                    -np.sin(Theta[str(step)][action][0] + Theta[str(step)][action][1]) *\\\n",
    "                math.exp(np.cos(Theta[str(step)][action][0] + Theta[str(step)][action][1]))\n",
    "\n",
    "            #print('softmax_denominators',softmax_denominator)\n",
    "            gradient_t1 = (-np.sin(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]) * (Theta[str(step)][str(done_action)][0]/abs(Theta[str(step)][str(done_action)][0])) *\\\n",
    "            math.exp(np.cos(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]))) - softmax_denominator\n",
    "\n",
    "            gradient_t2 = (-np.sin(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]) * (Theta[str(step)][str(done_action)][1]/abs(Theta[str(step)][str(done_action)][1])) *\\\n",
    "            math.exp(np.cos(Theta[str(step)][str(done_action)][0] + Theta[str(step)][str(done_action)][1]))) - softmax_denominator\n",
    "\n",
    "            t_Z[str(step)][str(done_action)][0] =  t_lambda * t_Z[str(step)][str(done_action)][0] +  gradient_t1\n",
    "            t_Z[str(step)][str(done_action)][1] =  t_lambda * t_Z[str(step)][str(done_action)][1] +  gradient_t2\n",
    "\n",
    "\n",
    "            W[str(step)][0] = W[str(step)][0] + w_alpha * delta * w_Z[str(step)][str(done_action)][0]\n",
    "\n",
    "            W[str(step)][1] = W[str(step)][1] + w_alpha * delta * w_Z[str(step)][str(done_action)][1]\n",
    "\n",
    "            \n",
    "            V[str(step)] = V[str(step)] + np.cos(abs(W[str(step)][0]) + abs(W[str(step)][1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #print('gradient',gradient)\n",
    "                \n",
    "            t1 = Theta[str(step)][str(done_action)][0] +\\\n",
    "                t_alpha * delta * t_Z[str(step)][str(done_action)][0]\n",
    "            \n",
    "            t2 = Theta[str(step)][str(done_action)][1] +\\\n",
    "                t_alpha * delta * t_Z[str(step)][str(done_action)][1]\n",
    "\n",
    "            Theta[str(step)][str(done_action)] = [t1,t2]\n",
    "\n",
    "            Q[str(step)][str(done_action)] = np.cos(abs(t1)+abs(t2))\n",
    "\n",
    "    \n",
    "    for state in environment[6]:\n",
    "\n",
    "        if str(state) in list(Q.keys()):\n",
    "\n",
    "            value_action_state = reverse_dictionary(Q[str(state)])\n",
    "            Max_val = max(list(value_action_state.keys()))\n",
    "            best_action = value_action_state[Max_val]\n",
    "            best_action = ast.literal_eval(best_action)\n",
    "            next_state = [x + y for x, y in zip(state, best_action)]\n",
    "\n",
    "            if next_state not in environment[4] and next_state in environment[6]:\n",
    "\n",
    "                Optimal_Policy[str(state)] = next_state\n",
    "\n",
    "            else:\n",
    "\n",
    "                Optimal_Policy[str(state)] = state\n",
    "\n",
    "\n",
    "\n",
    "    return V, Q, Optimal_Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': -9537.358598356795,\n",
       "  '[0, 2]': -33013.95947753112,\n",
       "  '[0, 3]': -115298.72837895376,\n",
       "  '[1, 0]': -23625.51886426553,\n",
       "  '[1, 1]': -51091.70222123474,\n",
       "  '[1, 2]': -70144.72982007058,\n",
       "  '[1, 3]': -86856.12667171471,\n",
       "  '[2, 1]': -30764.326726770818,\n",
       "  '[2, 2]': -18167.48556223864,\n",
       "  '[2, 3]': -11180.351199213515,\n",
       "  '[3, 1]': -1308.4932121574852,\n",
       "  '[3, 3]': -1303.4130824263973,\n",
       "  '[4, 0]': -461.47098533403175,\n",
       "  '[4, 1]': -126.95997859395781,\n",
       "  '[4, 2]': -51.452545155353114,\n",
       "  '[4, 3]': 0},\n",
       " {'[0, 1]': {'[1, 0]': -0.9913036104874409,\n",
       "   '[-1, 0]': -0.9979499910385808,\n",
       "   '[0, 1]': 0.8996328057517945,\n",
       "   '[0, -1]': 0.7380657960431292},\n",
       "  '[0, 2]': {'[1, 0]': -0.9461479686241047,\n",
       "   '[-1, 0]': -0.5799126115615092,\n",
       "   '[0, 1]': 0.9471481347759487,\n",
       "   '[0, -1]': 0.5652217362417044},\n",
       "  '[0, 3]': {'[1, 0]': 0.6857748319118119,\n",
       "   '[-1, 0]': -0.0022510836229773933,\n",
       "   '[0, 1]': 0.052745396270233946,\n",
       "   '[0, -1]': 0.37000012819363953},\n",
       "  '[1, 0]': {'[1, 0]': -0.1506198515145001,\n",
       "   '[-1, 0]': 0.059435731773732944,\n",
       "   '[0, 1]': -0.9336311290854282,\n",
       "   '[0, -1]': -0.7322348037326587},\n",
       "  '[1, 1]': {'[1, 0]': -0.9984693652135882,\n",
       "   '[-1, 0]': -0.954137977110856,\n",
       "   '[0, 1]': 0.3855932277924863,\n",
       "   '[0, -1]': -0.9006301128077195},\n",
       "  '[1, 2]': {'[1, 0]': 0.4919848907327106,\n",
       "   '[-1, 0]': 0.2498899905760503,\n",
       "   '[0, 1]': -0.8559008187824618,\n",
       "   '[0, -1]': 0.7060750642909469},\n",
       "  '[1, 3]': {'[1, 0]': -0.9172267187816024,\n",
       "   '[-1, 0]': -0.9722562262838643,\n",
       "   '[0, 1]': -0.8848997370201085,\n",
       "   '[0, -1]': -0.9989203054967839},\n",
       "  '[2, 1]': {'[1, 0]': -0.8523366719132554,\n",
       "   '[-1, 0]': -0.8799570635859865,\n",
       "   '[0, 1]': -0.06216473220785793,\n",
       "   '[0, -1]': -0.869814754572105},\n",
       "  '[2, 2]': {'[1, 0]': -0.5960892239774175,\n",
       "   '[-1, 0]': -0.9896459884133526,\n",
       "   '[0, 1]': -0.48824115497633364,\n",
       "   '[0, -1]': 0.9579187495561322},\n",
       "  '[2, 3]': {'[1, 0]': 0.9899293346580886,\n",
       "   '[-1, 0]': 0.7665339284175254,\n",
       "   '[0, 1]': -0.8774647670079596,\n",
       "   '[0, -1]': -0.5310410694392558},\n",
       "  '[3, 1]': {'[1, 0]': 0.04093681104452307,\n",
       "   '[-1, 0]': 0.708497072889624,\n",
       "   '[0, 1]': -0.9933311303868189,\n",
       "   '[0, -1]': -0.10859022828618133},\n",
       "  '[3, 3]': {'[1, 0]': 0.9997791984266403,\n",
       "   '[-1, 0]': -0.6136470357138567,\n",
       "   '[0, 1]': 0.1688308234768134,\n",
       "   '[0, -1]': -0.5875848834241616},\n",
       "  '[4, 0]': {'[1, 0]': -0.9193277872835642,\n",
       "   '[-1, 0]': 0.6249783347461468,\n",
       "   '[0, 1]': 0.42163970428616904,\n",
       "   '[0, -1]': 0.14765032310850046},\n",
       "  '[4, 1]': {'[1, 0]': -0.9400846838839295,\n",
       "   '[-1, 0]': 0.9272631065214462,\n",
       "   '[0, 1]': 0.5734354100683492,\n",
       "   '[0, -1]': -0.2841667118415936},\n",
       "  '[4, 2]': {'[1, 0]': -0.7451898866032911,\n",
       "   '[-1, 0]': 0.8938651645031667,\n",
       "   '[0, 1]': -0.22088075389956333,\n",
       "   '[0, -1]': -0.5000383462515499},\n",
       "  '[4, 3]': {'[1, 0]': 6.195753385441538e-09,\n",
       "   '[-1, 0]': 6.000902944159857e-09,\n",
       "   '[0, 1]': 6.45625910920527e-09,\n",
       "   '[0, -1]': 2.258995730760941e-09}},\n",
       " {'[0, 1]': [0, 2],\n",
       "  '[0, 2]': [0, 3],\n",
       "  '[0, 3]': [1, 3],\n",
       "  '[1, 0]': [1, 0],\n",
       "  '[1, 1]': [1, 2],\n",
       "  '[1, 2]': [1, 1],\n",
       "  '[1, 3]': [1, 3],\n",
       "  '[2, 1]': [2, 2],\n",
       "  '[2, 2]': [2, 1],\n",
       "  '[2, 3]': [3, 3],\n",
       "  '[3, 1]': [2, 1],\n",
       "  '[3, 3]': [4, 3],\n",
       "  '[4, 0]': [4, 0],\n",
       "  '[4, 1]': [3, 1],\n",
       "  '[4, 2]': [4, 2],\n",
       "  '[4, 3]': [4, 3]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eligibility_traces_actor_critic_continuing(100, 0.5, 0.5, 0.9, 'deterministic',0.2,0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
