{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_world(length, width,path_lenght,holes_number,Random_State):\n",
    "    \n",
    "    random.seed(Random_State)\n",
    "    #store all cells in a list\n",
    "    Grid_Cells = []\n",
    "    for row in range(length):\n",
    "        for col in range(width):\n",
    "            Grid_Cells.append([row,col])\n",
    "\n",
    "\n",
    "    #specify the number of holes in the gridworld\n",
    "    \n",
    "    #specify the start point as a random cell\n",
    "    start = [random.randint(0, length), random.randint(0, width)]\n",
    "\n",
    "    #create a path from start point\n",
    "    \"\"\"instead of defining start and goal points,\n",
    "      we define just a start point and a random path with a random lenght to\n",
    "       another point and name it as goal point\"\"\"\n",
    "    \n",
    "    def random_path(Start, Path_Lenght,length, width):\n",
    "        \n",
    "        Path = []\n",
    "        Path.append(Start)\n",
    "        for i in range(Path_Lenght):\n",
    "            \n",
    "            #there are two moves that take us on a random cell named Goal [1,0], [0,1]\n",
    "            \n",
    "            move = random.choice([[1,0], [0,1]])\n",
    "            \n",
    "            #update the start cell/point by the above move\n",
    "            Start = [x + y for x, y in zip(Start, move)]\n",
    "            \n",
    "            #if the movement take us out of our gridworld, we reverse the change in the start point\n",
    "            if Start[0] < 0 or Start[1] < 0 or Start[0] > length-1 or Start[1] > width-1:\n",
    "\n",
    "                Start = [x - y for x, y in zip(Start, move)]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                #create a path history\n",
    "                Path.append(Start)\n",
    "\n",
    "        Goal = Start\n",
    "\n",
    "        return Goal,Path\n",
    "    \n",
    "\n",
    "    GoalPath = random_path(start, path_lenght,length, width)\n",
    "\n",
    "    goal = GoalPath[0]\n",
    "    path = GoalPath[1]\n",
    "\n",
    "    #now we must eliminate the path cells from the Grid_Cells to choose hole cells from remaining cells\n",
    "\n",
    "    FreeCells = [x for x in Grid_Cells if x not in path]\n",
    "\n",
    "    Holes = random.sample(FreeCells, holes_number)\n",
    "\n",
    "    #Also, we can visualize our gridworld in a simple way\n",
    "\n",
    "    def mark_holes(holes):\n",
    "        marked_data = [[\"Hole\" if [row, col] in holes else [row, col] for col in range(width)] for row in range(length)]\n",
    "        return marked_data\n",
    "    \n",
    "    marked_matrix = mark_holes(Holes)\n",
    "\n",
    "    print(tabulate(marked_matrix, tablefmt=\"grid\"))\n",
    "\n",
    "    \n",
    "    return length, width, start, goal, Holes, path,Grid_Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_distribution(grid_size,randomness):\n",
    "    random.seed(40)\n",
    "    \n",
    "    #by this function we generate probabilities which their sum is equal to 1\n",
    "    def generate_probabilities(n):\n",
    "\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "        \n",
    "        return scaled_numbers\n",
    "    \n",
    "    cells_prob = {}\n",
    "    if randomness == 'stochastic':\n",
    "        for cell in range(grid_size):\n",
    "            \n",
    "            #we set the number of probs to 4 due to 4 possible action for each cell (go to its neighbors)\n",
    "            probs = generate_probabilities(4)\n",
    "\n",
    "            cells_prob[cell] = probs\n",
    "    elif randomness == 'equal probable':\n",
    "\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.25,0.25,0.25,0.25]\n",
    "    \n",
    "    elif randomness == 'deterministic':\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0,0,0,1]#[0.15,.15,0.1,0.6]\n",
    "\n",
    "\n",
    "    #Note that we consider the correspondence between probabilities and actions as below:\n",
    "    #probs = [p1, p2, p3, p4] ---> [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    return cells_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function specify the 4 neighbors cells around one arbitrary cell in the gridworld\n",
    "#we need this function to prevent redundant computstion in the Bellman equation\n",
    "def neighbor_cells(cell):\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    Neighbors = []\n",
    "\n",
    "    for action in Actions:\n",
    "\n",
    "        neighbor = [x + y for x, y in zip(cell, action)]\n",
    "        #if neighbor not in environment[4]:\n",
    "        Neighbors.append(neighbor)\n",
    "\n",
    "    return Neighbors\n",
    "\n",
    "def policy_evaluation(threshold,gamma,randomness):\n",
    "    random.seed(randomness)\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "    \n",
    "    grid_cells = environment[6]\n",
    "\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    prob_dist = probability_distribution(environment[0]*environment[1],randomness)\n",
    "    \n",
    "    #this dictionary stores state indices with their linear number; [0,0] = 0 ,..., [m,n] = m.n \n",
    "    indice_state_dict = {}\n",
    "    for num in range(grid_size):\n",
    "\n",
    "        indice_state_dict[num] = grid_cells[num]\n",
    "    \n",
    "\n",
    "    state_indice_dict = {}\n",
    "    counter = 0\n",
    "    for state in grid_cells:\n",
    "\n",
    "        state = str(state)\n",
    "        state_indice_dict[state] = counter\n",
    "        counter = counter + 1\n",
    "\n",
    "    #initialize the state values with zeros - minus 1 is because of terminal state or goal\n",
    "    V_old = np.zeros((grid_size , 1))\n",
    "    V_new = np.zeros((grid_size , 1))\n",
    "    \n",
    "    Counter = 0\n",
    "    delta = threshold + 0.2\n",
    "    while delta >= threshold:\n",
    "        Delta = []\n",
    "        for state_num in range(grid_size):\n",
    "\n",
    "            state = indice_state_dict[state_num]\n",
    "            #state_indice = '{}{}'.format(row,col)\n",
    "\n",
    "            if state not in environment[4]: \n",
    "\n",
    "                first_sigma = 0\n",
    "                #print('come to loop',state)\n",
    "            \n",
    "                for action in range(4):\n",
    "                    #print(action)\n",
    "\n",
    "\n",
    "                    #as we are working on random policy the pi(a|s) = 1/4; equal probable\n",
    "                    pi = 1/4\n",
    "\n",
    "                    neighbors = neighbor_cells(state)\n",
    "                    #print(neighbors)\n",
    "                    \n",
    "                    #second sigma in the Bellman equation\n",
    "                    \n",
    "                    intended_state = [x + y for x, y in zip(state, Actions[action])]\n",
    "                    #inverse_state = \n",
    "                    second_sigma = 0\n",
    "                    second_sigma_list = []\n",
    "                    for neighbor in neighbors:\n",
    "                        #print('third loop')\n",
    "\n",
    "                        prob_list = prob_dist[state_num].copy()\n",
    "\n",
    "                        if neighbor == intended_state:\n",
    "\n",
    "                            #print(prob_list)\n",
    "                            p = max(prob_list)\n",
    "                            prob_list.remove(p)\n",
    "                            #print(p)\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            p = random.choice(prob_dist[state_num]) #prob_dist[state_num][action-1]\n",
    "                            #print(p)\n",
    "                        \n",
    "                        if neighbor in grid_cells:\n",
    "                            indice = state_indice_dict[str(neighbor)]\n",
    "                        \n",
    "                        #if the agent reach the goal, we eliminate the -1 reward\n",
    "                        if intended_state == environment[3]:\n",
    "\n",
    "                            second_sigma = second_sigma + p*(100000+gamma*V_old[indice])\n",
    "                        \n",
    "                        #in this part we dedicated a very large negative reward if the agent drop on a hole\n",
    "                        elif neighbor in environment[4]:\n",
    "\n",
    "                            second_sigma = second_sigma + p*(-2 + gamma*V_old[indice])\n",
    "                        #in other states, which are not the teriminal state or holes; reward = -1\n",
    "\n",
    "                        #elif neighbor not in grid_cells:\n",
    "\n",
    "                        #   second_sigma = second_sigma + p*(-1)\n",
    "\n",
    "                        else:\n",
    "                            second_sigma = second_sigma + p*(-1 + gamma*V_old[indice])\n",
    "\n",
    "                    second_sigma_list.append(pi * second_sigma)\n",
    "                    \n",
    "                first_sigma = sum(second_sigma_list) #first_sigma + pi * second_sigma_list[action]\n",
    "                \n",
    "                V_new[state_num] = first_sigma\n",
    "                #if state == [3,3]:\n",
    "\n",
    "                #    print('[3,3]:',first_sigma,second_sigma_list)\n",
    "            \n",
    "        \n",
    "        \n",
    "            #if Counter == 0:\n",
    "\n",
    "            delta_ = max([0,np.abs(V_new[state_num] - V_old[state_num])])\n",
    "            Delta.append(delta_)\n",
    "\n",
    "                #Counter = Counter + 1\n",
    "\n",
    "            \"\"\"else:\n",
    "\n",
    "                delta_ = max([delta_,np.abs(V_new[state_num] - V_old[state_num])])\n",
    "                Delta.append(delta_)\n",
    "\n",
    "                Counter = Counter + 1\"\"\"\n",
    "            #print(V_old)\n",
    "            #print('=====')\n",
    "            #print(V_new)\n",
    "            V_old[state_num] = V_new[state_num]\n",
    "        \n",
    "        delta = max(Delta)\n",
    "        #print(delta)\n",
    "        #print(Delta)\n",
    "        #print('delta:',delta)\n",
    "        #print(Counter)\n",
    "            \n",
    "    Final_dict = {}\n",
    "    for state in grid_cells:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            Final_dict[str(state)] = V_new[state_indice_dict[str(state)]]\n",
    "\n",
    "    return Final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+\n",
      "| Hole   | [0, 1] | [0, 2] | [0, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [1, 0] | [1, 1] | [1, 2] | [1, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [2, 1] | [2, 2] | [2, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [3, 1] | Hole   | [3, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [4, 0] | [4, 1] | [4, 2] | [4, 3] |\n",
      "+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "environment = generate_grid_world(5, 4,4,4,39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 4,\n",
       " [1, 2],\n",
       " [4, 3],\n",
       " [[2, 0], [3, 2], [3, 0], [0, 0]],\n",
       " [[1, 2], [1, 3], [2, 3], [3, 3], [4, 3]],\n",
       " [[0, 0],\n",
       "  [0, 1],\n",
       "  [0, 2],\n",
       "  [0, 3],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [3, 0],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [4, 0],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': array([-0.42739015]),\n",
       " '[0, 2]': array([-0.3391511]),\n",
       " '[0, 3]': array([-0.29654947]),\n",
       " '[1, 0]': array([-0.7117473]),\n",
       " '[1, 1]': array([-0.44379083]),\n",
       " '[1, 2]': array([-0.44164117]),\n",
       " '[1, 3]': array([-0.39776595]),\n",
       " '[2, 1]': array([-0.50266239]),\n",
       " '[2, 2]': array([-0.5087653]),\n",
       " '[2, 3]': array([-0.45427718]),\n",
       " '[3, 1]': array([-0.55191182]),\n",
       " '[3, 3]': array([-0.40537271]),\n",
       " '[4, 0]': array([-0.63577404]),\n",
       " '[4, 1]': array([-0.49770679]),\n",
       " '[4, 2]': array([-0.40501756]),\n",
       " '[4, 3]': array([-0.41718401])}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = policy_evaluation(0.09,0.9,'stochastic')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_path(StateValue):\n",
    "\n",
    "    start = environment[2]\n",
    "    goal = environment[3]\n",
    "\n",
    "    path = []\n",
    "\n",
    "    #neighbors = neighbor_cells(start)\n",
    "    \n",
    "    next_move = start\n",
    "    ex_move = []\n",
    "    counter = 0\n",
    "    checked_states = []\n",
    "    \n",
    "    while goal not in neighbor_cells(next_move):\n",
    "\n",
    "        neighbor_values = {}\n",
    "        \"\"\"if counter != 0:\n",
    "            Neighbors_ = neighbor_cells(next_move)\n",
    "            Neighbors = Neighbors_.copy()\n",
    "            for neighbor in Neighbors:\n",
    "\n",
    "                if neighbor in checked_states:\n",
    "\n",
    "                    Neighbors.remove(neighbor)\n",
    "            #print(counter, Neighbors)\n",
    "        \n",
    "        else:\n",
    "            Neighbors = neighbor_cells(next_move).copy()\"\"\"\n",
    "        \n",
    "        #for neighbor in\n",
    "        Neighbors = neighbor_cells(next_move)\n",
    "\n",
    "        Allowed_Neighbors = Neighbors.copy()\n",
    "\n",
    "        for neighbor in Allowed_Neighbors:\n",
    "\n",
    "            if neighbor in checked_states:\n",
    "\n",
    "                    Allowed_Neighbors.remove(neighbor)\n",
    "\n",
    "\n",
    "\n",
    "        for neighbor in Allowed_Neighbors:\n",
    "            \n",
    "            if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "\n",
    "                value = StateValue[str(neighbor)][0]\n",
    "\n",
    "                neighbor_values[value] = neighbor\n",
    "                #checked_states.append(neighbor)\n",
    "        #ex_move = next_move\n",
    "        \"\"\"for state in Neighbors:\n",
    "\n",
    "            if state in environment[6]:\n",
    "\n",
    "                checked_states.append(state)\"\"\"\n",
    "        print(neighbor_values)\n",
    "\n",
    "        maximum_value = max(list(neighbor_values.keys()))\n",
    "\n",
    "        next_move = neighbor_values[maximum_value]\n",
    "\n",
    "        checked_states.append(next_move)\n",
    "\n",
    "        print(next_move)\n",
    "\n",
    "        path.append(next_move)\n",
    "        \n",
    "        counter += 1\n",
    "    path.append(environment[3])\n",
    "    return path\n",
    "\n",
    "    if goal in neighbor_cells(next_move):\n",
    "\n",
    "        return \"Just one step\" #should be edited later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-0.5087653025306074: [2, 2], -0.33915110215051236: [0, 2], -0.39776594606584725: [1, 3], -0.44379083226413846: [1, 1]}\n",
      "[0, 2]\n",
      "{-0.4416411695121747: [1, 2], -0.2965494745582413: [0, 3], -0.42739014841143563: [0, 1]}\n",
      "[0, 3]\n",
      "{-0.39776594606584725: [1, 3]}\n",
      "[1, 3]\n",
      "{-0.45427718375141346: [2, 3], -0.4416411695121747: [1, 2]}\n",
      "[1, 2]\n",
      "{-0.5087653025306074: [2, 2], -0.39776594606584725: [1, 3], -0.44379083226413846: [1, 1]}\n",
      "[1, 3]\n",
      "{-0.45427718375141346: [2, 3]}\n",
      "[2, 3]\n",
      "{-0.4053727058245691: [3, 3], -0.5087653025306074: [2, 2]}\n",
      "[3, 3]\n",
      "Optimal Path to the Goal:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 2], [0, 3], [1, 3], [1, 2], [1, 3], [2, 3], [3, 3], [4, 3]]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OptimalPath = find_optimal_path(result)\n",
    "\n",
    "print('Optimal Path to the Goal:')\n",
    "OptimalPath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': array([-0.42739015]),\n",
       " '[0, 2]': array([-0.3391511]),\n",
       " '[0, 3]': array([-0.29654947]),\n",
       " '[1, 0]': array([-0.7117473]),\n",
       " '[1, 1]': array([-0.44379083]),\n",
       " '[1, 2]': array([-0.44164117]),\n",
       " '[1, 3]': array([-0.39776595]),\n",
       " '[2, 1]': array([-0.50266239]),\n",
       " '[2, 2]': array([-0.5087653]),\n",
       " '[2, 3]': array([-0.45427718]),\n",
       " '[3, 1]': array([-0.55191182]),\n",
       " '[3, 3]': array([-0.40537271]),\n",
       " '[4, 0]': array([-0.63577404]),\n",
       " '[4, 1]': array([-0.49770679]),\n",
       " '[4, 2]': array([-0.40501756]),\n",
       " '[4, 3]': array([-0.41718401])}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_values = policy_evaluation(0.09,0.9,'stochastic')\n",
    "states_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "\n",
    "    Neighbors = neighbor_cells(state)\n",
    "\n",
    "    neighbor_values = {}\n",
    "\n",
    "    for neighbor in Neighbors:\n",
    "\n",
    "        if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "\n",
    "            neighbor_values[states_values['{}'.format(neighbor)][0]] = '{}'.format(neighbor)\n",
    "    \n",
    "    best_val = max(list(neighbor_values.keys()))\n",
    "    best_neighbor = neighbor_values[best_val]\n",
    "\n",
    "    return best_neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(threshold,gamma,randomness,state_prob_type):\n",
    "    \n",
    "    random.seed(randomness)\n",
    "    #V = np.zeros((grid_size , 1))\n",
    "\n",
    "    def arbitrary_policy(randomness):\n",
    "        random.seed(randomness)\n",
    "        \n",
    "        policy = {}\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                neighbors = neighbor_cells(state)\n",
    "\n",
    "                allowed_positions = []\n",
    "\n",
    "                for neighbor in neighbors:\n",
    "                    \n",
    "                    if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                        \n",
    "                        allowed_positions.append(neighbor)\n",
    "            \n",
    "                next_state = random.choice(allowed_positions)\n",
    "        \n",
    "                policy['{}'.format(state)] = next_state\n",
    "\n",
    "        return policy\n",
    "    \n",
    "    policy_0 = arbitrary_policy(42)\n",
    "    #print(policy_0)\n",
    "\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "\n",
    "    prob_dist = probability_distribution(environment[0]*environment[1],state_prob_type)\n",
    "    \n",
    "    #this dictionary stores state indices with their linear number; [0,0] = 0 ,..., [m,n] = m.n \n",
    "    indice_state_dict = {}\n",
    "    for num in range(grid_size):\n",
    "\n",
    "        indice_state_dict[num] = grid_cells[num]\n",
    "    \n",
    "\n",
    "    state_indice_dict = {}\n",
    "    counter = 0\n",
    "    for state in grid_cells:\n",
    "\n",
    "        state = str(state)\n",
    "        state_indice_dict[state] = counter\n",
    "        counter = counter + 1   \n",
    "\n",
    "    def PolicyEvaluation(policy,threshold,gamma,randomness):\n",
    "            \n",
    "        random.seed(randomness)\n",
    "\n",
    "        #initialize the state values with zeros - minus 1 is because of terminal state or goal\n",
    "        V_old = np.zeros((grid_size , 1))\n",
    "        V_new = np.zeros((grid_size , 1))\n",
    "        \n",
    "        delta = threshold + 0.2\n",
    "        while delta >= threshold:\n",
    "            Delta = []\n",
    "            for state_num in range(grid_size):\n",
    "\n",
    "                state = indice_state_dict[state_num]\n",
    "                #state_indice = '{}{}'.format(row,col)\n",
    "\n",
    "                if state not in environment[4]: \n",
    "\n",
    "                    #first_sigma = 0\n",
    "                    \n",
    "                    neighbors = neighbor_cells(state)\n",
    "                        \n",
    "                    #intended_state = [x + y for x, y in zip(state, policy(state))]\n",
    "                    \n",
    "                    intended_state = policy['{}'.format(state)]\n",
    "                    \n",
    "\n",
    "                    second_sigma = 0\n",
    "                    second_sigma_list = []\n",
    "                    for neighbor in neighbors:\n",
    "\n",
    "                        prob_list = prob_dist[state_num].copy()\n",
    "\n",
    "                        maximum_p = max(prob_list)\n",
    "                        prob_list.remove(maximum_p)\n",
    "\n",
    "                        if neighbor == intended_state:\n",
    "\n",
    "                            p = maximum_p\n",
    "                            #prob_list.remove(p)\n",
    "\n",
    "                            \"\"\"else:\n",
    "\n",
    "                            if len(prob_list) == 4:\n",
    "                                p = max(prob_list)\n",
    "                                prob_list.remove(p)\n",
    "\n",
    "                            p = random.choice(prob_dist[state_num]) #prob_dist[state_num][action-1]\"\"\"\n",
    "                        \n",
    "                        else:\n",
    "                        \n",
    "                            p = random.choice(prob_list)\n",
    "\n",
    "                            #print(p)\n",
    "                        \n",
    "                        if neighbor in grid_cells:\n",
    "                            indice = state_indice_dict[str(neighbor)]\n",
    "                            #print(indice)\n",
    "                        \n",
    "                        #if the agent reach the goal, we eliminate the -1 reward\n",
    "                        if intended_state == environment[3]:\n",
    "\n",
    "                            second_sigma = second_sigma + p*(10+gamma*V_old[indice])\n",
    "                            #print('goal:',second_sigma)\n",
    "\n",
    "                        \n",
    "                        #in this part we dedicated a very large negative reward if the agent drop on a hole\n",
    "                        elif neighbor in environment[4]:\n",
    "\n",
    "                            second_sigma = second_sigma + p*(-3 + gamma*V_old[indice])\n",
    "                            #print('hole:',second_sigma)\n",
    "                        #in other states, which are not the teriminal state or holes; reward = -1\n",
    "\n",
    "                        #elif neighbor not in grid_cells:\n",
    "\n",
    "                        #    second_sigma = second_sigma + p*(-2)\n",
    "                            #print('out:',second_sigma)\n",
    "\n",
    "                        else:\n",
    "                            second_sigma = second_sigma + p*(-1 + gamma*V_old[indice])\n",
    "                            #print('in:',second_sigma)\n",
    "\n",
    "                        #second_sigma_list.append(second_sigma)\n",
    "                    \n",
    "                    #first_sigma = sum(second_sigma_list) #first_sigma + pi * second_sigma_list[action]\n",
    "                    \n",
    "                    #V_new[state_num] = sum(second_sigma_list) #first_sigma\n",
    "                    V_new[state_num] = second_sigma # max(V_new[state_num], second_sigma)\n",
    "\n",
    "                delta_ = max([0,np.abs(V_new[state_num] - V_old[state_num])])\n",
    "                Delta.append(delta_)\n",
    "\n",
    "                V_old[state_num] = V_new[state_num]\n",
    "                \n",
    "            delta = max(Delta)\n",
    "            #print('delta',delta)\n",
    "                \n",
    "        Final_dict = {}\n",
    "        for state in grid_cells:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "\n",
    "                Final_dict[str(state)] = V_new[state_indice_dict[str(state)]]\n",
    "\n",
    "        return Final_dict\n",
    "                       \n",
    "\n",
    "\n",
    "\n",
    "    def policy_improvement(policy,threshold,gamma,randomness):\n",
    "\n",
    "        policy_stable = False\n",
    "        policy = policy_0\n",
    "        c = 0\n",
    "        while policy_stable == False:\n",
    "\n",
    "            State_Values = PolicyEvaluation(policy,threshold,gamma,randomness)\n",
    "            #print('sv',State_Values)\n",
    "\n",
    "            #print(max(list(State_Values.values())))\n",
    "            #print(c)\n",
    "            for state_num in range(grid_size):\n",
    "\n",
    "                state = indice_state_dict[state_num]\n",
    "                #state_indice = '{}{}'.format(row,col)\n",
    "\n",
    "                if state not in environment[4]:\n",
    "                    string_state = '{}'.format(state)\n",
    "\n",
    "                    first_sigma = 0\n",
    "                    \n",
    "                    neighbors = neighbor_cells(state)\n",
    "                        \n",
    "                    #intended_state = [x + y for x, y in zip(state, policy(state))]\n",
    "                    old_policy = policy\n",
    "                    intended_state = policy[string_state]\n",
    "\n",
    "                    second_sigma = 0\n",
    "                    second_sigma_dict = {}\n",
    "                    best_value = float(\"-inf\")  # Initialize the best value to negative infinity\n",
    "                    best_neighbor = None\n",
    "\n",
    "                    for neighbor in neighbors:\n",
    "\n",
    "                        if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "\n",
    "                            prob_list = prob_dist[state_num].copy()\n",
    "                            #print(prob_list)\n",
    "                            #print(neighbor)\n",
    "                            #print(State_Values)\n",
    "\n",
    "                            maximum_p = max(prob_list)\n",
    "                            prob_list.remove(maximum_p)\n",
    "\n",
    "                            if neighbor == intended_state:\n",
    "\n",
    "                                p = maximum_p\n",
    "                                #prob_list.remove(p)\n",
    "\n",
    "                            else:\n",
    "\n",
    "                                p = random.choice(prob_list) #prob_dist[state_num][action-1]\n",
    "                                #print(p)\n",
    "                            \n",
    "                            if neighbor in grid_cells:\n",
    "                                indice = state_indice_dict[str(neighbor)]\n",
    "                            #print(intended_state , environment[3])\n",
    "                            #if the agent reach the goal, we eliminate the -1 reward\n",
    "                            if intended_state == str(environment[3]):\n",
    "                                #print(type(intended_state) , type(environment[3]))\n",
    "\n",
    "                                second_sigma = second_sigma + p*(10 + gamma*State_Values[string_state])\n",
    "                            \n",
    "                            #in this part we dedicated a very large negative reward if the agent drop on a hole\n",
    "                            elif neighbor in environment[4]:\n",
    "\n",
    "                                second_sigma = second_sigma + p*(-3 + gamma*State_Values[string_state])\n",
    "                            #in other states, which are not the teriminal state or holes; reward = -1\n",
    "\n",
    "                            #elif neighbor not in grid_cells:\n",
    "\n",
    "                            #    second_sigma = second_sigma + p*(-2)\n",
    "\n",
    "                            else:\n",
    "                                second_sigma = second_sigma + p*(-1 + gamma*State_Values[string_state])\n",
    "\n",
    "                            #print(second_sigma,neighbor)\n",
    "                            #second_sigma = second_sigma[0]\n",
    "                            #second_sigma_dict[second_sigma] = neighbor\n",
    "                        \n",
    "                            if type(second_sigma) == float:\n",
    "                                val = round(second_sigma,5)\n",
    "                            else:\n",
    "                                val = round(second_sigma[0],5)\n",
    "                            second_sigma_dict[val] = str(neighbor)\n",
    "\n",
    "                    maximum_value = max(list(second_sigma_dict.keys()))\n",
    "                    best_neighbor = second_sigma_dict[maximum_value]\n",
    "\n",
    "                    if environment[3] in neighbors:\n",
    "\n",
    "                        policy[string_state] = str(environment[3])\n",
    "                    else:\n",
    "\n",
    "                        policy[string_state] = best_neighbor\n",
    "\n",
    "                    #if state == [4,2]:\n",
    "                        #print(second_sigma_dict)\n",
    "                        #print('policy:',policy)\n",
    "\n",
    "                    if old_policy == policy:\n",
    "                        policy_stable = True\n",
    "\n",
    "                    if str(intended_state) != best_neighbor:\n",
    "                        #print(intended_state,best_neighbor)\n",
    "                        \n",
    "                        policy_stable = False\n",
    "                            \n",
    "                        \"\"\"if second_sigma > best_value:\n",
    "                            best_value = second_sigma\n",
    "                            best_neighbor = neighbor\"\"\"\n",
    "            c +=1\n",
    "\n",
    "\n",
    "                    #if policy[string_state] != best_neighbor:\n",
    "\n",
    "                    #   policy_stable = False\n",
    "                \n",
    "            \"\"\"V_star = {}\n",
    "                for state in grid_cells:\n",
    "\n",
    "                    if state not in environment[4]:\n",
    "\n",
    "                        V_star[str(state)] = V_new[state_indice_dict[str(state)]]\"\"\"\n",
    "        \n",
    "        if policy_stable == True:\n",
    "\n",
    "            return State_Values, policy\n",
    "    \n",
    "    optimals = policy_improvement(policy,threshold,gamma,randomness)\n",
    "    optimal_value = optimals[0]\n",
    "    optimal_value.pop(str(environment[3]))\n",
    "    \n",
    "    optimal_policy = optimals[1]\n",
    "    optimal_policy.pop(str(environment[3]))\n",
    "\n",
    "    return optimal_value, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': array([-3.47613432]),\n",
       "  '[0, 2]': array([-4.07857832]),\n",
       "  '[0, 3]': array([-3.24017462]),\n",
       "  '[1, 0]': array([-2.88432395]),\n",
       "  '[1, 1]': array([-3.24337309]),\n",
       "  '[1, 2]': array([-3.42493393]),\n",
       "  '[1, 3]': array([-3.19244958]),\n",
       "  '[2, 1]': array([-3.36094248]),\n",
       "  '[2, 2]': array([-3.38440215]),\n",
       "  '[2, 3]': array([-3.43502333]),\n",
       "  '[3, 1]': array([-2.53430381]),\n",
       "  '[3, 3]': array([-3.16614567]),\n",
       "  '[4, 0]': array([-2.13237478]),\n",
       "  '[4, 1]': array([-1.34044141]),\n",
       "  '[4, 2]': array([6.28292761])},\n",
       " {'[0, 1]': '[1, 1]',\n",
       "  '[0, 2]': '[1, 2]',\n",
       "  '[0, 3]': '[1, 3]',\n",
       "  '[1, 0]': '[1, 1]',\n",
       "  '[1, 1]': '[2, 1]',\n",
       "  '[1, 2]': '[2, 2]',\n",
       "  '[1, 3]': '[2, 3]',\n",
       "  '[2, 1]': '[3, 1]',\n",
       "  '[2, 2]': '[1, 2]',\n",
       "  '[2, 3]': '[3, 3]',\n",
       "  '[3, 1]': '[4, 1]',\n",
       "  '[3, 3]': '[4, 3]',\n",
       "  '[4, 0]': '[4, 1]',\n",
       "  '[4, 1]': '[3, 1]',\n",
       "  '[4, 2]': '[4, 3]'})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Optimal_Path_Policy = policy_iteration(0.9,.7,42,'stochastic')\n",
    "Optimal_Path_Policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35f9c1c23917d6611610273579d122557dc1a2ecb49c80f9f67567946a3a1fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
