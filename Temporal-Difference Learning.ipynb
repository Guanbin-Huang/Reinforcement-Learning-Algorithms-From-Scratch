{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_world(length, width,path_lenght,holes_number,Random_State):\n",
    "    \n",
    "    random.seed(Random_State)\n",
    "    #store all cells in a list\n",
    "    Grid_Cells = []\n",
    "    for row in range(length):\n",
    "        for col in range(width):\n",
    "            Grid_Cells.append([row,col])\n",
    "\n",
    "\n",
    "    #specify the number of holes in the gridworld\n",
    "    \n",
    "    #specify the start point as a random cell\n",
    "    start = [random.randint(0, length), random.randint(0, width)]\n",
    "\n",
    "    #create a path from start point\n",
    "    \"\"\"instead of defining start and goal points,\n",
    "      we define just a start point and a random path with a random lenght to\n",
    "       another point and name it as goal point\"\"\"\n",
    "    \n",
    "    def random_path(Start, Path_Lenght,length, width):\n",
    "        \n",
    "        Path = []\n",
    "        Path.append(Start)\n",
    "        for i in range(Path_Lenght):\n",
    "            \n",
    "            #there are two moves that take us on a random cell named Goal [1,0], [0,1]\n",
    "            \n",
    "            move = random.choice([[1,0], [0,1]])\n",
    "            \n",
    "            #update the start cell/point by the above move\n",
    "            Start = [x + y for x, y in zip(Start, move)]\n",
    "            \n",
    "            #if the movement take us out of our gridworld, we reverse the change in the start point\n",
    "            if Start[0] < 0 or Start[1] < 0 or Start[0] > length-1 or Start[1] > width-1:\n",
    "\n",
    "                Start = [x - y for x, y in zip(Start, move)]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                #create a path history\n",
    "                Path.append(Start)\n",
    "\n",
    "        Goal = Start\n",
    "\n",
    "        return Goal,Path\n",
    "    \n",
    "\n",
    "    GoalPath = random_path(start, path_lenght,length, width)\n",
    "\n",
    "    goal = GoalPath[0]\n",
    "    path = GoalPath[1]\n",
    "\n",
    "    #now we must eliminate the path cells from the Grid_Cells to choose hole cells from remaining cells\n",
    "\n",
    "    FreeCells = [x for x in Grid_Cells if x not in path]\n",
    "\n",
    "    Holes = random.sample(FreeCells, holes_number)\n",
    "\n",
    "    #Also, we can visualize our gridworld in a simple way\n",
    "\n",
    "    def mark_holes(holes):\n",
    "        marked_data = [[\"Hole\" if [row, col] in holes else [row, col] for col in range(width)] for row in range(length)]\n",
    "        return marked_data\n",
    "    \n",
    "    marked_matrix = mark_holes(Holes)\n",
    "\n",
    "    print(tabulate(marked_matrix, tablefmt=\"grid\"))\n",
    "\n",
    "    \n",
    "    return length, width, start, goal, Holes, path,Grid_Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+\n",
      "| Hole   | [0, 1] | [0, 2] | [0, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [1, 0] | [1, 1] | [1, 2] | [1, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [2, 1] | [2, 2] | [2, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [3, 1] | Hole   | [3, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [4, 0] | [4, 1] | [4, 2] | [4, 3] |\n",
      "+--------+--------+--------+--------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 4,\n",
       " [1, 2],\n",
       " [4, 3],\n",
       " [[2, 0], [3, 2], [3, 0], [0, 0]],\n",
       " [[1, 2], [1, 3], [2, 3], [3, 3], [4, 3]],\n",
       " [[0, 0],\n",
       "  [0, 1],\n",
       "  [0, 2],\n",
       "  [0, 3],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [3, 0],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [4, 0],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = generate_grid_world(5, 4,4,4,39)\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_distribution(grid_size,randomness):\n",
    "    #random.seed(40)\n",
    "    \n",
    "    #by this function we generate probabilities which their sum is equal to 1\n",
    "    def generate_probabilities(n):\n",
    "\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "        \n",
    "        return scaled_numbers\n",
    "    \n",
    "    cells_prob = {}\n",
    "    if randomness == 'stochastic':\n",
    "        for cell in range(grid_size):\n",
    "            \n",
    "            #we set the number of probs to 4 due to 4 possible action for each cell (go to its neighbors)\n",
    "            probs = generate_probabilities(4)\n",
    "\n",
    "            cells_prob[cell] = probs\n",
    "    elif randomness == 'equal probable':\n",
    "\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.25,0.25,0.25,0.25]\n",
    "    \n",
    "    elif randomness == 'deterministic':\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.03,0.06,0.01,0.9] #[0,0,0,1] ##[0.15,.15,0.1,0.6]\n",
    "\n",
    "\n",
    "    #Note that we consider the correspondence between probabilities and actions as below:\n",
    "    #probs = [p1, p2, p3, p4] ---> [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    return cells_prob\n",
    "\n",
    "def neighbor_cells(cell):\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    Neighbors = []\n",
    "    Actions_Neighbors = []\n",
    "    for action in Actions:\n",
    "\n",
    "        neighbor = [x + y for x, y in zip(cell, action)]\n",
    "        #if neighbor not in environment[4]:\n",
    "        Neighbors.append(neighbor)\n",
    "        Actions_Neighbors.append(action)\n",
    "\n",
    "    return Neighbors, Actions_Neighbors\n",
    "\n",
    "#Note\n",
    "\"\"\"As we want to use monte carlo method for estimating the state values\n",
    "   it has been assumed that we have not any knowledge about the environment.\n",
    "   Therefore, we should consider the transitions into the holes cells\n",
    "   (against the case of policy iteration)\"\"\"\n",
    "\n",
    "def arbitrary_policy(randomness):\n",
    "    #random.seed(randomness)\n",
    "    \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            neighbors = neighbor_cells(state)[0]\n",
    "            Actions_Neighbors = neighbor_cells(state)[1]\n",
    "\n",
    "            allowed_positions = []\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                \n",
    "                if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                    \n",
    "                    allowed_positions.append(neighbor)\n",
    "        \n",
    "            next_state = random.choice(allowed_positions)\n",
    "\n",
    "            row = next_state[0] - state[0]\n",
    "            col = next_state[1] - state[1]\n",
    "            PolicyAction = [row, col]\n",
    "\n",
    "            policy['{}'.format(state)] = next_state\n",
    "            policy_action['{}'.format(state)] = PolicyAction\n",
    "\n",
    "\n",
    "    return policy, policy_action\n",
    "\n",
    "def state_reward(policy,state):\n",
    "\n",
    "    policy_state = policy[0]\n",
    "    \n",
    "    next_state = policy_state[str(state)]\n",
    "\n",
    "    if next_state in environment[4]:\n",
    "\n",
    "        r = -3\n",
    "    \n",
    "    elif next_state == environment[3]:\n",
    "\n",
    "        r = 100\n",
    "    \n",
    "    elif next_state not in environment[6]:\n",
    "\n",
    "        r = -2\n",
    "    \n",
    "    else:\n",
    "\n",
    "        r = -1\n",
    "    \n",
    "    return r\n",
    "\n",
    "\n",
    "state_indice_dict = {}\n",
    "counter = 0\n",
    "for state in environment[6]:\n",
    "\n",
    "    state = str(state)\n",
    "    state_indice_dict[state] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "def generate_trajectory(policy,randomness,environment_stochasticity):\n",
    "\n",
    "    policy_action = policy[1]\n",
    "    probs = probability_distribution(environment[0]*environment[1],environment_stochasticity)\n",
    "    start = environment[2]\n",
    "    terminate = start\n",
    "    trajectory = [start]\n",
    "    c = 0\n",
    "    while terminate != environment[3]:\n",
    "        random.seed(randomness+c)\n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        action = policy_action[str(terminate)]\n",
    "        Actions.remove(action)\n",
    "        sorted_actions = Actions + [action]\n",
    "        state_indice = state_indice_dict[str(terminate)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "\n",
    "        selected_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        \n",
    "        next_state = [x + y for x, y in zip(terminate, selected_action)]\n",
    "        \n",
    "        #if the agent goes out of the gridworld, it stays in its current state\n",
    "        if next_state not in environment[6]:\n",
    "            next_state = terminate\n",
    "        \n",
    "        #if it drops into the holes, it goes to the start points\n",
    "        elif next_state in environment[4]:\n",
    "            next_state = start  \n",
    "\n",
    "        terminate = next_state\n",
    "        trajectory.append(terminate)\n",
    "        c = c+1\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular TD(0) for estimating $v_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_zero(num_trials, policy, alpha, gamma,environment_stochasticity):\n",
    "\n",
    "    policy_state = policy[0]\n",
    "    policy_action = policy[1]\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "    \n",
    "    V = {}\n",
    "    for state in environment[6]:\n",
    "    \n",
    "        if state not in environment[4] and state != environment[3]:\n",
    "\n",
    "            V[str(state)] = 0\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        trajectory = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "        \n",
    "        #state start from start point to terminal point\n",
    "        for state in trajectory[:-1]:\n",
    "            \n",
    "            #s_prime in the algorithm pseudocode\n",
    "            next_state = policy_state[str(state)]\n",
    "\n",
    "            if next_state == environment[3]:\n",
    "                \n",
    "                V[str(next_state)] = 0\n",
    "\n",
    "            reward = state_reward(policy,state)\n",
    "\n",
    "            V[str(state)] = V[str(state)] +\\\n",
    "                alpha * (reward + gamma * V[str(next_state)] - V[str(state)])\n",
    "        \n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:13<00:00, 724.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': -9.999999999999108,\n",
       " '[0, 2]': -9.999999999999108,\n",
       " '[0, 3]': -9.999999999999108,\n",
       " '[1, 0]': 54.95260578515885,\n",
       " '[1, 1]': 62.17099999999748,\n",
       " '[1, 2]': -9.999999999999108,\n",
       " '[1, 3]': -9.999999999999108,\n",
       " '[2, 1]': 70.18999999999758,\n",
       " '[2, 2]': -9.999999999999108,\n",
       " '[2, 3]': -9.999999999999108,\n",
       " '[3, 1]': 79.09999999999809,\n",
       " '[3, 3]': -9.999999999999108,\n",
       " '[4, 0]': 79.00171819404777,\n",
       " '[4, 1]': 88.99999999999866,\n",
       " '[4, 2]': 99.99999999999929,\n",
       " '[4, 3]': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_0 = arbitrary_policy(41)\n",
    "\n",
    "TD_zero(10000, policy_0, 0.01, 0.9, 'deterministic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:38<00:00, 261.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': -9.999999999999108,\n",
       " '[0, 2]': -9.999999999999108,\n",
       " '[0, 3]': -9.999999999999108,\n",
       " '[1, 0]': 54.953899999997375,\n",
       " '[1, 1]': 62.17099999999748,\n",
       " '[1, 2]': -9.999999999999108,\n",
       " '[1, 3]': -9.999999999999108,\n",
       " '[2, 1]': 70.18999999999758,\n",
       " '[2, 2]': -9.999999999999108,\n",
       " '[2, 3]': -9.999999999999108,\n",
       " '[3, 1]': 79.09999999999809,\n",
       " '[3, 3]': -9.999999999999108,\n",
       " '[4, 0]': 79.09999999999809,\n",
       " '[4, 1]': 88.99999999999866,\n",
       " '[4, 2]': 99.99999999999929,\n",
       " '[4, 3]': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD_zero(10000, policy_0, 0.01, 0.9, 'stochastic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa (On-policy TD control) for estimating $Q \\approx q_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_reward_policy_free(state, Final_action):\n",
    "\n",
    "    next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "    if next_state in environment[4]:\n",
    "        r = -3\n",
    "    elif next_state == environment[3]:\n",
    "        r = 100\n",
    "    elif next_state not in environment[6]:\n",
    "        r = -2\n",
    "    else:\n",
    "        r = -1 \n",
    "    return r\n",
    "\n",
    "def argmax_policy(q_values):\n",
    "\n",
    "    policy = {}\n",
    "    for state in list(q_values.keys()):\n",
    "\n",
    "        value_action_state = reverse_dictionary(q_values[state])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        policy[state] = ast.literal_eval(best_action)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def reverse_dictionary(dict):\n",
    "    reverse_dict = {}\n",
    "    for key in list(dict.keys()):\n",
    "        val = dict[key]\n",
    "        reverse_dict[val] = key\n",
    "    return reverse_dict\n",
    "\n",
    "\n",
    "def sarsa(num_trials, alpha, gamma,environment_stochasticity, epsilon):\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "    \n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                #next_state = [x + y for x, y in zip(state, ast.literal_eval(action))]\n",
    "\n",
    "                #if (next_state in environment[6]) and next_state not in environment[4]:\n",
    "                    \n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "\n",
    "    def state_action_nextstate(current_state):\n",
    "\n",
    "        #probs = probability_distribution(grid_size,42)\n",
    "\n",
    "        if type(current_state) == str:\n",
    "\n",
    "            state = ast.literal_eval(current_state)\n",
    "        else:\n",
    "            state = current_state\n",
    "        #Choose action using policy derived from Q===================================\n",
    "        value_action_state = reverse_dictionary(Q[str(state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "\n",
    "        #============================================================================\n",
    "        #Epsilon Greedy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "            selected_action = best_action\n",
    "        \n",
    "        else:\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "            Actions.remove(best_action)\n",
    "            epsilon_action = random.choice(Actions)\n",
    "\n",
    "            selected_action = epsilon_action\n",
    "        #============================================================================\n",
    "        \n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        Actions.remove(selected_action)\n",
    "        sorted_actions = Actions + [selected_action]\n",
    "        state_indice = state_indice_dict[str(state)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "        #due to stochasticity of the environment\n",
    "        Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        #print(type(state), type(Final_action))\n",
    "        \n",
    "        next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "        if next_state not in environment[6] or next_state in environment[4]:\n",
    "\n",
    "            next_state = current_state\n",
    "\n",
    "        return Final_action, next_state\n",
    "    \n",
    "\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        next_state = environment[2]\n",
    "\n",
    "        while next_state != environment[3]:\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            action_nextstate = state_action_nextstate(state)\n",
    "\n",
    "            action = action_nextstate[0]\n",
    "            next_state = action_nextstate[1]\n",
    "\n",
    "            next_action = state_action_nextstate(next_state)[0]\n",
    "\n",
    "\n",
    "            if next_state == environment[3]:\n",
    "\n",
    "                for action in [[1,0],[-1,0],[0,1],[0,-1]]:\n",
    "                \n",
    "                    Q[str(next_state)][str(action)] = 0\n",
    "\n",
    "            reward = state_reward_policy_free(state, action)\n",
    "\n",
    "            Q[str(state)][str(action)] = Q[str(state)][str(action)] +\\\n",
    "                alpha * (reward + gamma * Q[str(next_state)][str(next_action)] - Q[str(state)][str(action)])\n",
    "        \n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:14<00:00, 684.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -11.535359788507034,\n",
       "  '[-1, 0]': -12.863424219391383,\n",
       "  '[0, 1]': -11.477634282403372,\n",
       "  '[0, -1]': -13.986793694189751},\n",
       " '[0, 2]': {'[1, 0]': -11.018717283503106,\n",
       "  '[-1, 0]': -12.082059842743401,\n",
       "  '[0, 1]': -10.609945966467095,\n",
       "  '[0, -1]': -12.110869134712946},\n",
       " '[0, 3]': {'[1, 0]': -9.572918978267332,\n",
       "  '[-1, 0]': -11.83531649715452,\n",
       "  '[0, 1]': -11.723790980244095,\n",
       "  '[0, -1]': -11.022489123404304},\n",
       " '[1, 0]': {'[1, 0]': -15.962348086147738,\n",
       "  '[-1, 0]': -15.784316278787013,\n",
       "  '[0, 1]': -11.82799537119439,\n",
       "  '[0, -1]': -15.589713903856037},\n",
       " '[1, 1]': {'[1, 0]': -10.980892663653316,\n",
       "  '[-1, 0]': -11.922267790513352,\n",
       "  '[0, 1]': -10.56625470492576,\n",
       "  '[0, -1]': -13.786075148993659},\n",
       " '[1, 2]': {'[1, 0]': -9.26367684058797,\n",
       "  '[-1, 0]': -10.932132172150302,\n",
       "  '[0, 1]': -9.99729352471521,\n",
       "  '[0, -1]': -12.113140414102501},\n",
       " '[1, 3]': {'[1, 0]': -7.023973236908226,\n",
       "  '[-1, 0]': -10.66604123628411,\n",
       "  '[0, 1]': -10.542717736712769,\n",
       "  '[0, -1]': -10.730007189204258},\n",
       " '[2, 1]': {'[1, 0]': -9.915733010067,\n",
       "  '[-1, 0]': -11.684974147038334,\n",
       "  '[0, 1]': -9.522500931850615,\n",
       "  '[0, -1]': -12.787934160880994},\n",
       " '[2, 2]': {'[1, 0]': -11.873318962917773,\n",
       "  '[-1, 0]': -10.456388902632163,\n",
       "  '[0, 1]': -7.0028370345406366,\n",
       "  '[0, -1]': -10.94463055647983},\n",
       " '[2, 3]': {'[1, 0]': -3.6511329632179663,\n",
       "  '[-1, 0]': -9.084964022346723,\n",
       "  '[0, 1]': -8.815871048198813,\n",
       "  '[0, -1]': -9.345497835914577},\n",
       " '[3, 1]': {'[1, 0]': -5.700880763146008,\n",
       "  '[-1, 0]': -11.152559730266887,\n",
       "  '[0, 1]': -11.371862726038053,\n",
       "  '[0, -1]': -11.138344424261968},\n",
       " '[3, 3]': {'[1, 0]': 6.196781285346008e-09,\n",
       "  '[-1, 0]': -6.15960653876753,\n",
       "  '[0, 1]': -4.504378506656181,\n",
       "  '[0, -1]': -4.635380395609933},\n",
       " '[4, 0]': {'[1, 0]': -8.863381003865728,\n",
       "  '[-1, 0]': -10.356291168769971,\n",
       "  '[0, 1]': -6.408439619713494,\n",
       "  '[0, -1]': -9.448175561164458},\n",
       " '[4, 1]': {'[1, 0]': -6.382552499910183,\n",
       "  '[-1, 0]': -9.001983278310792,\n",
       "  '[0, 1]': -1.8588311071634753,\n",
       "  '[0, -1]': -8.564344723125698},\n",
       " '[4, 2]': {'[1, 0]': -4.396237627532408,\n",
       "  '[-1, 0]': -3.8794176646455174,\n",
       "  '[0, 1]': 7.932548758324577e-09,\n",
       "  '[0, -1]': -3.370573028667188},\n",
       " '[4, 3]': {'[1, 0]': 0, '[-1, 0]': 0, '[0, 1]': 0, '[0, -1]': 0}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa(10000, 0.1, 0.9, 'stochastic', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning (off-policy TD control) for estimating ${\\pi} \\approx {\\pi}_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(num_trials, alpha, gamma,environment_stochasticity, epsilon):\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "\n",
    "\n",
    "    def state_action_nextstate(current_state):\n",
    "\n",
    "        #probs = probability_distribution(grid_size,42)\n",
    "\n",
    "        if type(current_state) == str:\n",
    "\n",
    "            state = ast.literal_eval(current_state)\n",
    "        else:\n",
    "            state = current_state\n",
    "        #Choose action using policy derived from Q===================================\n",
    "        value_action_state = reverse_dictionary(Q[str(state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "\n",
    "        #============================================================================\n",
    "        #Epsilon Greedy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "            selected_action = best_action\n",
    "        \n",
    "        else:\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "            Actions.remove(best_action)\n",
    "            epsilon_action = random.choice(Actions)\n",
    "\n",
    "            selected_action = epsilon_action\n",
    "        #============================================================================\n",
    "        \n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        Actions.remove(selected_action)\n",
    "        sorted_actions = Actions + [selected_action]\n",
    "        state_indice = state_indice_dict[str(state)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "        #due to stochasticity of the environment\n",
    "        Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        #print(type(state), type(Final_action))\n",
    "        \n",
    "        next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "        if next_state not in environment[6] or next_state in environment[4]:\n",
    "\n",
    "            next_state = current_state\n",
    "        \n",
    "        value_action_state = reverse_dictionary(Q[str(next_state)])\n",
    "        #max Q(s',s)\n",
    "        Max_q_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_q_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "\n",
    "        return Final_action, next_state, Max_q_val\n",
    "    \n",
    "    policy = {}\n",
    "    path = [environment[2]]\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        next_state = environment[2] #sorry for bad names\n",
    "\n",
    "        while next_state != environment[3]:\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            action_nextstate = state_action_nextstate(state)\n",
    "\n",
    "            action = action_nextstate[0]\n",
    "            next_state = action_nextstate[1]\n",
    "\n",
    "            next_action = state_action_nextstate(next_state)[0]\n",
    "\n",
    "\n",
    "            if next_state == environment[3]:\n",
    "\n",
    "                for action in [[1,0],[-1,0],[0,1],[0,-1]]:\n",
    "                \n",
    "                    Q[str(next_state)][str(action)] = 0\n",
    "\n",
    "            reward = state_reward_policy_free(state, action)\n",
    "\n",
    "            Max_q_val = action_nextstate[2]\n",
    "\n",
    "            Q[str(state)][str(action)] = Q[str(state)][str(action)] +\\\n",
    "                alpha * (reward + gamma * Max_q_val - Q[str(state)][str(action)])\n",
    "                \n",
    "            \n",
    "            if trial  == num_trials - 1: #the last trial\n",
    "\n",
    "                policy[str(state)] = [action, next_state]\n",
    "                path.append(next_state)\n",
    "\n",
    "        \n",
    "    \n",
    "    return Q, policy, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:46<00:00, 2168.44it/s]\n"
     ]
    }
   ],
   "source": [
    "Qlearning = Q_learning(100000, 0.1, 0.9, 'deterministic', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -4.095099996888001,\n",
       "  '[-1, 0]': -5.685518126911136,\n",
       "  '[0, 1]': -4.095099996888001,\n",
       "  '[0, -1]': -6.685589984172322},\n",
       " '[0, 2]': {'[1, 0]': -3.4389999965422264,\n",
       "  '[-1, 0]': -5.095099996888001,\n",
       "  '[0, 1]': -3.4389999965422264,\n",
       "  '[0, -1]': -4.685589997199196},\n",
       " '[0, 3]': {'[1, 0]': -2.7099999961580314,\n",
       "  '[-1, 0]': -4.438999996542224,\n",
       "  '[0, 1]': -4.438999996542224,\n",
       "  '[0, -1]': -4.095099996888001},\n",
       " '[1, 0]': {'[1, 0]': -6.667726150911664,\n",
       "  '[-1, 0]': -6.685074344977916,\n",
       "  '[0, 1]': -4.095099996888001,\n",
       "  '[0, -1]': -5.68558750553322},\n",
       " '[1, 1]': {'[1, 0]': -3.4389999965422264,\n",
       "  '[-1, 0]': -4.685589997199196,\n",
       "  '[0, 1]': -3.4389999965422264,\n",
       "  '[0, -1]': -4.685589997199196},\n",
       " '[1, 2]': {'[1, 0]': -2.7099999961580314,\n",
       "  '[-1, 0]': -4.095099996888001,\n",
       "  '[0, 1]': -2.7099999961580314,\n",
       "  '[0, -1]': -4.095099996888001},\n",
       " '[1, 3]': {'[1, 0]': -1.899999995731148,\n",
       "  '[-1, 0]': -3.4389999965422264,\n",
       "  '[0, 1]': -3.7099999961580314,\n",
       "  '[0, -1]': -3.4389999965422264},\n",
       " '[2, 1]': {'[1, 0]': -2.70999999689522,\n",
       "  '[-1, 0]': -4.095099773365455,\n",
       "  '[0, 1]': -2.7099999961580314,\n",
       "  '[0, -1]': -5.438999996533285},\n",
       " '[2, 2]': {'[1, 0]': -4.709999996158029,\n",
       "  '[-1, 0]': -3.4389999965422264,\n",
       "  '[0, 1]': -1.899999995731148,\n",
       "  '[0, -1]': -3.4389999965422264},\n",
       " '[2, 3]': {'[1, 0]': -0.9999999952568321,\n",
       "  '[-1, 0]': -2.7099999961580314,\n",
       "  '[0, 1]': -2.8999999957311466,\n",
       "  '[0, -1]': -2.7099999961580314},\n",
       " '[3, 1]': {'[1, 0]': -1.8999999965503511,\n",
       "  '[-1, 0]': -3.114651138271015,\n",
       "  '[0, 1]': -4.467326578964795,\n",
       "  '[0, -1]': -4.667585584988464},\n",
       " '[3, 3]': {'[1, 0]': 5.270185949046325e-09,\n",
       "  '[-1, 0]': -1.899999995731148,\n",
       "  '[0, 1]': -1.9999999952568315,\n",
       "  '[0, -1]': -2.999999999899717},\n",
       " '[4, 0]': {'[1, 0]': -2.0495514658740794,\n",
       "  '[-1, 0]': -2.020636341259894,\n",
       "  '[0, 1]': -1.8630411533237212,\n",
       "  '[0, -1]': -1.8998163321741086},\n",
       " '[4, 1]': {'[1, 0]': -2.756905892656082,\n",
       "  '[-1, 0]': -2.466414168874482,\n",
       "  '[0, 1]': -0.9999999961670598,\n",
       "  '[0, -1]': -2.525990007966551},\n",
       " '[4, 2]': {'[1, 0]': -1.6664563621858504,\n",
       "  '[-1, 0]': -2.7607006669512635,\n",
       "  '[0, 1]': 4.258821786587477e-09,\n",
       "  '[0, -1]': -1.0027221333830956},\n",
       " '[4, 3]': {'[1, 0]': 0, '[-1, 0]': 0, '[0, 1]': 0, '[0, -1]': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q-values\n",
    "\n",
    "Qlearning[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[1, 2]': [[0, 1], [1, 3]],\n",
       " '[1, 3]': [[1, 0], [2, 3]],\n",
       " '[2, 3]': [[1, 0], [3, 3]],\n",
       " '[0, 3]': [[1, 0], [1, 3]],\n",
       " '[3, 3]': [[0, -1], [4, 3]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimal policy\n",
    "Qlearning[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3], [2, 3], [1, 3], [0, 3], [1, 3], [2, 3], [3, 3], [4, 3]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path from start point to the terminal point\n",
    "Qlearning[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-learning, for estimating $Q_1 \\approx Q_2 \\approx q_{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_Q_learning(num_trials, alpha, gamma,environment_stochasticity, epsilon):\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "\n",
    "    Q1, Q2 = {}, {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q1[str(state)] = {}\n",
    "            Q2[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q1[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "                Q2[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "\n",
    "    #Choose A from S using the policy epsilon-greedy in Q1 + Q2\n",
    "    def state_action_nextstate(current_state,Q1, Q2):\n",
    "\n",
    "        Q = {}\n",
    "\n",
    "        for state in environment[6]:\n",
    "\n",
    "            if state not in environment[4]:\n",
    "                \n",
    "                Q[str(state)] = {}\n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                    Q[str(state)][action] = Q1[str(state)][action] + Q2[str(state)][action]\n",
    "\n",
    "        #probs = probability_distribution(grid_size,42)\n",
    "\n",
    "        if type(current_state) == str:\n",
    "\n",
    "            state = ast.literal_eval(current_state)\n",
    "        else:\n",
    "            state = current_state\n",
    "        #Choose action using policy derived from Q===================================\n",
    "        value_action_state = reverse_dictionary(Q[str(state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "\n",
    "        #============================================================================\n",
    "        #Epsilon Greedy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "            selected_action = best_action\n",
    "        \n",
    "        else:\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "            Actions.remove(best_action)\n",
    "            epsilon_action = random.choice(Actions)\n",
    "\n",
    "            selected_action = epsilon_action\n",
    "        #============================================================================\n",
    "        \n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        Actions.remove(selected_action)\n",
    "        sorted_actions = Actions + [selected_action]\n",
    "        state_indice = state_indice_dict[str(state)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "        #due to stochasticity of the environment\n",
    "        Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        #print(type(state), type(Final_action))\n",
    "        \n",
    "        next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "        if next_state not in environment[6] or next_state in environment[4]:\n",
    "\n",
    "            next_state = current_state\n",
    "        \n",
    "        value_action_state = reverse_dictionary(Q[str(next_state)])\n",
    "        #max Q(s',s)\n",
    "        Max_q_val = max(list(value_action_state.keys()))\n",
    "        next_best_action = value_action_state[Max_q_val]\n",
    "        next_best_action = ast.literal_eval(next_best_action)\n",
    "\n",
    "        return Final_action, next_state, Max_q_val, next_best_action\n",
    "    \n",
    "    policy = {}\n",
    "    path = [environment[2]]\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        next_state = environment[2] #sorry for bad names\n",
    "\n",
    "        while next_state != environment[3]:\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            action_nextstate = state_action_nextstate(state, Q1, Q2)\n",
    "\n",
    "            action = action_nextstate[0]\n",
    "            next_state = action_nextstate[1]\n",
    "            next_best_action =  action_nextstate[3]\n",
    "\n",
    "            if next_state == environment[3]:\n",
    "\n",
    "                for action in [[1,0],[-1,0],[0,1],[0,-1]]:\n",
    "                \n",
    "                    Q1[str(next_state)][str(action)] = 0\n",
    "                    Q2[str(next_state)][str(action)] = 0\n",
    "\n",
    "            reward = state_reward_policy_free(state, action)\n",
    "\n",
    "\n",
    "            if random.random() >= 0.5:\n",
    "\n",
    "                Q1[str(state)][str(action)] = Q1[str(state)][str(action)] +\\\n",
    "                    alpha * (reward + gamma * Q2[str(next_state)][str(next_best_action)] - Q1[str(state)][str(action)])\n",
    "            \n",
    "            else:\n",
    "\n",
    "                Q2[str(state)][str(action)] = Q2[str(state)][str(action)] +\\\n",
    "                    alpha * (reward + gamma * Q1[str(next_state)][str(next_best_action)] - Q2[str(state)][str(action)])\n",
    "\n",
    "            \n",
    "            if trial  == num_trials - 1: #the last trial\n",
    "\n",
    "                policy[str(state)] = [action, next_state]\n",
    "                path.append(next_state)\n",
    "\n",
    "        \n",
    "    \n",
    "    return Q1,Q2, policy, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:31<00:00, 1087.49it/s]\n"
     ]
    }
   ],
   "source": [
    "double_Qlearning = double_Q_learning(100000, 0.1, 0.9, 'deterministic', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -4.095099996790216,\n",
       "  '[-1, 0]': -5.631778879316117,\n",
       "  '[0, 1]': -4.095099995793727,\n",
       "  '[0, -1]': -6.682010011797055},\n",
       " '[0, 2]': {'[1, 0]': -3.438999998756275,\n",
       "  '[-1, 0]': -5.095099996525053,\n",
       "  '[0, 1]': -3.438999998756275,\n",
       "  '[0, -1]': -4.685589995065815},\n",
       " '[0, 3]': {'[1, 0]': -2.7099999957926713,\n",
       "  '[-1, 0]': -4.4389999987541415,\n",
       "  '[0, 1]': -4.438999998756273,\n",
       "  '[0, -1]': -4.095099996592058},\n",
       " '[1, 0]': {'[1, 0]': -6.417060494216024,\n",
       "  '[-1, 0]': -6.5574557685577615,\n",
       "  '[0, 1]': -4.095099996790216,\n",
       "  '[0, -1]': -5.680910127108725},\n",
       " '[1, 1]': {'[1, 0]': -3.438999993908424,\n",
       "  '[-1, 0]': -4.685589995062016,\n",
       "  '[0, 1]': -3.438999998756275,\n",
       "  '[0, -1]': -4.685589995065815},\n",
       " '[1, 2]': {'[1, 0]': -2.7099999957926713,\n",
       "  '[-1, 0]': -4.095099996592058,\n",
       "  '[0, 1]': -2.7099999957926713,\n",
       "  '[0, -1]': -4.095099996790216},\n",
       " '[1, 3]': {'[1, 0]': -1.899999998464541,\n",
       "  '[-1, 0]': -3.438999998756275,\n",
       "  '[0, 1]': -3.7099999957926713,\n",
       "  '[0, -1]': -3.438999998756275},\n",
       " '[2, 1]': {'[1, 0]': -2.7099999960373093,\n",
       "  '[-1, 0]': -4.095099996784033,\n",
       "  '[0, 1]': -2.7099999957926713,\n",
       "  '[0, -1]': -5.438999993908421},\n",
       " '[2, 2]': {'[1, 0]': -4.709999995792655,\n",
       "  '[-1, 0]': -3.438999998756275,\n",
       "  '[0, 1]': -1.899999998464541,\n",
       "  '[0, -1]': -3.438999993908424},\n",
       " '[2, 3]': {'[1, 0]': -0.9999999948057703,\n",
       "  '[-1, 0]': -2.7099999957926713,\n",
       "  '[0, 1]': -2.8999999984645397,\n",
       "  '[0, -1]': -2.7099999957926713},\n",
       " '[3, 1]': {'[1, 0]': -1.89999999247954,\n",
       "  '[-1, 0]': -3.438999993902493,\n",
       "  '[0, 1]': -4.709999996037308,\n",
       "  '[0, -1]': -4.709999996037308},\n",
       " '[3, 3]': {'[1, 0]': 1.895626864635446e-09,\n",
       "  '[-1, 0]': -1.899999998464541,\n",
       "  '[0, 1]': -1.99999999480577,\n",
       "  '[0, -1]': -2.999999999955},\n",
       " '[4, 0]': {'[1, 0]': -3.4407295763872168,\n",
       "  '[-1, 0]': -4.6918973376948845,\n",
       "  '[0, 1]': -1.89999999247954,\n",
       "  '[0, -1]': -3.7029772322779086},\n",
       " '[4, 1]': {'[1, 0]': -2.8999999924738784,\n",
       "  '[-1, 0]': -2.7099999960373093,\n",
       "  '[0, 1]': -0.9999999951077922,\n",
       "  '[0, -1]': -2.7099999960373093},\n",
       " '[4, 2]': {'[1, 0]': -1.9999999950166263,\n",
       "  '[-1, 0]': -2.9999999951077907,\n",
       "  '[0, 1]': 9.284516636296922e-09,\n",
       "  '[0, -1]': -1.0112036394503838},\n",
       " '[4, 3]': {'[1, 0]': 0, '[-1, 0]': 0, '[0, 1]': 0, '[0, -1]': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "double_Qlearning[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -4.095099994517577,\n",
       "  '[-1, 0]': -5.6527618204148276,\n",
       "  '[0, 1]': -4.095099998790683,\n",
       "  '[0, -1]': -6.680480313154636},\n",
       " '[0, 2]': {'[1, 0]': -3.4389999962134024,\n",
       "  '[-1, 0]': -5.095099998877521,\n",
       "  '[0, 1]': -3.4389999962134024,\n",
       "  '[0, -1]': -4.68558999711119},\n",
       " '[0, 3]': {'[1, 0]': -2.709999998618085,\n",
       "  '[-1, 0]': -4.438999996129162,\n",
       "  '[0, 1]': -4.438999996213401,\n",
       "  '[0, -1]': -4.095099998880643},\n",
       " '[1, 0]': {'[1, 0]': -6.509301216245291,\n",
       "  '[-1, 0]': -6.6293776601586565,\n",
       "  '[0, 1]': -4.095099994517577,\n",
       "  '[0, -1]': -5.670099302412765},\n",
       " '[1, 1]': {'[1, 0]': -3.4389999964335765,\n",
       "  '[-1, 0]': -4.685589997110178,\n",
       "  '[0, 1]': -3.4389999962134024,\n",
       "  '[0, -1]': -4.68558999711119},\n",
       " '[1, 2]': {'[1, 0]': -2.709999998618085,\n",
       "  '[-1, 0]': -4.095099998880643,\n",
       "  '[0, 1]': -2.709999998618085,\n",
       "  '[0, -1]': -4.095099994517586},\n",
       " '[1, 3]': {'[1, 0]': -1.8999999953251923,\n",
       "  '[-1, 0]': -3.4389999962134024,\n",
       "  '[0, 1]': -3.709999998618085,\n",
       "  '[0, -1]': -3.4389999962134024},\n",
       " '[2, 1]': {'[1, 0]': -2.709999993231584,\n",
       "  '[-1, 0]': -4.095099994517446,\n",
       "  '[0, 1]': -2.709999998618085,\n",
       "  '[0, -1]': -5.438999996433575},\n",
       " '[2, 2]': {'[1, 0]': -4.709999998618082,\n",
       "  '[-1, 0]': -3.4389999962134024,\n",
       "  '[0, 1]': -1.8999999953251923,\n",
       "  '[0, -1]': -3.4389999964335765},\n",
       " '[2, 3]': {'[1, 0]': -0.9999999982939354,\n",
       "  '[-1, 0]': -2.709999998618085,\n",
       "  '[0, 1]': -2.8999999953251914,\n",
       "  '[0, -1]': -2.709999998618085},\n",
       " '[3, 1]': {'[1, 0]': -1.8999999955970122,\n",
       "  '[-1, 0]': -3.438999996420027,\n",
       "  '[0, 1]': -4.709999993231582,\n",
       "  '[0, -1]': -4.709999993231582},\n",
       " '[3, 3]': {'[1, 0]': 5.771365711719732e-09,\n",
       "  '[-1, 0]': -1.8999999953251923,\n",
       "  '[0, 1]': -1.999999998293935,\n",
       "  '[0, -1]': -2.9999999999891016},\n",
       " '[4, 0]': {'[1, 0]': -3.593738947175316,\n",
       "  '[-1, 0]': -4.668167100105652,\n",
       "  '[0, 1]': -1.8999999955970122,\n",
       "  '[0, -1]': -3.7055484531584044},\n",
       " '[4, 1]': {'[1, 0]': -2.89999999559536,\n",
       "  '[-1, 0]': -2.709999993231584,\n",
       "  '[0, 1]': -0.9999999916439346,\n",
       "  '[0, -1]': -2.709999993231584},\n",
       " '[4, 2]': {'[1, 0]': -1.9999999916270417,\n",
       "  '[-1, 0]': -2.999999991643933,\n",
       "  '[0, 1]': 5.435785886151388e-09,\n",
       "  '[0, -1]': -1.1061457221428657},\n",
       " '[4, 3]': {'[1, 0]': 0, '[-1, 0]': 0, '[0, 1]': 0, '[0, -1]': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "double_Qlearning[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[1, 2]': [[0, 1], [1, 3]],\n",
       " '[1, 1]': [[0, 1], [1, 2]],\n",
       " '[1, 3]': [[1, 0], [2, 3]],\n",
       " '[2, 3]': [[1, 0], [3, 3]],\n",
       " '[3, 3]': [[0, -1], [4, 3]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimal policy\n",
    "double_Qlearning[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [1, 1], [1, 2], [1, 3], [2, 3], [3, 3], [4, 3]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimal path\n",
    "double_Qlearning[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
