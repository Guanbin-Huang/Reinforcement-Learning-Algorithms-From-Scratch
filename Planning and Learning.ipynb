{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_world(length, width,path_lenght,holes_number,Random_State):\n",
    "    \n",
    "    random.seed(Random_State)\n",
    "    #store all cells in a list\n",
    "    Grid_Cells = []\n",
    "    for row in range(length):\n",
    "        for col in range(width):\n",
    "            Grid_Cells.append([row,col])\n",
    "\n",
    "    #specify the number of holes in the gridworld\n",
    "    \n",
    "    #specify the start point as a random cell\n",
    "    start = [random.randint(0, length), random.randint(0, width)]\n",
    "\n",
    "    #create a path from start point\n",
    "    \"\"\"instead of defining start and goal points,\n",
    "      we define just a start point and a random path with a random lenght to\n",
    "       another point and name it as goal point\"\"\"\n",
    "    \n",
    "    def random_path(Start, Path_Lenght,length, width):\n",
    "        \n",
    "        Path = []\n",
    "        Path.append(Start)\n",
    "        for i in range(Path_Lenght):\n",
    "            \n",
    "            #there are two moves that take us on a random cell named Goal [1,0], [0,1]\n",
    "            \n",
    "            move = random.choice([[1,0], [0,1]])\n",
    "            \n",
    "            #update the start cell/point by the above move\n",
    "            Start = [x + y for x, y in zip(Start, move)]\n",
    "            \n",
    "            #if the movement take us out of our gridworld, we reverse the change in the start point\n",
    "            if Start[0] < 0 or Start[1] < 0 or Start[0] > length-1 or Start[1] > width-1:\n",
    "\n",
    "                Start = [x - y for x, y in zip(Start, move)]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                #create a path history\n",
    "                Path.append(Start)\n",
    "\n",
    "        Goal = Start\n",
    "\n",
    "        return Goal,Path\n",
    "    \n",
    "\n",
    "    GoalPath = random_path(start, path_lenght,length, width)\n",
    "\n",
    "    goal = GoalPath[0]\n",
    "    path = GoalPath[1]\n",
    "\n",
    "    #now we must eliminate the path cells from the Grid_Cells to choose hole cells from remaining cells\n",
    "\n",
    "    FreeCells = [x for x in Grid_Cells if x not in path]\n",
    "\n",
    "    Holes = random.sample(FreeCells, holes_number)\n",
    "\n",
    "    #Also, we can visualize our gridworld in a simple way\n",
    "\n",
    "    def mark_holes(holes):\n",
    "        marked_data = [[\"Hole\" if [row, col] in holes else [row, col] for col in range(width)] for row in range(length)]\n",
    "        return marked_data\n",
    "    \n",
    "    marked_matrix = mark_holes(Holes)\n",
    "\n",
    "    print(tabulate(marked_matrix, tablefmt=\"grid\"))\n",
    "\n",
    "    \n",
    "    return length, width, start, goal, Holes, path,Grid_Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+\n",
      "| Hole   | [0, 1] | [0, 2] | [0, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [1, 0] | [1, 1] | [1, 2] | [1, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [2, 1] | [2, 2] | [2, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [3, 1] | Hole   | [3, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [4, 0] | [4, 1] | [4, 2] | [4, 3] |\n",
      "+--------+--------+--------+--------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 4,\n",
       " [1, 2],\n",
       " [4, 3],\n",
       " [[2, 0], [3, 2], [3, 0], [0, 0]],\n",
       " [[1, 2], [1, 3], [2, 3], [3, 3], [4, 3]],\n",
       " [[0, 0],\n",
       "  [0, 1],\n",
       "  [0, 2],\n",
       "  [0, 3],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [3, 0],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [4, 0],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = generate_grid_world(5, 4,4,4,39)\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_distribution(grid_size,randomness):\n",
    "    #random.seed(40)\n",
    "    \n",
    "    #by this function we generate probabilities which their sum is equal to 1\n",
    "    def generate_probabilities(n):\n",
    "\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "        \n",
    "        return scaled_numbers\n",
    "    \n",
    "    cells_prob = {}\n",
    "    if randomness == 'stochastic':\n",
    "        for cell in range(grid_size):\n",
    "            \n",
    "            #we set the number of probs to 4 due to 4 possible action for each cell (go to its neighbors)\n",
    "            probs = generate_probabilities(4)\n",
    "\n",
    "            cells_prob[cell] = probs\n",
    "    elif randomness == 'equal probable':\n",
    "\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.25,0.25,0.25,0.25]\n",
    "    \n",
    "    elif randomness == 'deterministic':\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.03,0.06,0.01,0.9] #[0,0,0,1] ##[0.15,.15,0.1,0.6]\n",
    "\n",
    "\n",
    "    #Note that we consider the correspondence between probabilities and actions as below:\n",
    "    #probs = [p1, p2, p3, p4] ---> [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    return cells_prob\n",
    "\n",
    "def neighbor_cells(cell):\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    Neighbors = []\n",
    "    Actions_Neighbors = []\n",
    "    for action in Actions:\n",
    "\n",
    "        neighbor = [x + y for x, y in zip(cell, action)]\n",
    "        #if neighbor not in environment[4]:\n",
    "        Neighbors.append(neighbor)\n",
    "        Actions_Neighbors.append(action)\n",
    "\n",
    "    return Neighbors, Actions_Neighbors\n",
    "\n",
    "def neighbor_cells_actions(cell):\n",
    "\n",
    "    def reverse_action(action):\n",
    "\n",
    "        for i in range(2):\n",
    "            if action[i] != 0:\n",
    "\n",
    "                action[i] = - action[i]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    Neighbors = {}\n",
    "    #Actions_Neighbors = []\n",
    "    for action in Actions:\n",
    "\n",
    "        neighbor = [x + y for x, y in zip(cell, action)]\n",
    "        #if neighbor not in environment[4]:\n",
    "\n",
    "        Neighbors[str(neighbor)] = reverse_action(action)\n",
    "\n",
    "    return Neighbors\n",
    "\n",
    "#Note\n",
    "\"\"\"As we want to use monte carlo method for estimating the state values\n",
    "   it has been assumed that we have not any knowledge about the environment.\n",
    "   Therefore, we should consider the transitions into the holes cells\n",
    "   (against the case of policy iteration)\"\"\"\n",
    "\n",
    "def arbitrary_policy(randomness):\n",
    "    #random.seed(randomness)\n",
    "    \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            neighbors = neighbor_cells(state)[0]\n",
    "            Actions_Neighbors = neighbor_cells(state)[1]\n",
    "\n",
    "            allowed_positions = []\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                \n",
    "                if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                    \n",
    "                    allowed_positions.append(neighbor)\n",
    "        \n",
    "            next_state = random.choice(allowed_positions)\n",
    "\n",
    "            row = next_state[0] - state[0]\n",
    "            col = next_state[1] - state[1]\n",
    "            PolicyAction = [row, col]\n",
    "\n",
    "            policy['{}'.format(state)] = next_state\n",
    "            policy_action['{}'.format(state)] = PolicyAction\n",
    "\n",
    "\n",
    "    return policy, policy_action\n",
    "\n",
    "def state_reward(next_state):\n",
    "\n",
    "    if next_state in environment[4]:\n",
    "\n",
    "        r = -3\n",
    "    \n",
    "    elif next_state == environment[3]:\n",
    "\n",
    "        r = 0\n",
    "    \n",
    "    elif next_state not in environment[6]:\n",
    "\n",
    "        r = -2\n",
    "    \n",
    "    else:\n",
    "\n",
    "        r = -1\n",
    "    \n",
    "    return r\n",
    "\n",
    "def reverse_dictionary(dict):\n",
    "    reverse_dict = {}\n",
    "    for key in list(dict.keys()):\n",
    "        val = dict[key]\n",
    "        reverse_dict[val] = key\n",
    "    return reverse_dict\n",
    "\n",
    "\n",
    "state_indice_dict = {}\n",
    "counter = 0\n",
    "for state in environment[6]:\n",
    "\n",
    "    state = str(state)\n",
    "    state_indice_dict[state] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "def generate_trajectory(policy,randomness,environment_stochasticity):\n",
    "\n",
    "    policy_action = policy[1]\n",
    "    probs = probability_distribution(environment[0]*environment[1],environment_stochasticity)\n",
    "    start = environment[2]\n",
    "    terminate = start\n",
    "    trajectory = []\n",
    "    pure_trajectory = [start]\n",
    "    c = 0\n",
    "    while terminate != environment[3]:\n",
    "        random.seed(randomness+c)\n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        action = policy_action[str(terminate)]\n",
    "        Actions.remove(action)\n",
    "        sorted_actions = Actions + [action]\n",
    "        state_indice = state_indice_dict[str(terminate)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "\n",
    "        selected_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        current_state = terminate\n",
    "        next_state = [x + y for x, y in zip(terminate, selected_action)]\n",
    "        pure_trajectory.append(next_state)\n",
    "        \n",
    "        #if the agent goes out of the gridworld, it stays in its current state\n",
    "        if next_state not in environment[6]:\n",
    "            next_state = terminate\n",
    "        \n",
    "        #if it drops into the holes, it goes to the start points\n",
    "        elif next_state in environment[4]:\n",
    "            next_state = start  \n",
    "\n",
    "        terminate = next_state\n",
    "        trajectory.append((current_state,selected_action))\n",
    "        c = c+1\n",
    "    \n",
    "    trajectory.append((environment[3],[0,0]))\n",
    "    pure_trajectory.append(environment[3])\n",
    "\n",
    "    return trajectory,pure_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random-sample one-step tabular Q-planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(Environment):\n",
    "\n",
    "    \"\"\" As we are working on the gridworld, we know the reward function in general.\n",
    "        Also, we know the start and terminal point.\n",
    "        we dedicate -1 reward to each state transition except when the next state is the terminal.\n",
    "        As we do not have any information about the environmen's details, detecting holes\n",
    "        and returning high negative rewsard is not possible. \n",
    "    \"\"\"\n",
    "    #grid_size = Environment[0]*Environment[1]\n",
    "    #start = Environment[2]\n",
    "    terminal = Environment[3]\n",
    "\n",
    "    state_action_reward_nextstate = {}\n",
    "\n",
    "    for state in Environment[6]:\n",
    "\n",
    "        state_action_reward_nextstate[str(state)] = {}\n",
    "\n",
    "        for action in [[1, 0],[-1, 0],[0, 1],[0, -1]]:\n",
    "\n",
    "            state_action_reward_nextstate[str(state)][str(action)] = []\n",
    "\n",
    "    for state in Environment[6]:\n",
    "\n",
    "        for action in [[1, 0],[-1, 0],[0, 1],[0, -1]]:\n",
    "\n",
    "            next_state = [x + y for x, y in zip(state, action)]\n",
    "\n",
    "            if next_state == terminal:\n",
    "\n",
    "                state_action_reward_nextstate[str(state)][str(action)] = [0, next_state]\n",
    "            \n",
    "            else:\n",
    "\n",
    "                if next_state in Environment[6]:\n",
    "\n",
    "                    state_action_reward_nextstate[str(state)][str(action)] = [-1, next_state]\n",
    "                \n",
    "                else:\n",
    "\n",
    "                    state_action_reward_nextstate[str(state)][str(action)] = [-1, state]\n",
    "    \n",
    "\n",
    "    return state_action_reward_nextstate    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 0]': {'[1, 0]': [-1, [1, 0]],\n",
       "  '[-1, 0]': [-1, [0, 0]],\n",
       "  '[0, 1]': [-1, [0, 1]],\n",
       "  '[0, -1]': [-1, [0, 0]]},\n",
       " '[0, 1]': {'[1, 0]': [-1, [1, 1]],\n",
       "  '[-1, 0]': [-1, [0, 1]],\n",
       "  '[0, 1]': [-1, [0, 2]],\n",
       "  '[0, -1]': [-1, [0, 0]]},\n",
       " '[0, 2]': {'[1, 0]': [-1, [1, 2]],\n",
       "  '[-1, 0]': [-1, [0, 2]],\n",
       "  '[0, 1]': [-1, [0, 3]],\n",
       "  '[0, -1]': [-1, [0, 1]]},\n",
       " '[0, 3]': {'[1, 0]': [-1, [1, 3]],\n",
       "  '[-1, 0]': [-1, [0, 3]],\n",
       "  '[0, 1]': [-1, [0, 3]],\n",
       "  '[0, -1]': [-1, [0, 2]]},\n",
       " '[1, 0]': {'[1, 0]': [-1, [2, 0]],\n",
       "  '[-1, 0]': [-1, [0, 0]],\n",
       "  '[0, 1]': [-1, [1, 1]],\n",
       "  '[0, -1]': [-1, [1, 0]]},\n",
       " '[1, 1]': {'[1, 0]': [-1, [2, 1]],\n",
       "  '[-1, 0]': [-1, [0, 1]],\n",
       "  '[0, 1]': [-1, [1, 2]],\n",
       "  '[0, -1]': [-1, [1, 0]]},\n",
       " '[1, 2]': {'[1, 0]': [-1, [2, 2]],\n",
       "  '[-1, 0]': [-1, [0, 2]],\n",
       "  '[0, 1]': [-1, [1, 3]],\n",
       "  '[0, -1]': [-1, [1, 1]]},\n",
       " '[1, 3]': {'[1, 0]': [-1, [2, 3]],\n",
       "  '[-1, 0]': [-1, [0, 3]],\n",
       "  '[0, 1]': [-1, [1, 3]],\n",
       "  '[0, -1]': [-1, [1, 2]]},\n",
       " '[2, 0]': {'[1, 0]': [-1, [3, 0]],\n",
       "  '[-1, 0]': [-1, [1, 0]],\n",
       "  '[0, 1]': [-1, [2, 1]],\n",
       "  '[0, -1]': [-1, [2, 0]]},\n",
       " '[2, 1]': {'[1, 0]': [-1, [3, 1]],\n",
       "  '[-1, 0]': [-1, [1, 1]],\n",
       "  '[0, 1]': [-1, [2, 2]],\n",
       "  '[0, -1]': [-1, [2, 0]]},\n",
       " '[2, 2]': {'[1, 0]': [-1, [3, 2]],\n",
       "  '[-1, 0]': [-1, [1, 2]],\n",
       "  '[0, 1]': [-1, [2, 3]],\n",
       "  '[0, -1]': [-1, [2, 1]]},\n",
       " '[2, 3]': {'[1, 0]': [-1, [3, 3]],\n",
       "  '[-1, 0]': [-1, [1, 3]],\n",
       "  '[0, 1]': [-1, [2, 3]],\n",
       "  '[0, -1]': [-1, [2, 2]]},\n",
       " '[3, 0]': {'[1, 0]': [-1, [4, 0]],\n",
       "  '[-1, 0]': [-1, [2, 0]],\n",
       "  '[0, 1]': [-1, [3, 1]],\n",
       "  '[0, -1]': [-1, [3, 0]]},\n",
       " '[3, 1]': {'[1, 0]': [-1, [4, 1]],\n",
       "  '[-1, 0]': [-1, [2, 1]],\n",
       "  '[0, 1]': [-1, [3, 2]],\n",
       "  '[0, -1]': [-1, [3, 0]]},\n",
       " '[3, 2]': {'[1, 0]': [-1, [4, 2]],\n",
       "  '[-1, 0]': [-1, [2, 2]],\n",
       "  '[0, 1]': [-1, [3, 3]],\n",
       "  '[0, -1]': [-1, [3, 1]]},\n",
       " '[3, 3]': {'[1, 0]': [0, [4, 3]],\n",
       "  '[-1, 0]': [-1, [2, 3]],\n",
       "  '[0, 1]': [-1, [3, 3]],\n",
       "  '[0, -1]': [-1, [3, 2]]},\n",
       " '[4, 0]': {'[1, 0]': [-1, [4, 0]],\n",
       "  '[-1, 0]': [-1, [3, 0]],\n",
       "  '[0, 1]': [-1, [4, 1]],\n",
       "  '[0, -1]': [-1, [4, 0]]},\n",
       " '[4, 1]': {'[1, 0]': [-1, [4, 1]],\n",
       "  '[-1, 0]': [-1, [3, 1]],\n",
       "  '[0, 1]': [-1, [4, 2]],\n",
       "  '[0, -1]': [-1, [4, 0]]},\n",
       " '[4, 2]': {'[1, 0]': [-1, [4, 2]],\n",
       "  '[-1, 0]': [-1, [3, 2]],\n",
       "  '[0, 1]': [0, [4, 3]],\n",
       "  '[0, -1]': [-1, [4, 1]]},\n",
       " '[4, 3]': {'[1, 0]': [-1, [4, 3]],\n",
       "  '[-1, 0]': [-1, [3, 3]],\n",
       "  '[0, 1]': [-1, [4, 3]],\n",
       "  '[0, -1]': [-1, [4, 2]]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample_model = sample_model(environment)\n",
    "test_sample_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_Q_planning(num_trials, gamma, alpha, environment_stochasticity, sample_model):\n",
    "\n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "            \n",
    "        Q[str(state)] = {}\n",
    "\n",
    "        for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "            Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    next_state = environment[2] #start state\n",
    "\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        Actions = [[1, 0],[-1, 0],[0, 1],[0, -1]]\n",
    "\n",
    "        random_action = random.choice(Actions)\n",
    "        \n",
    "        Actions.remove(random_action)\n",
    "        sorted_actions = Actions + [random_action]\n",
    "        state_indice = state_indice_dict[str(state)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "        #due to stochasticity of the environment\n",
    "        Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "\n",
    "        new_state = [x + y for x, y in zip(current_state, Final_action)]\n",
    "\n",
    "        if new_state in environment[6]:\n",
    "\n",
    "            next_state = new_state\n",
    "        \n",
    "        else:\n",
    "\n",
    "            next_state = current_state\n",
    "\n",
    "        reward = sample_model[str(next_state)][str(Final_action)][0]\n",
    "\n",
    "        value_action_state = reverse_dictionary(Q[str(next_state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "\n",
    "        Q[str(current_state)][str(Final_action)] = Q[str(current_state)][str(Final_action)] + alpha * (reward + gamma * Max_val - Q[str(current_state)][str(Final_action)])\n",
    "\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:12<00:00, 83072.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 0]': {'[1, 0]': -6.892157894736823,\n",
       "  '[-1, 0]': -7.20294210526314,\n",
       "  '[0, 1]': -6.892157894736823,\n",
       "  '[0, -1]': -7.20294210526314},\n",
       " '[0, 1]': {'[1, 0]': -6.546842105263139,\n",
       "  '[-1, 0]': -6.892157894736823,\n",
       "  '[0, 1]': -6.546842105263139,\n",
       "  '[0, -1]': -7.20294210526314},\n",
       " '[0, 2]': {'[1, 0]': -6.163157894736823,\n",
       "  '[-1, 0]': -6.546842105263139,\n",
       "  '[0, 1]': -6.163157894736823,\n",
       "  '[0, -1]': -6.892157894736823},\n",
       " '[0, 3]': {'[1, 0]': -5.736842105263139,\n",
       "  '[-1, 0]': -6.163157894736823,\n",
       "  '[0, 1]': -6.163157894736823,\n",
       "  '[0, -1]': -6.546842105263139},\n",
       " '[1, 0]': {'[1, 0]': -6.546842105263139,\n",
       "  '[-1, 0]': -7.20294210526314,\n",
       "  '[0, 1]': -6.546842105263139,\n",
       "  '[0, -1]': -6.892157894736823},\n",
       " '[1, 1]': {'[1, 0]': -6.163157894736823,\n",
       "  '[-1, 0]': -6.892157894736823,\n",
       "  '[0, 1]': -6.163157894736823,\n",
       "  '[0, -1]': -6.892157894736823},\n",
       " '[1, 2]': {'[1, 0]': -5.736842105263139,\n",
       "  '[-1, 0]': -6.546842105263139,\n",
       "  '[0, 1]': -5.736842105263139,\n",
       "  '[0, -1]': -6.546842105263139},\n",
       " '[1, 3]': {'[1, 0]': -5.263157894736823,\n",
       "  '[-1, 0]': -6.163157894736823,\n",
       "  '[0, 1]': -5.736842105263139,\n",
       "  '[0, -1]': -6.163157894736823},\n",
       " '[2, 0]': {'[1, 0]': -6.163157894736823,\n",
       "  '[-1, 0]': -6.892157894736823,\n",
       "  '[0, 1]': -6.163157894736823,\n",
       "  '[0, -1]': -6.546842105263139},\n",
       " '[2, 1]': {'[1, 0]': -5.736842105263139,\n",
       "  '[-1, 0]': -6.546842105263139,\n",
       "  '[0, 1]': -5.736842105263139,\n",
       "  '[0, -1]': -6.546842105263139},\n",
       " '[2, 2]': {'[1, 0]': -6.163157894736823,\n",
       "  '[-1, 0]': -6.163157894736823,\n",
       "  '[0, 1]': -5.263157894736823,\n",
       "  '[0, -1]': -6.163157894736823},\n",
       " '[2, 3]': {'[1, 0]': -4.736842105263139,\n",
       "  '[-1, 0]': -5.736842105263139,\n",
       "  '[0, 1]': -5.263157894736823,\n",
       "  '[0, -1]': -5.736842105263139},\n",
       " '[3, 0]': {'[1, 0]': -5.736842105263139,\n",
       "  '[-1, 0]': -6.546842105263139,\n",
       "  '[0, 1]': -5.736842105263139,\n",
       "  '[0, -1]': -6.163157894736823},\n",
       " '[3, 1]': {'[1, 0]': -5.263157894736823,\n",
       "  '[-1, 0]': -6.163157894736823,\n",
       "  '[0, 1]': -6.163157894736823,\n",
       "  '[0, -1]': -6.163157894736823},\n",
       " '[3, 2]': {'[1, 0]': -5.736842105263139,\n",
       "  '[-1, 0]': -5.736842105263139,\n",
       "  '[0, 1]': -5.736842105263139,\n",
       "  '[0, -1]': -5.736842105263139},\n",
       " '[3, 3]': {'[1, 0]': -6.163157894736823,\n",
       "  '[-1, 0]': -5.263157894736823,\n",
       "  '[0, 1]': -5.736842105263139,\n",
       "  '[0, -1]': -6.163157894736823},\n",
       " '[4, 0]': {'[1, 0]': -5.736842105263139,\n",
       "  '[-1, 0]': -6.163157894736823,\n",
       "  '[0, 1]': -5.263157894736823,\n",
       "  '[0, -1]': -5.736842105263139},\n",
       " '[4, 1]': {'[1, 0]': -5.263157894736823,\n",
       "  '[-1, 0]': -5.736842105263139,\n",
       "  '[0, 1]': -4.736842105263139,\n",
       "  '[0, -1]': -5.736842105263139},\n",
       " '[4, 2]': {'[1, 0]': -5.736842105263139,\n",
       "  '[-1, 0]': -6.163157894736823,\n",
       "  '[0, 1]': -6.163157894736823,\n",
       "  '[0, -1]': -5.263157894736823},\n",
       " '[4, 3]': {'[1, 0]': -6.163157894736823,\n",
       "  '[-1, 0]': -5.736842105263139,\n",
       "  '[0, 1]': -6.163157894736823,\n",
       "  '[0, -1]': -5.736842105263139}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample_Q_planning(1000000, 0.9, 0.2, 'deterministic',test_sample_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Dyna-Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dyna_Q(num_trials, n, gamma, alpha, sample_model, epsilon):\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "            \n",
    "        Q[str(state)] = {}\n",
    "\n",
    "        for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "            Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    next_state = environment[2] #start state\n",
    "\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        Observation = {}\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        Actions = [[1, 0],[-1, 0],[0, 1],[0, -1]]\n",
    "\n",
    "        value_action_state = reverse_dictionary(Q[str(state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "        #Epsilon Greedy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "            selected_action = best_action\n",
    "        \n",
    "        else:\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "            Actions.remove(best_action)\n",
    "            epsilon_action = random.choice(Actions)\n",
    "\n",
    "            selected_action = epsilon_action \n",
    "\n",
    "        #As the book mentioned that \"assuming deterministic environment\"\n",
    "        new_state = [x + y for x, y in zip(current_state, selected_action)]\n",
    "\n",
    "        if new_state in environment[6]:\n",
    "\n",
    "            next_state = new_state\n",
    "        \n",
    "        else:\n",
    "\n",
    "            next_state = current_state\n",
    "\n",
    "        reward = state_reward(next_state)\n",
    "\n",
    "        value_action_state = reverse_dictionary(Q[str(next_state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "\n",
    "        Q[str(current_state)][str(selected_action)] = Q[str(current_state)][str(selected_action)] + alpha * (reward + gamma * Max_val - Q[str(current_state)][str(selected_action)])\n",
    "\n",
    "        sample_model[str(next_state)][str(selected_action)] = [reward, next_state]\n",
    "        \n",
    "        if str(next_state) not in list(Observation.keys()):\n",
    "\n",
    "            Observation[str(next_state)] = []\n",
    "\n",
    "\n",
    "        Observation[str(next_state)] = Observation[str(next_state)] + [selected_action]\n",
    "\n",
    "        #Loop repeat n times\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            rand_state = random.choice(list(Observation.keys()))\n",
    "\n",
    "            observed_actions = Observation[str(rand_state)]\n",
    "\n",
    "            rand_action = random.choice(observed_actions)\n",
    "\n",
    "            reward = sample_model[rand_state][str(rand_action)][0]\n",
    "\n",
    "            new_state = sample_model[rand_state][str(rand_action)][1]\n",
    "\n",
    "            value_action_state = reverse_dictionary(Q[str(new_state)])\n",
    "            Max_val = max(list(value_action_state.keys()))\n",
    "\n",
    "            Q[str(rand_state)][str(rand_action)] = Q[str(rand_state)][str(rand_action)] + alpha * (reward + gamma * Max_val - Q[str(rand_state)][str(rand_action)])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:12<00:00, 1377.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 0]': {'[1, 0]': 4.974413565101005e-09,\n",
       "  '[-1, 0]': 2.9565443422414644e-09,\n",
       "  '[0, 1]': 9.19572492471542e-09,\n",
       "  '[0, -1]': 4.543081010130831e-09},\n",
       " '[0, 1]': {'[1, 0]': -0.2999999948802594,\n",
       "  '[-1, 0]': -1.2699999953922332,\n",
       "  '[0, 1]': -5.6368647203166224,\n",
       "  '[0, -1]': -1.2699999953922332},\n",
       " '[0, 2]': {'[1, 0]': -6.969827730685684,\n",
       "  '[-1, 0]': -7.272844957617115,\n",
       "  '[0, 1]': -6.915741568195993,\n",
       "  '[0, -1]': -7.095865359396701},\n",
       " '[0, 3]': {'[1, 0]': -7.0879818046904814,\n",
       "  '[-1, 0]': -7.248376517772972,\n",
       "  '[0, 1]': -7.248376517772972,\n",
       "  '[0, -1]': -6.942640575303304},\n",
       " '[1, 0]': {'[1, 0]': 3.978026700837884e-09,\n",
       "  '[-1, 0]': 8.605723992687524e-09,\n",
       "  '[0, 1]': 6.0460054595622626e-09,\n",
       "  '[0, -1]': 9.085532240992951e-09},\n",
       " '[1, 1]': {'[1, 0]': -0.9999999984898461,\n",
       "  '[-1, 0]': 1.6779484108989953e-09,\n",
       "  '[0, 1]': -5.674901276128609,\n",
       "  '[0, -1]': -0.9999999984898463},\n",
       " '[1, 2]': {'[1, 0]': -7.317847485318706,\n",
       "  '[-1, 0]': -7.433934414198949,\n",
       "  '[0, 1]': -7.045785065041295,\n",
       "  '[0, -1]': -7.397926346788187},\n",
       " '[1, 3]': {'[1, 0]': -6.2701778727668,\n",
       "  '[-1, 0]': -6.806994718729882,\n",
       "  '[0, 1]': -6.643160085490119,\n",
       "  '[0, -1]': -7.24455041825492},\n",
       " '[2, 0]': {'[1, 0]': -5.846104661581195,\n",
       "  '[-1, 0]': -2.9999999927885814,\n",
       "  '[0, 1]': -0.812999993445911,\n",
       "  '[0, -1]': -2.999999992788581},\n",
       " '[2, 1]': {'[1, 0]': -3.6991381731622237,\n",
       "  '[-1, 0]': -1.899999996499229,\n",
       "  '[0, 1]': -5.73763620353851,\n",
       "  '[0, -1]': -2.7099999968493065},\n",
       " '[2, 2]': {'[1, 0]': -6.757713154834732,\n",
       "  '[-1, 0]': -6.41141452115682,\n",
       "  '[0, 1]': -6.365833481763125,\n",
       "  '[0, -1]': -6.672913305300727},\n",
       " '[2, 3]': {'[1, 0]': -6.3476660840873524,\n",
       "  '[-1, 0]': -6.47068081895063,\n",
       "  '[0, 1]': -6.511891925467788,\n",
       "  '[0, -1]': -6.124324361630877},\n",
       " '[3, 0]': {'[1, 0]': -5.946813140172612,\n",
       "  '[-1, 0]': -7.7389072571273205,\n",
       "  '[0, 1]': -5.415266455998204,\n",
       "  '[0, -1]': -7.7389072571273205},\n",
       " '[3, 1]': {'[1, 0]': -4.372181872715823,\n",
       "  '[-1, 0]': -5.675858826878097,\n",
       "  '[0, 1]': -6.996559075685826,\n",
       "  '[0, -1]': -5.764832334849109},\n",
       " '[3, 2]': {'[1, 0]': -7.670983676289592,\n",
       "  '[-1, 0]': -6.179227431380045,\n",
       "  '[0, 1]': -5.189981862543992,\n",
       "  '[0, -1]': -7.352980949048336},\n",
       " '[3, 3]': {'[1, 0]': -6.233110969279198,\n",
       "  '[-1, 0]': -5.989225760537742,\n",
       "  '[0, 1]': -6.233110969279198,\n",
       "  '[0, -1]': -6.703542861632032},\n",
       " '[4, 0]': {'[1, 0]': -5.841770215548186,\n",
       "  '[-1, 0]': -7.738906493209365,\n",
       "  '[0, 1]': -4.66338683289772,\n",
       "  '[0, -1]': -5.625620242071073},\n",
       " '[4, 1]': {'[1, 0]': -2.220670559443947,\n",
       "  '[-1, 0]': -5.5523503583671445,\n",
       "  '[0, 1]': -1.2565110154574224,\n",
       "  '[0, -1]': -2.0109579420379435},\n",
       " '[4, 2]': {'[1, 0]': -2.373017288709522,\n",
       "  '[-1, 0]': -6.600830090014456,\n",
       "  '[0, 1]': -0.0005283347989960608,\n",
       "  '[0, -1]': -1.0006792875987094},\n",
       " '[4, 3]': {'[1, 0]': 7.4e-323,\n",
       "  '[-1, 0]': -3.292676386018838,\n",
       "  '[0, 1]': 7.4e-323,\n",
       "  '[0, -1]': -1.0044822288317203}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dyna_Q(100000, 100, 0.9, 0.3, test_sample_model, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized sweeping for a deterministic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritized_sweeping(num_trials, n, gamma, alpha, sample_model, epsilon, theta):\n",
    "    \n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "            \n",
    "        Q[str(state)] = {}\n",
    "\n",
    "        for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "            Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    next_state = environment[2] #start state\n",
    "\n",
    "    PQueue = {}\n",
    "\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        random.seed(trial)\n",
    "\n",
    "        pqueue_initialized = False\n",
    "\n",
    "        #Observation = {}\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        Actions = [[1, 0],[-1, 0],[0, 1],[0, -1]]\n",
    "\n",
    "        value_action_state = reverse_dictionary(Q[str(state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "        #Epsilon Greedy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "            selected_action = best_action\n",
    "        \n",
    "        else:\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "            Actions.remove(best_action)\n",
    "            epsilon_action = random.choice(Actions)\n",
    "\n",
    "            selected_action = epsilon_action \n",
    "\n",
    "        #As the book mentioned that \"assuming deterministic environment\"\n",
    "        new_state = [x + y for x, y in zip(current_state, selected_action)]\n",
    "\n",
    "        if new_state in environment[6]:\n",
    "\n",
    "            next_state = new_state\n",
    "        \n",
    "        else:\n",
    "\n",
    "            next_state = current_state\n",
    "\n",
    "        reward = state_reward(next_state)\n",
    "\n",
    "        sample_model[str(next_state)][str(selected_action)] = [reward, next_state]\n",
    "\n",
    "        value_action_state = reverse_dictionary(Q[str(next_state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "\n",
    "        #Q[str(current_state)][str(selected_action)] = Q[str(current_state)][str(selected_action)] + alpha * (reward + gamma * Max_val - Q[str(current_state)][str(selected_action)])\n",
    "\n",
    "        P = abs(reward + gamma * Max_val - Q[str(current_state)][str(selected_action)])\n",
    "        \n",
    "        if P > theta:\n",
    "\n",
    "            PQueue[P] = [str(next_state) , str(selected_action)]\n",
    "            pqueue_initialized = True\n",
    "      \n",
    "        \n",
    "        #Loop repeat n times\n",
    "        \n",
    "        if pqueue_initialized:\n",
    "\n",
    "            for i in range(n):\n",
    "\n",
    "                Max_P = max(list(PQueue.keys()))\n",
    "\n",
    "                p_state = PQueue[Max_P][0]\n",
    "                p_action = PQueue[Max_P][1]\n",
    "\n",
    "                reward = sample_model[p_state][p_action][0]\n",
    "\n",
    "                new_state = sample_model[p_state][p_action][1]\n",
    "\n",
    "                value_action_state = reverse_dictionary(Q[str(new_state)])\n",
    "                Max_val = max(list(value_action_state.keys()))\n",
    "\n",
    "                Q[p_state][p_action] = Q[p_state][p_action] + alpha * (reward + gamma * Max_val - Q[p_state][p_action])\n",
    "\n",
    "                Neighbors = neighbor_cells_actions(current_state)\n",
    "\n",
    "                #PQueue.pop(Max_P)\n",
    "\n",
    "                for neighbor in list(Neighbors.keys()):\n",
    "\n",
    "                    if neighbor in environment[6]:\n",
    "\n",
    "                        reward_bar = sample_model[neighbor][str(Neighbors[neighbor])][0]\n",
    "\n",
    "                        value_action_state = reverse_dictionary(Q[p_state])\n",
    "                        Max_val = max(list(value_action_state.keys()))\n",
    "\n",
    "                        P = abs(reward_bar + gamma * Max_val - Q[neighbor][str(Neighbors[neighbor])])\n",
    "\n",
    "                        if P > theta:\n",
    "\n",
    "                            PQueue[P] = [str(neighbor) , str(Neighbors[neighbor])]\n",
    "\n",
    "                \n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:06<00:00, 1608.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 0]': {'[1, 0]': 2.4186645208625706e-09,\n",
       "  '[-1, 0]': 1.6804537041467109e-09,\n",
       "  '[0, 1]': 8.117227284243745e-09,\n",
       "  '[0, -1]': 3.568562474659183e-09},\n",
       " '[0, 1]': {'[1, 0]': 2.7053689225449517e-09,\n",
       "  '[-1, 0]': 7.992351572104794e-09,\n",
       "  '[0, 1]': 8.996999444543614e-09,\n",
       "  '[0, -1]': 3.72836759423407e-09},\n",
       " '[0, 2]': {'[1, 0]': 5.937771193889715e-09,\n",
       "  '[-1, 0]': 1.533590918322071e-09,\n",
       "  '[0, 1]': 6.610377540870549e-09,\n",
       "  '[0, -1]': 9.871855090982205e-09},\n",
       " '[0, 3]': {'[1, 0]': 5.261529117687289e-09,\n",
       "  '[-1, 0]': 8.823769392756114e-09,\n",
       "  '[0, 1]': 1.618134082319989e-09,\n",
       "  '[0, -1]': 3.6557091229120374e-09},\n",
       " '[1, 0]': {'[1, 0]': 2.8888866178959943e-09,\n",
       "  '[-1, 0]': 6.938325383800879e-09,\n",
       "  '[0, 1]': 2.6397428626196352e-09,\n",
       "  '[0, -1]': 6.25878894583558e-09},\n",
       " '[1, 1]': {'[1, 0]': 3.537075797887861e-09,\n",
       "  '[-1, 0]': 8.890823032263774e-09,\n",
       "  '[0, 1]': 4.6193176582994805e-09,\n",
       "  '[0, -1]': 5.374951183074789e-09},\n",
       " '[1, 2]': {'[1, 0]': 6.257975098370217e-09,\n",
       "  '[-1, 0]': 6.058873950457493e-09,\n",
       "  '[0, 1]': 9.253877059932134e-09,\n",
       "  '[0, -1]': 8.240159722719602e-09},\n",
       " '[1, 3]': {'[1, 0]': 6.098863085983547e-09,\n",
       "  '[-1, 0]': 2.68615679688885e-09,\n",
       "  '[0, 1]': -0.9999999931695882,\n",
       "  '[0, -1]': 7.589346493536841e-09},\n",
       " '[2, 0]': {'[1, 0]': 1.2221745845088394e-09,\n",
       "  '[-1, 0]': 4.444976421032533e-09,\n",
       "  '[0, 1]': 7.529145267991387e-09,\n",
       "  '[0, -1]': 2.034354856846812e-09},\n",
       " '[2, 1]': {'[1, 0]': 4.681850499397735e-09,\n",
       "  '[-1, 0]': 8.604097609029156e-09,\n",
       "  '[0, 1]': 9.068009600998607e-09,\n",
       "  '[0, -1]': 9.730359623415525e-09},\n",
       " '[2, 2]': {'[1, 0]': 2.1258477443949817e-09,\n",
       "  '[-1, 0]': 2.6584314962346027e-09,\n",
       "  '[0, 1]': 5.711936818185487e-09,\n",
       "  '[0, -1]': 7.320410758978722e-09},\n",
       " '[2, 3]': {'[1, 0]': 9.389647360774703e-09,\n",
       "  '[-1, 0]': 1.1131189989620373e-09,\n",
       "  '[0, 1]': 8.755433347531878e-09,\n",
       "  '[0, -1]': 9.841322213123052e-09},\n",
       " '[3, 0]': {'[1, 0]': 6.5931775809409715e-09,\n",
       "  '[-1, 0]': 5.553318121579033e-09,\n",
       "  '[0, 1]': 3.809719587044042e-09,\n",
       "  '[0, -1]': 7.423519317546338e-09},\n",
       " '[3, 1]': {'[1, 0]': 2.8568316388945184e-09,\n",
       "  '[-1, 0]': 2.8078642524521397e-09,\n",
       "  '[0, 1]': 5.828817453941757e-09,\n",
       "  '[0, -1]': 3.5756991125906287e-09},\n",
       " '[3, 2]': {'[1, 0]': 2.519331079961143e-09,\n",
       "  '[-1, 0]': 3.62754993709e-09,\n",
       "  '[0, 1]': -2.999999996735205,\n",
       "  '[0, -1]': -2.999999993249474},\n",
       " '[3, 3]': {'[1, 0]': -0.9999999979331311,\n",
       "  '[-1, 0]': 1.6955563026450191e-09,\n",
       "  '[0, 1]': 2.2965210378881967e-09,\n",
       "  '[0, -1]': 1.3108591410629518e-09},\n",
       " '[4, 0]': {'[1, 0]': 1.7418495826661863e-09,\n",
       "  '[-1, 0]': 8.66385600022426e-09,\n",
       "  '[0, 1]': 5.308059911720177e-09,\n",
       "  '[0, -1]': 2.043680604884112e-09},\n",
       " '[4, 1]': {'[1, 0]': 2.290567585309375e-09,\n",
       "  '[-1, 0]': 2.964394761136728e-09,\n",
       "  '[0, 1]': 8.772858529591064e-09,\n",
       "  '[0, -1]': 1.3063122711007528e-09},\n",
       " '[4, 2]': {'[1, 0]': 8.925059834686174e-09,\n",
       "  '[-1, 0]': 8.83454740696516e-09,\n",
       "  '[0, 1]': 1.6927334468196494e-09,\n",
       "  '[0, -1]': 6.890179104307809e-09},\n",
       " '[4, 3]': {'[1, 0]': 1.441787974655084e-09,\n",
       "  '[-1, 0]': 1.976741676485493e-09,\n",
       "  '[0, 1]': 4.2993288010666774e-09,\n",
       "  '[0, -1]': 1.4052845156341587e-09}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prioritized_sweeping(10000, 50, 0.9, 0.6, test_sample_model, 0.1, 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
