{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_world(length, width,path_lenght,holes_number,Random_State):\n",
    "    \n",
    "    random.seed(Random_State)\n",
    "    #store all cells in a list\n",
    "    Grid_Cells = []\n",
    "    for row in range(length):\n",
    "        for col in range(width):\n",
    "            Grid_Cells.append([row,col])\n",
    "\n",
    "\n",
    "    #specify the number of holes in the gridworld\n",
    "    \n",
    "    #specify the start point as a random cell\n",
    "    start = [random.randint(0, length), random.randint(0, width)]\n",
    "\n",
    "    #create a path from start point\n",
    "    \"\"\"instead of defining start and goal points,\n",
    "      we define just a start point and a random path with a random lenght to\n",
    "       another point and name it as goal point\"\"\"\n",
    "    \n",
    "    def random_path(Start, Path_Lenght,length, width):\n",
    "        \n",
    "        Path = []\n",
    "        Path.append(Start)\n",
    "        for i in range(Path_Lenght):\n",
    "            \n",
    "            #there are two moves that take us on a random cell named Goal [1,0], [0,1]\n",
    "            \n",
    "            move = random.choice([[1,0], [0,1]])\n",
    "            \n",
    "            #update the start cell/point by the above move\n",
    "            Start = [x + y for x, y in zip(Start, move)]\n",
    "            \n",
    "            #if the movement take us out of our gridworld, we reverse the change in the start point\n",
    "            if Start[0] < 0 or Start[1] < 0 or Start[0] > length-1 or Start[1] > width-1:\n",
    "\n",
    "                Start = [x - y for x, y in zip(Start, move)]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                #create a path history\n",
    "                Path.append(Start)\n",
    "\n",
    "        Goal = Start\n",
    "\n",
    "        return Goal,Path\n",
    "    \n",
    "\n",
    "    GoalPath = random_path(start, path_lenght,length, width)\n",
    "\n",
    "    goal = GoalPath[0]\n",
    "    path = GoalPath[1]\n",
    "\n",
    "    #now we must eliminate the path cells from the Grid_Cells to choose hole cells from remaining cells\n",
    "\n",
    "    FreeCells = [x for x in Grid_Cells if x not in path]\n",
    "\n",
    "    Holes = random.sample(FreeCells, holes_number)\n",
    "\n",
    "    #Also, we can visualize our gridworld in a simple way\n",
    "\n",
    "    def mark_holes(holes):\n",
    "        marked_data = [[\"Hole\" if [row, col] in holes else [row, col] for col in range(width)] for row in range(length)]\n",
    "        return marked_data\n",
    "    \n",
    "    marked_matrix = mark_holes(Holes)\n",
    "\n",
    "    print(tabulate(marked_matrix, tablefmt=\"grid\"))\n",
    "\n",
    "    \n",
    "    return length, width, start, goal, Holes, path,Grid_Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+\n",
      "| Hole   | [0, 1] | [0, 2] | [0, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [1, 0] | [1, 1] | [1, 2] | [1, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [2, 1] | [2, 2] | [2, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [3, 1] | Hole   | [3, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [4, 0] | [4, 1] | [4, 2] | [4, 3] |\n",
      "+--------+--------+--------+--------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 4,\n",
       " [1, 2],\n",
       " [4, 3],\n",
       " [[2, 0], [3, 2], [3, 0], [0, 0]],\n",
       " [[1, 2], [1, 3], [2, 3], [3, 3], [4, 3]],\n",
       " [[0, 0],\n",
       "  [0, 1],\n",
       "  [0, 2],\n",
       "  [0, 3],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [3, 0],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [4, 0],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = generate_grid_world(5, 4,4,4,39)\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_distribution(grid_size,randomness):\n",
    "    #random.seed(40)\n",
    "    \n",
    "    #by this function we generate probabilities which their sum is equal to 1\n",
    "    def generate_probabilities(n):\n",
    "\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "        \n",
    "        return scaled_numbers\n",
    "    \n",
    "    cells_prob = {}\n",
    "    if randomness == 'stochastic':\n",
    "        for cell in range(grid_size):\n",
    "            \n",
    "            #we set the number of probs to 4 due to 4 possible action for each cell (go to its neighbors)\n",
    "            probs = generate_probabilities(4)\n",
    "\n",
    "            cells_prob[cell] = probs\n",
    "    elif randomness == 'equal probable':\n",
    "\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.25,0.25,0.25,0.25]\n",
    "    \n",
    "    elif randomness == 'deterministic':\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.03,0.06,0.01,0.9] #[0,0,0,1] ##[0.15,.15,0.1,0.6]\n",
    "\n",
    "\n",
    "    #Note that we consider the correspondence between probabilities and actions as below:\n",
    "    #probs = [p1, p2, p3, p4] ---> [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    return cells_prob\n",
    "\n",
    "def neighbor_cells(cell):\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    Neighbors = []\n",
    "    Actions_Neighbors = []\n",
    "    for action in Actions:\n",
    "\n",
    "        neighbor = [x + y for x, y in zip(cell, action)]\n",
    "        #if neighbor not in environment[4]:\n",
    "        Neighbors.append(neighbor)\n",
    "        Actions_Neighbors.append(action)\n",
    "\n",
    "    return Neighbors, Actions_Neighbors\n",
    "\n",
    "#Note\n",
    "\"\"\"As we want to use monte carlo method for estimating the state values\n",
    "   it has been assumed that we have not any knowledge about the environment.\n",
    "   Therefore, we should consider the transitions into the holes cells\n",
    "   (against the case of policy iteration)\"\"\"\n",
    "\n",
    "def arbitrary_policy(randomness):\n",
    "    #random.seed(randomness)\n",
    "    \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            neighbors = neighbor_cells(state)[0]\n",
    "            Actions_Neighbors = neighbor_cells(state)[1]\n",
    "\n",
    "            allowed_positions = []\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                \n",
    "                if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                    \n",
    "                    allowed_positions.append(neighbor)\n",
    "        \n",
    "            next_state = random.choice(allowed_positions)\n",
    "\n",
    "            row = next_state[0] - state[0]\n",
    "            col = next_state[1] - state[1]\n",
    "            PolicyAction = [row, col]\n",
    "\n",
    "            policy['{}'.format(state)] = next_state\n",
    "            policy_action['{}'.format(state)] = PolicyAction\n",
    "\n",
    "\n",
    "    return policy, policy_action\n",
    "\n",
    "state_indice_dict = {}\n",
    "counter = 0\n",
    "for state in environment[6]:\n",
    "\n",
    "    state = str(state)\n",
    "    state_indice_dict[state] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "def generate_trajectory(policy,randomness):\n",
    "\n",
    "    policy_action = policy[1]\n",
    "\n",
    "    probs = probability_distribution(environment[0]*environment[1],'stochastic')\n",
    "    \n",
    "    start = environment[2]\n",
    "\n",
    "    terminate = start\n",
    "\n",
    "    trajectory = [start]\n",
    "    c = 0\n",
    "    test = []\n",
    "    while terminate != environment[3]:\n",
    "        random.seed(randomness+c)\n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "        action = policy_action[str(terminate)]\n",
    "        Actions.remove(action)\n",
    "        #sorted_actions = [action]\n",
    "        sorted_actions = Actions + [action]\n",
    "        #print(sorted_actions)\n",
    "        state_indice = state_indice_dict[str(terminate)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "        #print(actions_prob)\n",
    "        #print(actions_prob)\n",
    "\n",
    "\n",
    "        selected_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        \n",
    "\n",
    "        \"\"\"if c==0:\n",
    "           print(sorted_actions)\n",
    "           print(actions_prob)\n",
    "           print(selected_action)\n",
    "           test.append(selected_action)\"\"\"\n",
    "        \n",
    "        next_state = [x + y for x, y in zip(terminate, selected_action)]\n",
    "        \n",
    "        #if the agent goes out of the gridworld, it stays in its current state\n",
    "        if next_state not in environment[6]:\n",
    "\n",
    "            next_state = terminate\n",
    "        \n",
    "        #if it drops into the holes, it goes to the start points\n",
    "        elif next_state in environment[4]:\n",
    "\n",
    "            next_state = start\n",
    "\n",
    "        \n",
    "        terminate = next_state\n",
    "\n",
    "        trajectory.append(terminate)\n",
    "        c = c+1\n",
    "\n",
    "    return trajectory #,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[0, 1]': [0, 2],\n",
       "  '[0, 2]': [0, 1],\n",
       "  '[0, 3]': [0, 2],\n",
       "  '[1, 0]': [1, 1],\n",
       "  '[1, 1]': [2, 1],\n",
       "  '[1, 2]': [1, 3],\n",
       "  '[1, 3]': [1, 2],\n",
       "  '[2, 1]': [3, 1],\n",
       "  '[2, 2]': [2, 3],\n",
       "  '[2, 3]': [3, 3],\n",
       "  '[3, 1]': [4, 1],\n",
       "  '[3, 3]': [2, 3],\n",
       "  '[4, 0]': [4, 1],\n",
       "  '[4, 1]': [4, 2],\n",
       "  '[4, 2]': [4, 3],\n",
       "  '[4, 3]': [4, 2]},\n",
       " {'[0, 1]': [0, 1],\n",
       "  '[0, 2]': [0, -1],\n",
       "  '[0, 3]': [0, -1],\n",
       "  '[1, 0]': [0, 1],\n",
       "  '[1, 1]': [1, 0],\n",
       "  '[1, 2]': [0, 1],\n",
       "  '[1, 3]': [0, -1],\n",
       "  '[2, 1]': [1, 0],\n",
       "  '[2, 2]': [0, 1],\n",
       "  '[2, 3]': [1, 0],\n",
       "  '[3, 1]': [1, 0],\n",
       "  '[3, 3]': [-1, 0],\n",
       "  '[4, 0]': [0, 1],\n",
       "  '[4, 1]': [0, 1],\n",
       "  '[4, 2]': [0, 1],\n",
       "  '[4, 3]': [0, -1]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_0 = arbitrary_policy(41)\n",
    "policy_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-visit MC prediction, for estimationg $V \\approx v_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_reward(policy,state):\n",
    "\n",
    "    policy_state = policy[0]\n",
    "    \n",
    "    next_state = policy_state[str(state)]\n",
    "\n",
    "    if next_state in environment[4]:\n",
    "\n",
    "        r = -3\n",
    "    \n",
    "    elif next_state == environment[3]:\n",
    "\n",
    "        r = 100\n",
    "    \n",
    "    elif next_state not in environment[6]:\n",
    "\n",
    "        r = -2\n",
    "    \n",
    "    else:\n",
    "\n",
    "        r = -1\n",
    "    \n",
    "    return r\n",
    "\n",
    "#Note that here we want to evaluate just a fixed policy\n",
    "# and so we are not trying to optimize it \n",
    "def monte_carlo_prediction(num_trials, policy, gamma):\n",
    "\n",
    "    #V = np.zeros((environment[6],1))\n",
    "\n",
    "    #store returns of each trajectory\n",
    "    Returns = {} #np.zeros((environment[6],1))\n",
    "    #Lens = []\n",
    "    #Loop for ever (for each episode)\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "        \n",
    "        #generate an episode\n",
    "        trajectory = generate_trajectory(policy,trial)\n",
    "        #Lens.append(trajectory)\n",
    "\n",
    "        #limit the lenght of trajectory\n",
    "\n",
    "        #total reward\n",
    "        G = 0\n",
    "\n",
    "        trajectory.reverse()\n",
    "        \n",
    "        \n",
    "        returns = {}\n",
    "\n",
    "        for state in environment[6]:\n",
    "            \n",
    "            if state not in environment[4] and state != environment[3]:\n",
    "\n",
    "                returns[str(state)] = 0\n",
    "\n",
    "        first_visit = []\n",
    "        for step in trajectory[1:]:\n",
    "\n",
    "            if step not in first_visit:\n",
    "\n",
    "                first_visit.append(step)\n",
    "\n",
    "                r = state_reward(policy,step)\n",
    "\n",
    "                G = gamma * G + r\n",
    "\n",
    "                returns[str(step)] = returns[str(step)] + G\n",
    "        \n",
    "        #Returns[trial] = returns\n",
    "    \n",
    "    V = {}\n",
    "    for step in list(returns.keys()):\n",
    "\n",
    "        V[step] = returns[step]/num_trials\n",
    "    \n",
    "\n",
    "    return V,returns #, Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [06:10<00:00, 269.98it/s]\n"
     ]
    }
   ],
   "source": [
    "MC_prediction = monte_carlo_prediction(100000,policy_0,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': 0.00021067249012910008,\n",
       " '[0, 2]': 0.00017960524111619008,\n",
       " '[0, 3]': 0.0001516447170045711,\n",
       " '[1, 0]': 0.0005495390000000001,\n",
       " '[1, 1]': 0.0006217100000000001,\n",
       " '[1, 2]': 0.0004845851000000001,\n",
       " '[1, 3]': 0.00042612659000000006,\n",
       " '[2, 1]': 0.0007019000000000001,\n",
       " '[2, 2]': 0.0003261625379000001,\n",
       " '[2, 3]': 0.0002835462841100001,\n",
       " '[3, 1]': 0.000791,\n",
       " '[3, 3]': 0.00024519165569900005,\n",
       " '[4, 0]': 0.0003735139310000001,\n",
       " '[4, 1]': 0.00089,\n",
       " '[4, 2]': 0.001}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': 21.067249012910008,\n",
       " '[0, 2]': 17.96052411161901,\n",
       " '[0, 3]': 15.16447170045711,\n",
       " '[1, 0]': 54.95390000000001,\n",
       " '[1, 1]': 62.171000000000014,\n",
       " '[1, 2]': 48.45851000000001,\n",
       " '[1, 3]': 42.61265900000001,\n",
       " '[2, 1]': 70.19000000000001,\n",
       " '[2, 2]': 32.61625379000001,\n",
       " '[2, 3]': 28.354628411000007,\n",
       " '[3, 1]': 79.10000000000001,\n",
       " '[3, 3]': 24.519165569900007,\n",
       " '[4, 0]': 37.35139310000001,\n",
       " '[4, 1]': 89.0,\n",
       " '[4, 2]': 100.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC_prediction[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy first-visit MC control (for $\\epsilon$-soft policies), estimates ${\\pi} \\approx {\\pi}_{*} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_action_reward(policy,state):\n",
    "\n",
    "    \"\"\"if type(policy) == tuple:\n",
    "        \n",
    "        policy_state = policy[0]\n",
    "        next_state = policy_state[str(state)]\n",
    "    \n",
    "    else:\"\"\"\n",
    "    policy_action = policy[str(state)]\n",
    "    next_state = [x + y for x, y in zip(state, policy_action)]\n",
    "    \n",
    "\n",
    "    if next_state in environment[4]:\n",
    "\n",
    "        r = -3\n",
    "    \n",
    "    elif next_state == environment[3]:\n",
    "\n",
    "        r = 100\n",
    "    \n",
    "    elif next_state not in environment[6]:\n",
    "\n",
    "        r = -2\n",
    "    \n",
    "    else:\n",
    "\n",
    "        r = -1\n",
    "    \n",
    "    return r\n",
    "    \n",
    "\n",
    "def derive_action(current_state, next_state):\n",
    "\n",
    "    row = next_state[0] - current_state[0]\n",
    "    col = next_state[1] - current_state[1]\n",
    "    action = [row, col]\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "def generate_trajectory_probability_based(policy,randomness,epsilon,traj_len,action_prob_type):\n",
    "\n",
    "    probs = probability_distribution(environment[0]*environment[1],action_prob_type)\n",
    "  \n",
    "    start = environment[2]\n",
    "    terminate = start\n",
    "    trajectory = [start]\n",
    "    c = 0\n",
    "    test = []\n",
    "    while terminate != environment[3]:\n",
    "        random.seed(randomness+c)\n",
    "        Actions = [[1, 0],[-1, 0],[0, 1],[0, -1]]\n",
    "\n",
    "        #we have two probabilities for epsilon-greedy action selection\n",
    "        #It's a kind of exploration-exploitation balancing\n",
    "        \n",
    "        #probability for exploration on not best action values\n",
    "        low_prob = epsilon/len(Actions)\n",
    "        high_prob = 1 - epsilon #+ (epsilon/len(Actions))\n",
    "\n",
    "        #this random action selection is for balancing exploration-exploitation trade-off\n",
    "\n",
    "        exex_probs = [low_prob,low_prob,low_prob,high_prob]\n",
    "        if type(policy) == tuple:\n",
    "            policy = policy[1]\n",
    "        \n",
    "        best_action_value = policy[str(terminate)]\n",
    "        #print(type(best_action_value))\n",
    "        Actions_copy = Actions.copy()\n",
    "        #print(Actions_copy)\n",
    "        Actions_copy.remove(best_action_value)\n",
    "        exex_actions = Actions_copy + [best_action_value]\n",
    "        #print(exex_actions)\n",
    "        #print(exex_probs)\n",
    "        \n",
    "        action = random.choices(exex_actions, exex_probs)[0]\n",
    "\n",
    "        #second part of action selection\n",
    "        Actions.remove(action)\n",
    "        sorted_actions = Actions + [action]\n",
    "        state_indice = state_indice_dict[str(terminate)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "\n",
    "        #print(sorted_actions)\n",
    "        #print(actions_prob)\n",
    "        #this random action selection is due to the randomness of the environment\n",
    "        selected_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        #print(selected_action)\n",
    "        #print('=====')\n",
    "        next_state = [x + y for x, y in zip(terminate, selected_action)]\n",
    "        #if the agent goes out of the gridworld, it stays in its current state\n",
    "        if next_state not in environment[6]:\n",
    "            next_state = terminate\n",
    "        #if it drops into the holes, it goes to the start points\n",
    "        elif next_state in environment[4]:\n",
    "            next_state = start\n",
    "        terminate = next_state\n",
    "        trajectory.append(terminate)\n",
    "        c = c+1\n",
    "\n",
    "        if c >traj_len:\n",
    "            break\n",
    "            #c = traj_len + 1\n",
    "    \n",
    "    if c > traj_len:\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    else:\n",
    "        return trajectory\n",
    "    \n",
    "\n",
    "def OnPolicy_MC_prediction(num_trials, policy, gamma, epsilon,traj_len, action_prob_type):\n",
    "    \n",
    "    def reverse_dictionary(dict):\n",
    "        reverse_dict = {}\n",
    "        for key in list(dict.keys()):\n",
    "            val = dict[key]\n",
    "            reverse_dict[val] = key\n",
    "        return reverse_dict\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                #next_state = [x + y for x, y in zip(state, ast.literal_eval(action))]\n",
    "\n",
    "                #if (next_state in environment[6]) and next_state not in environment[4]:\n",
    "                    \n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    counter = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            counter[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                #next_state = [x + y for x, y in zip(state, ast.literal_eval(action))]\n",
    "\n",
    "                #if (next_state in environment[6]) and next_state not in environment[4]:\n",
    "                    \n",
    "                counter[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    done_trials = 0\n",
    "    Policies = [policy]\n",
    "    cp = 0\n",
    "    for trial in tqdm(range(1,num_trials)):\n",
    "        #print(policy['[3,3]'])\n",
    "        policy = Policies[cp]\n",
    "        trajectory = generate_trajectory_probability_based(policy, trial, epsilon,traj_len, action_prob_type)\n",
    "        #print(len(trajectory))\n",
    "\n",
    "        #if len(trajectory) < 100:\n",
    "        #print(trajectory)\n",
    "        \n",
    "        if trajectory:\n",
    "            #print(len(trajectory))\n",
    "\n",
    "            done_trials +=1 \n",
    "        \n",
    "\n",
    "            G = 0\n",
    "            returns = {}\n",
    "            first_visit = []\n",
    "\n",
    "            for state in environment[6]:\n",
    "\n",
    "                if state not in environment[4]:# and state != environment[3]:\n",
    "\n",
    "                    returns[str(state)] = {}\n",
    "\n",
    "            for state in environment[6]:\n",
    "                \n",
    "                if state not in environment[4]:# and state != environment[3]:\n",
    "\n",
    "                    for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                        #next_state = [x + y for x, y in zip(state, ast.literal_eval(action))]\n",
    "\n",
    "                        #if (next_state in environment[6]) and next_state not in environment[4]:\n",
    "\n",
    "                        returns[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "\n",
    "                \n",
    "            #print(returns)\n",
    "\n",
    "            for i in range(len(trajectory[1:])):\n",
    "                step = trajectory[1:][i]\n",
    "\n",
    "                if step not in first_visit:\n",
    "                    \n",
    "                    \"\"\"state_str = str(step)\n",
    "                    if state_str not in returns:\n",
    "                        returns[state_str] = {}\n",
    "                        for action in [\"[1,0]\", \"[-1,0]\", \"[0,1]\", \"[0,-1]\"]:\n",
    "                            returns[state_str][action] = 0\"\"\"  # Initialize all actions with value 0\n",
    "                            \n",
    "                    first_visit.append(step)\n",
    "                    #action = derive_action(trajectory[1:][i + 1], trajectory[1:][i])\n",
    "                    last_step = str(trajectory[1:][i])\n",
    "                    if type(policy) == tuple:\n",
    "                        policy = policy[1]\n",
    "                        \n",
    "                    action = policy[last_step]\n",
    "                    #if action == [0,0]:\n",
    "                    r = state_action_reward(policy, step)\n",
    "                    G = gamma * G + r\n",
    "                    #print(G)\n",
    "                    #action_str = str(action)\n",
    "                    #print(action_str)\n",
    "                    #print(returns[str(step)])\n",
    "                    returns[str(step)][str(action)] += G\n",
    "                    #print(returns[str(step)][action_str])\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            for state in list(returns.keys()):\n",
    "                for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "                    #print(state,action)\n",
    "                    #print('q',Q[state][action])\n",
    "                    #print(returns[state][action])\n",
    "                    #if returns[state][\"[-1, 0]\"] != 0:\n",
    "                    #    print(returns[state][\"[-1, 0]\"])\n",
    "                    #Q[state][action] = returns[state][action] / trial\n",
    "\n",
    "                    if abs(returns[state][action]) > 1e-3:\n",
    "\n",
    "                        counter[state][action] = counter[state][action] + 1\n",
    "\n",
    "                        Q[state][action] = Q[state][action] + returns[state][action]\n",
    "\n",
    "                        Q[state][action] = Q[state][action] / round(counter[state][action])\n",
    "                        #print('f')\n",
    "                    \n",
    "                    #else:\n",
    "\n",
    "                    #    Q[state][action] = Q[state][action] + returns[state][action]\n",
    "\n",
    "            policy = {}\n",
    "            for state in list(Q.keys()):\n",
    "                #print('d')\n",
    "                if Q[state] != {}:\n",
    "                    value_action_state = reverse_dictionary(Q[state])\n",
    "                    #print('value_action_state:',value_action_state)\n",
    "                    #print(state)\n",
    "                    #print(value_action_state)\n",
    "                    Max_val = max(list(value_action_state.keys()))\n",
    "                    best_action = value_action_state[Max_val]\n",
    "                    policy[state] = ast.literal_eval(best_action)\n",
    "            #print(policy)\n",
    "            #if policy != policy_0:\n",
    "            #    print('f')\n",
    "\n",
    "            Policies.append(policy)\n",
    "            cp = cp + 1\n",
    "            if cp == 100:\n",
    "                print(cp)\n",
    "    \n",
    "        \n",
    "    return policy, Q, done_trials, Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 148392/499999 [21:57<49:07, 119.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499999/499999 [1:01:52<00:00, 134.67it/s]\n"
     ]
    }
   ],
   "source": [
    "policy_0 = arbitrary_policy(41)\n",
    "first_try = OnPolicy_MC_prediction(500000, policy_0, 0.99, 0.01,500, 'deterministic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of done trials: 133695\n",
      "======\n",
      "The last/best policy:\n",
      "{'[0, 1]': [1, 0], '[0, 2]': [0, -1], '[0, 3]': [1, 0], '[1, 0]': [0, 1], '[1, 1]': [0, -1], '[1, 2]': [0, 1], '[1, 3]': [0, -1], '[2, 1]': [0, 1], '[2, 2]': [0, -1], '[2, 3]': [0, 1], '[3, 1]': [1, 0], '[3, 3]': [1, 0], '[4, 0]': [-1, 0], '[4, 1]': [-1, 0], '[4, 2]': [0, 1], '[4, 3]': [0, -1]}\n",
      "======\n",
      "The last Q-value:\n",
      "{'[0, 1]': {'[1, 0]': -1.8283883849486395e-05, '[-1, 0]': -10.561792490924729, '[0, 1]': -10.580512440120565, '[0, -1]': -13.444754677956233}, '[0, 2]': {'[1, 0]': -3.272345016064863, '[-1, 0]': -6.880996994491299, '[0, 1]': -4.930398993402942, '[0, -1]': -1.0091425198079127e-05}, '[0, 3]': {'[1, 0]': -1.994637377482096e-05, '[-1, 0]': -3.9700999933616004, '[0, 1]': -3.9700999894111524, '[0, -1]': -10.550257254364263}, '[1, 0]': {'[1, 0]': -9.754061204193054, '[-1, 0]': -5.445497494295858, '[0, 1]': -3.2437003371962826e-05, '[0, -1]': -4.940398983769299}, '[1, 1]': {'[1, 0]': -4.900994995343974, '[-1, 0]': -3.9501999898233535, '[0, 1]': -4.9302999894010515, '[0, -1]': -2.4203560538304563e-05}, '[1, 2]': {'[1, 0]': -3.9699999858034376, '[-1, 0]': -1.989999996352714, '[0, 1]': -1.5022324970059896e-05, '[0, -1]': -2.979999989367715}, '[1, 3]': {'[1, 0]': -4.930398993542756, '[-1, 0]': -9.646724495951535, '[0, 1]': -1.9999999843845548, '[0, -1]': -7.510608696039391e-06}, '[2, 1]': {'[1, 0]': -9.685826156933889, '[-1, 0]': -8.73406514565274, '[0, 1]': -3.6393008905575904e-05, '[0, -1]': -7.852081591073343}, '[2, 2]': {'[1, 0]': -6.876832595499806, '[-1, 0]': -5.881095005283653, '[0, 1]': -8.763175098832997, '[0, -1]': -4.44859052653781e-05}, '[2, 3]': {'[1, 0]': -5.881095000005422, '[-1, 0]': -9.675543351054452, '[0, 1]': -5.326864922541928e-05, '[0, -1]': -7.812187021597797}, '[3, 1]': {'[1, 0]': -0.00022693349276598198, '[-1, 0]': -8.648275241298021, '[0, 1]': -16.310307132970216, '[0, -1]': -16.384470585688444}, '[3, 3]': {'[1, 0]': 0.0007802896209172406, '[-1, 0]': -3.960099991815933, '[0, 1]': -11.578787909798322, '[0, -1]': -8.822284052761738}, '[4, 0]': {'[1, 0]': -9.715530540575967, '[-1, 0]': -0.0023733310173844947, '[0, 1]': -17.975732018635465, '[0, -1]': -14.189377838848108}, '[4, 1]': {'[1, 0]': -7.793465205807074, '[-1, 0]': -0.00028852129557654845, '[0, 1]': -10.009554356171222, '[0, -1]': -17.147204066830803}, '[4, 2]': {'[1, 0]': 9.099177279835424e-09, '[-1, 0]': 9.381843660634019e-09, '[0, 1]': 0.006665201428871299, '[0, -1]': 7.661993025568137e-09}, '[4, 3]': {'[1, 0]': 2.9592502752794698e-09, '[-1, 0]': 5.509788519535226e-09, '[0, 1]': 2.704705557362288e-09, '[0, -1]': 0.0006827857325053389}}\n"
     ]
    }
   ],
   "source": [
    "print('number of done trials:',first_try[2])\n",
    "print('======')\n",
    "print('The last/best policy:')\n",
    "print(first_try[0])\n",
    "print('======')\n",
    "print('The last Q-value:')\n",
    "print(first_try[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': [0, 1],\n",
       " '[0, 2]': [1, 0],\n",
       " '[0, 3]': [0, -1],\n",
       " '[1, 0]': [0, 1],\n",
       " '[1, 1]': [1, 0],\n",
       " '[1, 2]': [-1, 0],\n",
       " '[1, 3]': [0, 1],\n",
       " '[2, 1]': [1, 0],\n",
       " '[2, 2]': [-1, 0],\n",
       " '[2, 3]': [-1, 0],\n",
       " '[3, 1]': [-1, 0],\n",
       " '[3, 3]': [1, 0],\n",
       " '[4, 0]': [1, 0],\n",
       " '[4, 1]': [0, 1],\n",
       " '[4, 2]': [0, 1],\n",
       " '[4, 3]': [-1, 0]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_try[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimized Action-Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -2.979999987139254,\n",
       "  '[-1, 0]': -5.910697986174125,\n",
       "  '[0, 1]': -3.573587644419302e-05,\n",
       "  '[0, -1]': -6.900995004712646},\n",
       " '[0, 2]': {'[1, 0]': -7.909637128694817e-06,\n",
       "  '[-1, 0]': -1.9999999919938294,\n",
       "  '[0, 1]': -5.85198505216081,\n",
       "  '[0, -1]': -3.9501999848769587},\n",
       " '[0, 3]': {'[1, 0]': -4.900995006840813,\n",
       "  '[-1, 0]': -5.9106979888099715,\n",
       "  '[0, 1]': -4.940398984044857,\n",
       "  '[0, -1]': -2.8524644595298792e-05},\n",
       " '[1, 0]': {'[1, 0]': -11.62837523523017,\n",
       "  '[-1, 0]': -9.7637641968131,\n",
       "  '[0, 1]': -6.039999989894305e-05,\n",
       "  '[0, -1]': -10.628375246559365},\n",
       " '[1, 1]': {'[1, 0]': -5.3833626427161314e-05,\n",
       "  '[-1, 0]': -12.57829191040981,\n",
       "  '[0, 1]': -6.793465194072248,\n",
       "  '[0, -1]': -6.832085048349945},\n",
       " '[1, 2]': {'[1, 0]': -3.9403989864352393,\n",
       "  '[-1, 0]': -1.5728423714962523e-05,\n",
       "  '[0, 1]': -6.851591017026077,\n",
       "  '[0, -1]': -3.9403989821862577},\n",
       " '[1, 3]': {'[1, 0]': -8.739883627714896,\n",
       "  '[-1, 0]': -6.851591012512707,\n",
       "  '[0, 1]': -6.046618060246079e-05,\n",
       "  '[0, -1]': -5.89099500409816},\n",
       " '[2, 1]': {'[1, 0]': -6.104190801146548e-05,\n",
       "  '[-1, 0]': -8.705244350944822,\n",
       "  '[0, 1]': -4.900995003121992,\n",
       "  '[0, -1]': -10.753665201820265},\n",
       " '[2, 2]': {'[1, 0]': -11.695244351221033,\n",
       "  '[-1, 0]': -0.00010775417531573437,\n",
       "  '[0, 1]': -5.7610049919248825,\n",
       "  '[0, -1]': -10.666126559255748},\n",
       " '[2, 3]': {'[1, 0]': -11.416789884034971,\n",
       "  '[-1, 0]': -0.00038480165393759813,\n",
       "  '[0, 1]': -8.783075095703392,\n",
       "  '[0, -1]': -11.559465294863715},\n",
       " '[3, 1]': {'[1, 0]': -5.851985050737495,\n",
       "  '[-1, 0]': -6.828934998659707e-05,\n",
       "  '[0, 1]': -13.646128544817278,\n",
       "  '[0, -1]': -6.90099500291775},\n",
       " '[3, 3]': {'[1, 0]': 0.1371051197654729,\n",
       "  '[-1, 0]': 7.67339892849306e-09,\n",
       "  '[0, 1]': 1.3674531036561743e-09,\n",
       "  '[0, -1]': 9.483215871158506e-09},\n",
       " '[4, 0]': {'[1, 0]': -0.0010093181875277472,\n",
       "  '[-1, 0]': -14.312502875026443,\n",
       "  '[0, 1]': -13.331612823932213,\n",
       "  '[0, -1]': -9.715530545110402},\n",
       " '[4, 1]': {'[1, 0]': -10.124819881080112,\n",
       "  '[-1, 0]': -11.456174567510349,\n",
       "  '[0, 1]': -8.987854392400455e-05,\n",
       "  '[0, -1]': -11.426770572988488},\n",
       " '[4, 2]': {'[1, 0]': -13.312502865145891,\n",
       "  '[-1, 0]': -13.512191487935647,\n",
       "  '[0, 1]': 0.0006989105201123967,\n",
       "  '[0, -1]': -8.715530544044048},\n",
       " '[4, 3]': {'[1, 0]': 4.575241335277369e-09,\n",
       "  '[-1, 0]': 0.0006803385096810913,\n",
       "  '[0, 1]': 5.395813196101065e-09,\n",
       "  '[0, -1]': 1.1526676244773544e-09}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_try[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
