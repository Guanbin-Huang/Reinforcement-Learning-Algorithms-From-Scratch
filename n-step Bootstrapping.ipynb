{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_world(length, width,path_lenght,holes_number,Random_State):\n",
    "    \n",
    "    random.seed(Random_State)\n",
    "    #store all cells in a list\n",
    "    Grid_Cells = []\n",
    "    for row in range(length):\n",
    "        for col in range(width):\n",
    "            Grid_Cells.append([row,col])\n",
    "\n",
    "\n",
    "    #specify the number of holes in the gridworld\n",
    "    \n",
    "    #specify the start point as a random cell\n",
    "    start = [random.randint(0, length), random.randint(0, width)]\n",
    "\n",
    "    #create a path from start point\n",
    "    \"\"\"instead of defining start and goal points,\n",
    "      we define just a start point and a random path with a random lenght to\n",
    "       another point and name it as goal point\"\"\"\n",
    "    \n",
    "    def random_path(Start, Path_Lenght,length, width):\n",
    "        \n",
    "        Path = []\n",
    "        Path.append(Start)\n",
    "        for i in range(Path_Lenght):\n",
    "            \n",
    "            #there are two moves that take us on a random cell named Goal [1,0], [0,1]\n",
    "            \n",
    "            move = random.choice([[1,0], [0,1]])\n",
    "            \n",
    "            #update the start cell/point by the above move\n",
    "            Start = [x + y for x, y in zip(Start, move)]\n",
    "            \n",
    "            #if the movement take us out of our gridworld, we reverse the change in the start point\n",
    "            if Start[0] < 0 or Start[1] < 0 or Start[0] > length-1 or Start[1] > width-1:\n",
    "\n",
    "                Start = [x - y for x, y in zip(Start, move)]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                #create a path history\n",
    "                Path.append(Start)\n",
    "\n",
    "        Goal = Start\n",
    "\n",
    "        return Goal,Path\n",
    "    \n",
    "\n",
    "    GoalPath = random_path(start, path_lenght,length, width)\n",
    "\n",
    "    goal = GoalPath[0]\n",
    "    path = GoalPath[1]\n",
    "\n",
    "    #now we must eliminate the path cells from the Grid_Cells to choose hole cells from remaining cells\n",
    "\n",
    "    FreeCells = [x for x in Grid_Cells if x not in path]\n",
    "\n",
    "    Holes = random.sample(FreeCells, holes_number)\n",
    "\n",
    "    #Also, we can visualize our gridworld in a simple way\n",
    "\n",
    "    def mark_holes(holes):\n",
    "        marked_data = [[\"Hole\" if [row, col] in holes else [row, col] for col in range(width)] for row in range(length)]\n",
    "        return marked_data\n",
    "    \n",
    "    marked_matrix = mark_holes(Holes)\n",
    "\n",
    "    print(tabulate(marked_matrix, tablefmt=\"grid\"))\n",
    "\n",
    "    \n",
    "    return length, width, start, goal, Holes, path,Grid_Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+\n",
      "| Hole   | [0, 1] | [0, 2] | [0, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [1, 0] | [1, 1] | [1, 2] | [1, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [2, 1] | [2, 2] | [2, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| Hole   | [3, 1] | Hole   | [3, 3] |\n",
      "+--------+--------+--------+--------+\n",
      "| [4, 0] | [4, 1] | [4, 2] | [4, 3] |\n",
      "+--------+--------+--------+--------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 4,\n",
       " [1, 2],\n",
       " [4, 3],\n",
       " [[2, 0], [3, 2], [3, 0], [0, 0]],\n",
       " [[1, 2], [1, 3], [2, 3], [3, 3], [4, 3]],\n",
       " [[0, 0],\n",
       "  [0, 1],\n",
       "  [0, 2],\n",
       "  [0, 3],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [3, 0],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [4, 0],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = generate_grid_world(5, 4,4,4,39)\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_distribution(grid_size,randomness):\n",
    "    #random.seed(40)\n",
    "    \n",
    "    #by this function we generate probabilities which their sum is equal to 1\n",
    "    def generate_probabilities(n):\n",
    "\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "        \n",
    "        return scaled_numbers\n",
    "    \n",
    "    cells_prob = {}\n",
    "    if randomness == 'stochastic':\n",
    "        for cell in range(grid_size):\n",
    "            \n",
    "            #we set the number of probs to 4 due to 4 possible action for each cell (go to its neighbors)\n",
    "            probs = generate_probabilities(4)\n",
    "\n",
    "            cells_prob[cell] = probs\n",
    "    elif randomness == 'equal probable':\n",
    "\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.25,0.25,0.25,0.25]\n",
    "    \n",
    "    elif randomness == 'deterministic':\n",
    "        for cell in range(grid_size):\n",
    "\n",
    "            cells_prob[cell] = [0.03,0.06,0.01,0.9] #[0,0,0,1] ##[0.15,.15,0.1,0.6]\n",
    "\n",
    "\n",
    "    #Note that we consider the correspondence between probabilities and actions as below:\n",
    "    #probs = [p1, p2, p3, p4] ---> [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    return cells_prob\n",
    "\n",
    "def neighbor_cells(cell):\n",
    "\n",
    "    grid_cells = environment[6]\n",
    "    Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    Neighbors = []\n",
    "    Actions_Neighbors = []\n",
    "    for action in Actions:\n",
    "\n",
    "        neighbor = [x + y for x, y in zip(cell, action)]\n",
    "        #if neighbor not in environment[4]:\n",
    "        Neighbors.append(neighbor)\n",
    "        Actions_Neighbors.append(action)\n",
    "\n",
    "    return Neighbors, Actions_Neighbors\n",
    "\n",
    "#Note\n",
    "\"\"\"As we want to use monte carlo method for estimating the state values\n",
    "   it has been assumed that we have not any knowledge about the environment.\n",
    "   Therefore, we should consider the transitions into the holes cells\n",
    "   (against the case of policy iteration)\"\"\"\n",
    "\n",
    "def arbitrary_policy(randomness):\n",
    "    #random.seed(randomness)\n",
    "    \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            neighbors = neighbor_cells(state)[0]\n",
    "            Actions_Neighbors = neighbor_cells(state)[1]\n",
    "\n",
    "            allowed_positions = []\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                \n",
    "                if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                    \n",
    "                    allowed_positions.append(neighbor)\n",
    "        \n",
    "            next_state = random.choice(allowed_positions)\n",
    "\n",
    "            row = next_state[0] - state[0]\n",
    "            col = next_state[1] - state[1]\n",
    "            PolicyAction = [row, col]\n",
    "\n",
    "            policy['{}'.format(state)] = next_state\n",
    "            policy_action['{}'.format(state)] = PolicyAction\n",
    "\n",
    "\n",
    "    return policy, policy_action\n",
    "\n",
    "def state_reward(next_state):\n",
    "\n",
    "    if next_state in environment[4]:\n",
    "\n",
    "        r = -3\n",
    "    \n",
    "    elif next_state == environment[3]:\n",
    "\n",
    "        r = 100\n",
    "    \n",
    "    elif next_state not in environment[6]:\n",
    "\n",
    "        r = -2\n",
    "    \n",
    "    else:\n",
    "\n",
    "        r = -1\n",
    "    \n",
    "    return r\n",
    "\n",
    "def reverse_dictionary(dict):\n",
    "    reverse_dict = {}\n",
    "    for key in list(dict.keys()):\n",
    "        val = dict[key]\n",
    "        reverse_dict[val] = key\n",
    "    return reverse_dict\n",
    "\n",
    "\n",
    "state_indice_dict = {}\n",
    "counter = 0\n",
    "for state in environment[6]:\n",
    "\n",
    "    state = str(state)\n",
    "    state_indice_dict[state] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "def generate_trajectory(policy,randomness,environment_stochasticity):\n",
    "\n",
    "    policy_action = policy[1]\n",
    "    probs = probability_distribution(environment[0]*environment[1],environment_stochasticity)\n",
    "    start = environment[2]\n",
    "    terminate = start\n",
    "    trajectory = []\n",
    "    pure_trajectory = [start]\n",
    "    c = 0\n",
    "    while terminate != environment[3]:\n",
    "        random.seed(randomness+c)\n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        action = policy_action[str(terminate)]\n",
    "        Actions.remove(action)\n",
    "        sorted_actions = Actions + [action]\n",
    "        state_indice = state_indice_dict[str(terminate)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "\n",
    "        selected_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        current_state = terminate\n",
    "        next_state = [x + y for x, y in zip(terminate, selected_action)]\n",
    "        pure_trajectory.append(next_state)\n",
    "        \n",
    "        #if the agent goes out of the gridworld, it stays in its current state\n",
    "        if next_state not in environment[6]:\n",
    "            next_state = terminate\n",
    "        \n",
    "        #if it drops into the holes, it goes to the start points\n",
    "        elif next_state in environment[4]:\n",
    "            next_state = start  \n",
    "\n",
    "        terminate = next_state\n",
    "        trajectory.append((current_state,selected_action))\n",
    "        c = c+1\n",
    "    \n",
    "    trajectory.append((environment[3],[0,0]))\n",
    "    pure_trajectory.append(environment[3])\n",
    "\n",
    "    return trajectory,pure_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-step TD for estimating $V \\approx  v_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_TD(num_trials, n, policy, gamma, alpha, environment_stochasticity):\n",
    "\n",
    "    V = {}\n",
    "    for state in environment[6]:\n",
    "    \n",
    "        if state not in environment[4]:\n",
    "\n",
    "            V[str(state)] = 0\n",
    "    \n",
    "    indice_state_dict = {}\n",
    "    counter = 0\n",
    "    for state in environment[6]:\n",
    "\n",
    "        #state = str(state)\n",
    "        indice_state_dict[counter] = state\n",
    "        counter = counter + 1\n",
    "    \n",
    "    state_policy = policy[0]\n",
    "    action_policy = policy[1]\n",
    "\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        TRAJECTORY = generate_trajectory(policy,trial,environment_stochasticity)\n",
    "\n",
    "        trajectory = TRAJECTORY[0]\n",
    "        pure_trajectory = TRAJECTORY[1]\n",
    "        \n",
    "        T = float('inf')\n",
    "        tau = 0\n",
    "        t = -1\n",
    "        while tau != T - 1:\n",
    "\n",
    "            t = t + 1\n",
    "\n",
    "            if t < T:\n",
    "                \n",
    "                #t_state = indice_state_dict[t]\n",
    "                #next_state = state_policy[str(t_state)]\n",
    "                \n",
    "\n",
    "                if pure_trajectory[t+1] == environment[3]:\n",
    "                    T = t + 1\n",
    "            \n",
    "            tau = t - n + 1\n",
    "\n",
    "            if tau >= 0:\n",
    "\n",
    "                G = 0\n",
    "\n",
    "                for i in range(tau+1, min(tau+n,T)+1):\n",
    "\n",
    "                    r = state_reward(pure_trajectory[i+1])\n",
    "\n",
    "                    G = G + (gamma ** (i-tau-1)) * r\n",
    "\n",
    "                if tau + n < T:\n",
    "\n",
    "                    #print(tau , n)\n",
    "                    \n",
    "                    tau_n_state = trajectory[tau + n] #indice_state_dict[tau + n]\n",
    "\n",
    "                    if tau_n_state not in environment[4] and tau_n_state != environment[3]:\n",
    "\n",
    "                        G = G + (gamma ** n) * V[str(tau_n_state)]\n",
    "                \n",
    "                tau_state = trajectory[tau] #indice_state_dict[tau]\n",
    "                #tau\n",
    "                if tau_state not in environment[4]:\n",
    "                    #print(type(tau_state))\n",
    "\n",
    "\n",
    "                    V[str(tau_state)] = V[str(tau_state)] + alpha * (G - V[str(tau_state)])\n",
    "        \n",
    "\n",
    "    \n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:21<00:00,  4.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': -1.4285724302339287,\n",
       " '[0, 2]': -1.443169885479679,\n",
       " '[0, 3]': -2.3288197695616843,\n",
       " '[1, 0]': -1.4289050421621532,\n",
       " '[1, 1]': -1.428804696583943,\n",
       " '[1, 2]': -1.42903754063062,\n",
       " '[1, 3]': -1.4504922034868155,\n",
       " '[2, 1]': -1.4287574405696546,\n",
       " '[2, 2]': 34.05655312979737,\n",
       " '[2, 3]': 116.85704179643528,\n",
       " '[3, 1]': 0,\n",
       " '[3, 3]': 100.0,\n",
       " '[4, 0]': 0,\n",
       " '[4, 1]': 0,\n",
       " '[4, 2]': 0,\n",
       " '[4, 3]': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_0 = arbitrary_policy(41)\n",
    "\n",
    "n_step_TD(1000, 2, policy_0, 0.3, 0.9, 'deterministic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': -1.4301128100715403,\n",
       " '[0, 2]': -1.575520135560438,\n",
       " '[0, 3]': -2.5984038493598653,\n",
       " '[1, 0]': -1.4629465272519007,\n",
       " '[1, 1]': -3.2805724318680447,\n",
       " '[1, 2]': 8.202714452634787,\n",
       " '[1, 3]': -1.4410685563616235,\n",
       " '[2, 1]': -1.4347263015381202,\n",
       " '[2, 2]': 34.05743741875119,\n",
       " '[2, 3]': 128.81786799436864,\n",
       " '[3, 1]': -1.5939095375377672,\n",
       " '[3, 3]': 100.0,\n",
       " '[4, 0]': -1.633465751738598,\n",
       " '[4, 1]': 117.33555165854531,\n",
       " '[4, 2]': 94.45066221705233,\n",
       " '[4, 3]': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_TD(1000, 2, policy_0, 0.3, 0.9, 'stochastic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-step Sarsa for estimating $Q \\approx q_{*}$ or $q_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_sarsa(num_trials, n, gamma, alpha, environment_stochasticity,epsilon):\n",
    "    \n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "    \n",
    "    def state_action_nextstate(current_state):\n",
    "\n",
    "        if type(current_state) == str:\n",
    "\n",
    "            state = ast.literal_eval(current_state)\n",
    "        else:\n",
    "            state = current_state\n",
    "        #Choose action using policy derived from Q===================================\n",
    "        value_action_state = reverse_dictionary(Q[str(state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "\n",
    "        #============================================================================\n",
    "        #Epsilon Greedy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "            selected_action = best_action\n",
    "        \n",
    "        else:\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "            Actions.remove(best_action)\n",
    "            epsilon_action = random.choice(Actions)\n",
    "\n",
    "            selected_action = epsilon_action\n",
    "        #============================================================================\n",
    "        \n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        Actions.remove(selected_action)\n",
    "        sorted_actions = Actions + [selected_action]\n",
    "        state_indice = state_indice_dict[str(state)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "        #due to stochasticity of the environment\n",
    "        Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        #print(type(state), type(Final_action))\n",
    "        \n",
    "        next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "        if next_state not in environment[6] or next_state in environment[4]:\n",
    "\n",
    "            next_state = current_state\n",
    "\n",
    "        return Final_action, next_state\n",
    "\n",
    "\n",
    "    state_t  = environment[2]\n",
    "\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "        \n",
    "        T = float('inf')\n",
    "        tau = 0\n",
    "        t = -1\n",
    "        trajectory = []\n",
    "        state_t  = environment[2]\n",
    "        while tau != T - 1:\n",
    "\n",
    "            t = t + 1\n",
    "\n",
    "            if t < T:\n",
    "                trajectory.append(state_t)\n",
    "                next_state = state_action_nextstate(state_t)[1]\n",
    "                #last_state = state_t\n",
    "                state_t = next_state\n",
    "                \n",
    "\n",
    "                if next_state == environment[3]:\n",
    "                    T = t + 1\n",
    "                \n",
    "                #else:\n",
    "                #    next_action = \n",
    "\n",
    "            \n",
    "            tau = t - n + 1\n",
    "\n",
    "            if tau >= 0:\n",
    "\n",
    "                G = 0\n",
    "                \n",
    "                state_tau = trajectory[tau]\n",
    "                state_i = state_tau\n",
    "                action_tau = state_action_nextstate(state_tau)[0]\n",
    "\n",
    "                action_ii = 0\n",
    "                for i in range(tau+1, min(tau+n,T)+1):\n",
    "\n",
    "                    #actionTN = action_ii\n",
    "\n",
    "                    state_ii = state_action_nextstate(state_i)[1]\n",
    "                    action_ii = state_action_nextstate(state_i)[0]\n",
    "\n",
    "                    #stateTN = state_i #s_(t+n) this is store for the next part\n",
    "                     \n",
    "\n",
    "                    #n_state_action.append((state_ii, action_ii))\n",
    "\n",
    "                    #state_i = state_ii\n",
    "                    r = state_reward(state_ii)\n",
    "\n",
    "                    G = G + (gamma ** (i-tau-1)) * r\n",
    "\n",
    "                    if i < min(tau+n,T):\n",
    "\n",
    "                        state_i = state_ii\n",
    "\n",
    "                if tau + n < T:\n",
    "\n",
    "                    stateTN = state_action_nextstate(state_ii)[1]\n",
    "                    actionTN = state_action_nextstate(state_ii)[0]\n",
    "\n",
    "                    #print(tau , n)\n",
    "                    \n",
    "                    #tau_n_state = trajectory[tau + n] #indice_state_dict[tau + n]\n",
    "\n",
    "                    #if state_ii not in environment[4] and state_ii != environment[3]:\n",
    "\n",
    "                    G = G + (gamma ** n) * Q[str(stateTN)][str(actionTN)]\n",
    "\n",
    "                if state_tau not in environment[4] and state_tau != environment[3]:\n",
    "                    #print(type(tau_state))\n",
    "\n",
    "\n",
    "                    Q[str(state_tau)][str(action_tau)] = Q[str(state_tau)][str(action_tau)] + alpha * (G - Q[str(state_tau)][str(action_tau)])\n",
    "    \n",
    "    del Q[str(environment[3])]\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:22<00:00, 443.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -1.9998506871277644,\n",
       "  '[-1, 0]': -1.999973573566599,\n",
       "  '[0, 1]': -1.9991081569850793,\n",
       "  '[0, -1]': -1.9998815608527245},\n",
       " '[0, 2]': {'[1, 0]': -1.9998660868462808,\n",
       "  '[-1, 0]': -1.9999981834913314,\n",
       "  '[0, 1]': -1.9529836722854879,\n",
       "  '[0, -1]': -1.9999908086645948},\n",
       " '[0, 3]': {'[1, 0]': 18.9047252921085,\n",
       "  '[-1, 0]': -1.999933890806153,\n",
       "  '[0, 1]': -1.9998340930800869,\n",
       "  '[0, -1]': -1.999970321163092},\n",
       " '[1, 0]': {'[1, 0]': -1.9990593415278748,\n",
       "  '[-1, 0]': -1.9999200543396722,\n",
       "  '[0, 1]': -1.9871698061532683,\n",
       "  '[0, -1]': -1.9999796422954659},\n",
       " '[1, 1]': {'[1, 0]': -1.9281429767641176,\n",
       "  '[-1, 0]': -1.9999938530855874,\n",
       "  '[0, 1]': -1.9186346046362386,\n",
       "  '[0, -1]': -1.996230986689947},\n",
       " '[1, 2]': {'[1, 0]': 0.29603812201068536,\n",
       "  '[-1, 0]': -1.9984342969597275,\n",
       "  '[0, 1]': -1.9997616813639847,\n",
       "  '[0, -1]': -1.9999667916798454},\n",
       " '[1, 3]': {'[1, 0]': -1.9924411095024679,\n",
       "  '[-1, 0]': -1.9978947113381718,\n",
       "  '[0, 1]': 18.904850597488544,\n",
       "  '[0, -1]': -1.9998464286390722},\n",
       " '[2, 1]': {'[1, 0]': 1.9536580853819387,\n",
       "  '[-1, 0]': -1.979208960143322,\n",
       "  '[0, 1]': -1.9818209720217754,\n",
       "  '[0, -1]': -1.989192683412346},\n",
       " '[2, 2]': {'[1, 0]': -1.9935220086027718,\n",
       "  '[-1, 0]': -1.7721050445669286,\n",
       "  '[0, 1]': 0.729541064368572,\n",
       "  '[0, -1]': -1.5499365107822498},\n",
       " '[2, 3]': {'[1, 0]': 48.9743833100135,\n",
       "  '[-1, 0]': -1.5272560863021756,\n",
       "  '[0, 1]': -1.3861355538275386,\n",
       "  '[0, -1]': -1.4989462879380773},\n",
       " '[3, 1]': {'[1, 0]': 21.00171471548849,\n",
       "  '[-1, 0]': -1.8148023661466777,\n",
       "  '[0, 1]': -1.5495793879019408,\n",
       "  '[0, -1]': -1.5494488356522513},\n",
       " '[3, 3]': {'[1, 0]': 13.599090912645096,\n",
       "  '[-1, 0]': -0.06001488522782594,\n",
       "  '[0, 1]': 89.81393691490601,\n",
       "  '[0, -1]': -0.13311537356587877},\n",
       " '[4, 0]': {'[1, 0]': -1.9992663455322626,\n",
       "  '[-1, 0]': -1.9991053689512364,\n",
       "  '[0, 1]': -1.4308665118053967,\n",
       "  '[0, -1]': -1.9025614243664233},\n",
       " '[4, 1]': {'[1, 0]': -1.5458542278756568,\n",
       "  '[-1, 0]': -1.545213209779738,\n",
       "  '[0, 1]': 48.95409044766832,\n",
       "  '[0, -1]': 2.6125756116930816},\n",
       " '[4, 2]': {'[1, 0]': -0.482372848623454,\n",
       "  '[-1, 0]': 5.356976819656602,\n",
       "  '[0, 1]': 9.099999095970972,\n",
       "  '[0, -1]': -0.1790290284177729}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_sarsa(10000, 2, 0.5, 0.9, 'deterministic', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/10000 [00:00<02:59, 55.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [07:12<00:00, 23.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -1.9999999064780398,\n",
       "  '[-1, 0]': -1.9916717032210114,\n",
       "  '[0, 1]': -1.9999984996471798,\n",
       "  '[0, -1]': -1.9999966617626654},\n",
       " '[0, 2]': {'[1, 0]': -1.9896415080467664,\n",
       "  '[-1, 0]': -1.9981241652021624,\n",
       "  '[0, 1]': -1.9953929022944161,\n",
       "  '[0, -1]': -1.9998117495612322},\n",
       " '[0, 3]': {'[1, 0]': -1.9896402616665674,\n",
       "  '[-1, 0]': -1.9999944024229663,\n",
       "  '[0, 1]': -1.9999986294905152,\n",
       "  '[0, -1]': -1.9996990461336703},\n",
       " '[1, 0]': {'[1, 0]': -1.999989261812574,\n",
       "  '[-1, 0]': -1.9989050829933983,\n",
       "  '[0, 1]': -1.9999966760232106,\n",
       "  '[0, -1]': -1.9951467446826243},\n",
       " '[1, 1]': {'[1, 0]': -1.9999903517935316,\n",
       "  '[-1, 0]': -1.9981006138167339,\n",
       "  '[0, 1]': -1.97843297703386,\n",
       "  '[0, -1]': -1.9999436666039765},\n",
       " '[1, 2]': {'[1, 0]': -1.9999103990731075,\n",
       "  '[-1, 0]': -1.9999461288306093,\n",
       "  '[0, 1]': -1.9956046318812999,\n",
       "  '[0, -1]': -1.9990066755772213},\n",
       " '[1, 3]': {'[1, 0]': -1.9999649666200894,\n",
       "  '[-1, 0]': -1.9998944721981722,\n",
       "  '[0, 1]': 18.585287964650373,\n",
       "  '[0, -1]': -1.9996474613458959},\n",
       " '[2, 1]': {'[1, 0]': -1.827960579592029,\n",
       "  '[-1, 0]': -1.986157311182494,\n",
       "  '[0, 1]': -1.9951780842332687,\n",
       "  '[0, -1]': -1.9786586962761377},\n",
       " '[2, 2]': {'[1, 0]': -1.9999450903541316,\n",
       "  '[-1, 0]': -1.9999904953473806,\n",
       "  '[0, 1]': -1.991727771250274,\n",
       "  '[0, -1]': -1.9980520952742027},\n",
       " '[2, 3]': {'[1, 0]': -1.9999999179406598,\n",
       "  '[-1, 0]': -1.999947492527569,\n",
       "  '[0, 1]': -1.9999089951869884,\n",
       "  '[0, -1]': -1.9995510204722018},\n",
       " '[3, 1]': {'[1, 0]': -1.946870816814039,\n",
       "  '[-1, 0]': -1.9959355565045307,\n",
       "  '[0, 1]': -1.9724268207516602,\n",
       "  '[0, -1]': -1.9785643052093138},\n",
       " '[3, 3]': {'[1, 0]': 143.9399230082638,\n",
       "  '[-1, 0]': -1.0908423618817458,\n",
       "  '[0, 1]': -1.545469759303876,\n",
       "  '[0, -1]': -1.0907195206856914},\n",
       " '[4, 0]': {'[1, 0]': -1.9289383531831257,\n",
       "  '[-1, 0]': -1.9450315142721113,\n",
       "  '[0, 1]': -1.882344403713388,\n",
       "  '[0, -1]': -1.8845677018259088},\n",
       " '[4, 1]': {'[1, 0]': -1.9442904449397027,\n",
       "  '[-1, 0]': -1.8618453848751075,\n",
       "  '[0, 1]': 43.936495970537216,\n",
       "  '[0, -1]': -1.991971416277841},\n",
       " '[4, 2]': {'[1, 0]': -1.048787320426864,\n",
       "  '[-1, 0]': 11.558114947328747,\n",
       "  '[0, 1]': -0.5389281220923507,\n",
       "  '[0, -1]': 0.4131001311061109}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_sarsa(10000, 2, 0.5, 0.9, 'stochastic', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy n-step Sarsa for estimating $Q \\approx q_{*}$ or $q_{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonzero_probabilities(n,RandomSeed):\n",
    "    \n",
    "    Seed = RandomSeed\n",
    "    status = 'Not Done'\n",
    "    while status != 'Done':\n",
    "        random.seed(Seed)\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "\n",
    "        if min(scaled_numbers) > 0:\n",
    "\n",
    "            status = 'Done'\n",
    "        \n",
    "        else:\n",
    "            Seed = Seed + 1\n",
    "    \n",
    "    return scaled_numbers\n",
    "\n",
    "def state_action_probs_policy(RandomSeed):\n",
    "        \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "\n",
    "    state_action_probs = {}\n",
    "    \n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            state_action_probs[str(state)] = {}\n",
    "            state_probs = generate_nonzero_probabilities(4,RandomSeed + c)\n",
    "            \n",
    "            #prob = 0\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                state_action_probs[str(state)][action] = 0\n",
    "                    \n",
    "                #state_action_probs[str(state)][action] = state_probs[prob]\n",
    "                ##prob = prob + 1\n",
    "    \n",
    "        #c =  c + 1\n",
    "    \n",
    "    c = 0\n",
    "\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            neighbors = neighbor_cells(state)[0]\n",
    "            Actions_Neighbors = neighbor_cells(state)[1]\n",
    "\n",
    "            allowed_positions = []\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                \n",
    "                if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                    \n",
    "                    allowed_positions.append(neighbor)\n",
    "        \n",
    "            next_state = random.choice(allowed_positions)\n",
    "\n",
    "            row = next_state[0] - state[0]\n",
    "            col = next_state[1] - state[1]\n",
    "            PolicyAction = [row, col]\n",
    "\n",
    "            policy['{}'.format(state)] = next_state\n",
    "            policy_action['{}'.format(state)] = PolicyAction\n",
    "    \n",
    "            state_probs = generate_nonzero_probabilities(4,RandomSeed + c)\n",
    "\n",
    "            state_action_probs[str(state)][str(PolicyAction)] = max(state_probs)\n",
    "\n",
    "            state_probs.remove(max(state_probs))\n",
    "\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "            Actions.remove(PolicyAction)\n",
    "\n",
    "            state_action_probs[str(state)][str(Actions[0])] = state_probs[0]\n",
    "            state_action_probs[str(state)][str(Actions[1])] = state_probs[1]\n",
    "            state_action_probs[str(state)][str(Actions[2])] = state_probs[2]\n",
    "\n",
    "            c = c + 1\n",
    "\n",
    "    return policy, policy_action,state_action_probs\n",
    "\n",
    "def off_policy_n_step_sarsa(num_trials, n,  policy_b, gamma, alpha, environment_stochasticity,epsilon):\n",
    "    \n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "\n",
    "    \n",
    "\n",
    "    policy_b_state = policy_b[0]\n",
    "    policy_b_action = policy_b[1]\n",
    "    policy_b_probs = policy_b[2]\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "\n",
    "    \n",
    "    def greedy_policy_pi(current_state):\n",
    "\n",
    "        if type(current_state) == str:\n",
    "\n",
    "            state = ast.literal_eval(current_state)\n",
    "        else:\n",
    "            state = current_state\n",
    "        #Choose action using policy derived from Q===================================\n",
    "        value_action_state = reverse_dictionary(Q[str(state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "\n",
    "        #============================================================================\n",
    "        #Epsilon Greedy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "            selected_action = best_action\n",
    "            selection_prob = 1 - epsilon\n",
    "        \n",
    "        else:\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "            Actions.remove(best_action)\n",
    "            epsilon_action = random.choice(Actions)\n",
    "\n",
    "            selected_action = epsilon_action\n",
    "            selection_prob = epsilon\n",
    "        #============================================================================\n",
    "        \n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        Actions.remove(selected_action)\n",
    "        sorted_actions = Actions + [selected_action]\n",
    "        state_indice = state_indice_dict[str(state)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "        #due to stochasticity of the environment\n",
    "        Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        #print(type(state), type(Final_action))\n",
    "        \n",
    "        next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "        if next_state not in environment[6] or next_state in environment[4]:\n",
    "\n",
    "            next_state = current_state\n",
    "\n",
    "        return Final_action, next_state, selection_prob\n",
    "\n",
    "\n",
    "    state_t  = environment[2]\n",
    "\n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        trajectory_path = generate_trajectory(policy_b,randomness,environment_stochasticity)[0]\n",
    "        \n",
    "        T = float('inf')\n",
    "        tau = 0\n",
    "        t = -1\n",
    "        trajectory = []\n",
    "        state_t  = environment[2]\n",
    "        while tau != T - 1:\n",
    "\n",
    "            t = t + 1\n",
    "\n",
    "            if t < T:\n",
    "                trajectory.append(state_t)\n",
    "                next_state = policy_b_state[str(state_t)]\n",
    "                #last_state = state_t\n",
    "                state_t = next_state\n",
    "\n",
    "                print(next_state)\n",
    "                \n",
    "\n",
    "                if next_state == environment[3]:\n",
    "                    T = t + 1\n",
    "                    \n",
    "                \n",
    "                #else:\n",
    "                #    next_action = \n",
    "\n",
    "            \n",
    "            tau = t - n + 1\n",
    "\n",
    "            if tau >= 0:\n",
    "\n",
    "                G = 0\n",
    "                \n",
    "                state_tau = trajectory[tau]\n",
    "                state_i = state_tau\n",
    "                action_tau = policy_b_action[str(state_tau)]\n",
    "\n",
    "                action_ii = 0\n",
    "\n",
    "                ratio = 1\n",
    "\n",
    "                for i in range(tau+1, min(tau+n-1,T-1)+1):\n",
    "\n",
    "                    r_state_ii = policy_b_state[str(state_i)]\n",
    "                    r_action_ii = policy_b_action[str(state_i)]\n",
    "\n",
    "                    greedy_pi = greedy_policy_pi(state_i)\n",
    "\n",
    "                    if r_action_ii == greedy_pi[0]:\n",
    "\n",
    "                        pi = greedy_pi[2]\n",
    "                    else:\n",
    "                        pi = 1 - greedy_pi[2]\n",
    "                    \n",
    "                    b_prob = policy_b_probs[str(r_state_ii)][str(r_action_ii)]\n",
    "\n",
    "                    ratio = ratio * (pi/b_prob)\n",
    "\n",
    "                    state_i = r_state_ii\n",
    "                \n",
    "                \n",
    "\n",
    "                #===============\n",
    "\n",
    "                state_i = state_tau\n",
    "                action_tau = policy_b_action[str(state_tau)]\n",
    "\n",
    "                action_ii = 0\n",
    "\n",
    "                for i in range(tau+1, min(tau+n,T)+1):\n",
    "\n",
    "                    #actionTN = action_ii\n",
    "\n",
    "                    state_ii = policy_b_state[str(state_i)]\n",
    "                    action_ii = policy_b_action[str(state_i)]\n",
    "\n",
    "                    #stateTN = state_i #s_(t+n) this is store for the next part\n",
    "                     \n",
    "\n",
    "                    #n_state_action.append((state_ii, action_ii))\n",
    "\n",
    "                    #state_i = state_ii\n",
    "                    r = state_reward(state_ii)\n",
    "\n",
    "                    G = G + (gamma ** (i-tau-1)) * r\n",
    "\n",
    "                    if i < min(tau+n,T):\n",
    "\n",
    "                        state_i = state_ii\n",
    "\n",
    "                if tau + n < T:\n",
    "\n",
    "                    stateTN = policy_b_state[str(state_ii)]\n",
    "                    actionTN = policy_b_action[str(state_ii)]\n",
    "\n",
    "\n",
    "                    #print(tau , n)\n",
    "                    \n",
    "                    #tau_n_state = trajectory[tau + n] #indice_state_dict[tau + n]\n",
    "\n",
    "                    #if state_ii not in environment[4] and state_ii != environment[3]:\n",
    "\n",
    "                    G = G + (gamma ** n) * Q[str(stateTN)][str(actionTN)]\n",
    "\n",
    "                if state_tau not in environment[4] and state_tau != environment[3]:\n",
    "                    #print(type(tau_state))\n",
    "\n",
    "\n",
    "                    Q[str(state_tau)][str(action_tau)] = Q[str(state_tau)][str(action_tau)] + alpha * ratio * (G - Q[str(state_tau)][str(action_tau)])\n",
    "    \n",
    "    del Q[str(environment[3])]\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonzero_probabilities(n,RandomSeed):\n",
    "    \n",
    "    Seed = RandomSeed\n",
    "    status = 'Not Done'\n",
    "    while status != 'Done':\n",
    "        random.seed(Seed)\n",
    "        numbers = [random.random() for _ in range(n)]\n",
    "        total_sum = sum(numbers)\n",
    "        scaled_numbers = [num / total_sum for num in numbers]\n",
    "\n",
    "        if min(scaled_numbers) > 0:\n",
    "\n",
    "            status = 'Done'\n",
    "        \n",
    "        else:\n",
    "            Seed = Seed + 1\n",
    "    \n",
    "    return scaled_numbers\n",
    "\n",
    "def state_action_probs_policy(RandomSeed):\n",
    "        \n",
    "    policy = {}\n",
    "    policy_action = {}\n",
    "\n",
    "    state_action_probs = {}\n",
    "    \n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            state_action_probs[str(state)] = {}\n",
    "            #state_probs = generate_nonzero_probabilities(4,RandomSeed + c)\n",
    "            \n",
    "            #prob = 0\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                state_action_probs[str(state)][action] = 0\n",
    "                    \n",
    "                #state_action_probs[str(state)][action] = state_probs[prob]\n",
    "                ##prob = prob + 1\n",
    "    \n",
    "        #c =  c + 1\n",
    "    \n",
    "    c = 0\n",
    "\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "\n",
    "            neighbors = neighbor_cells(state)[0]\n",
    "            Actions_Neighbors = neighbor_cells(state)[1]\n",
    "\n",
    "            allowed_positions = []\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                \n",
    "                if neighbor in environment[6] and neighbor not in environment[4]:\n",
    "                    \n",
    "                    allowed_positions.append(neighbor)\n",
    "        \n",
    "            next_state = random.choice(allowed_positions)\n",
    "\n",
    "            row = next_state[0] - state[0]\n",
    "            col = next_state[1] - state[1]\n",
    "            PolicyAction = [row, col]\n",
    "\n",
    "            policy['{}'.format(state)] = next_state\n",
    "            policy_action['{}'.format(state)] = PolicyAction\n",
    "    \n",
    "            state_probs = generate_nonzero_probabilities(4,RandomSeed + c)\n",
    "\n",
    "            state_action_probs[str(state)][str(PolicyAction)] = max(state_probs)\n",
    "\n",
    "            state_probs.remove(max(state_probs))\n",
    "\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "            Actions.remove(PolicyAction)\n",
    "\n",
    "            state_action_probs[str(state)][str(Actions[0])] = state_probs[0]\n",
    "            state_action_probs[str(state)][str(Actions[1])] = state_probs[1]\n",
    "            state_action_probs[str(state)][str(Actions[2])] = state_probs[2]\n",
    "\n",
    "            c = c + 1\n",
    "\n",
    "    return policy, policy_action,state_action_probs\n",
    "\n",
    "def off_policy_n_step_sarsa(num_trials, n,  policy_b, gamma, alpha, environment_stochasticity,epsilon):\n",
    "    \n",
    "    grid_size = environment[0]*environment[1]\n",
    "\n",
    "    probs = probability_distribution(grid_size,environment_stochasticity)\n",
    "\n",
    "    \n",
    "\n",
    "    policy_b_state = policy_b[0]\n",
    "    policy_b_action = policy_b[1]\n",
    "    policy_b_probs = policy_b[2]\n",
    "\n",
    "    Q = {}\n",
    "    for state in environment[6]:\n",
    "\n",
    "        if state not in environment[4]:\n",
    "            \n",
    "            Q[str(state)] = {}\n",
    "\n",
    "            for action in [\"[1, 0]\",\"[-1, 0]\",\"[0, 1]\",\"[0, -1]\"]:\n",
    "\n",
    "                Q[str(state)][action] = random.uniform(1e-9, 1e-8)\n",
    "\n",
    "    \n",
    "    def greedy_policy_pi(current_state):\n",
    "\n",
    "        if type(current_state) == str:\n",
    "\n",
    "            state = ast.literal_eval(current_state)\n",
    "        else:\n",
    "            state = current_state\n",
    "        #Choose action using policy derived from Q===================================\n",
    "        value_action_state = reverse_dictionary(Q[str(state)])\n",
    "        Max_val = max(list(value_action_state.keys()))\n",
    "        best_action = value_action_state[Max_val]\n",
    "        best_action = ast.literal_eval(best_action)\n",
    "\n",
    "        #============================================================================\n",
    "        #Epsilon Greedy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "\n",
    "            selected_action = best_action\n",
    "            selection_prob = 1 - epsilon\n",
    "        \n",
    "        else:\n",
    "            Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "            Actions.remove(best_action)\n",
    "            epsilon_action = random.choice(Actions)\n",
    "\n",
    "            selected_action = epsilon_action\n",
    "            selection_prob = epsilon\n",
    "        #============================================================================\n",
    "        \n",
    "        Actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        Actions.remove(selected_action)\n",
    "        sorted_actions = Actions + [selected_action]\n",
    "        state_indice = state_indice_dict[str(state)]\n",
    "        actions_prob = probs[state_indice]\n",
    "        actions_prob.sort()\n",
    "        #due to stochasticity of the environment\n",
    "        Final_action = random.choices(sorted_actions, actions_prob)[0]\n",
    "        #print(type(state), type(Final_action))\n",
    "        \n",
    "        next_state = [x + y for x, y in zip(state, Final_action)]\n",
    "\n",
    "        if next_state not in environment[6] or next_state in environment[4]:\n",
    "\n",
    "            next_state = current_state\n",
    "\n",
    "        return Final_action, next_state, selection_prob\n",
    "\n",
    "\n",
    "    state_t  = environment[2]\n",
    "    \n",
    "    for trial in tqdm(range(num_trials)):\n",
    "\n",
    "        trajectory_path = generate_trajectory(policy_b,trial,environment_stochasticity)[0]\n",
    "        \n",
    "        T = float('inf')\n",
    "        tau = 0\n",
    "        t = -1\n",
    "        trajectory = []\n",
    "        state_t  = environment[2]\n",
    "        while tau != T - 1:\n",
    "\n",
    "            t = t + 1\n",
    "\n",
    "            if t < T:\n",
    "                trajectory.append(state_t)\n",
    "                next_state = trajectory_path[t][0]\n",
    "                #last_state = state_t\n",
    "                state_t = next_state\n",
    "\n",
    "                #print(next_state)\n",
    "                \n",
    "\n",
    "                if next_state == environment[3]:\n",
    "                    T = t + 1\n",
    "\n",
    "\n",
    "            \n",
    "            tau = t - n + 1\n",
    "\n",
    "            if tau >= 0:\n",
    "\n",
    "                G = 0\n",
    "                \n",
    "                state_tau = trajectory_path[tau][0]\n",
    "                state_i = state_tau\n",
    "                action_tau =  trajectory_path[tau][1]\n",
    "\n",
    "                action_ii = 0\n",
    "\n",
    "                ratio = 1\n",
    "\n",
    "                for i in range(tau+1, min(tau+n-1,T-1)):\n",
    "\n",
    "                    r_state_ii = trajectory_path[i][0]\n",
    "                    r_action_ii = trajectory_path[i][1]\n",
    "\n",
    "                    greedy_pi = greedy_policy_pi(trajectory_path[i-1][0])\n",
    "\n",
    "                    if r_action_ii == greedy_pi[0]:\n",
    "\n",
    "                        pi = greedy_pi[2]\n",
    "                    else:\n",
    "                        pi = 1 - greedy_pi[2]\n",
    "                    \n",
    "                    b_prob = policy_b_probs[str(r_state_ii)][str(r_action_ii)]\n",
    "\n",
    "                    ratio = ratio * (pi/b_prob)\n",
    "\n",
    "                    state_i = r_state_ii\n",
    "                \n",
    "                \n",
    "\n",
    "                #===============\n",
    "\n",
    "                state_i = state_tau\n",
    "                action_tau = trajectory_path[tau][1]\n",
    "\n",
    "                action_ii = 0\n",
    "\n",
    "                for i in range(tau+1, min(tau+n,T)):\n",
    "\n",
    "                    #actionTN = action_ii\n",
    "                    r_state_ii = trajectory_path[i][0]\n",
    "                    r_action_ii = trajectory_path[i][1]\n",
    "\n",
    "                    #stateTN = state_i #s_(t+n) this is store for the next part\n",
    "                     \n",
    "\n",
    "                    #n_state_action.append((state_ii, action_ii))\n",
    "\n",
    "                    #state_i = state_ii\n",
    "                    r = state_reward(r_state_ii)\n",
    "\n",
    "                    G = G + (gamma ** (i-tau-1)) * r\n",
    "\n",
    "                    if i < min(tau+n,T):\n",
    "\n",
    "                        state_i = r_state_ii\n",
    "\n",
    "                if tau + n < T:\n",
    "\n",
    "                    stateTN = trajectory_path[tau + n - 1][0]\n",
    "                    actionTN = trajectory_path[tau + n - 1][1]\n",
    "\n",
    "\n",
    "                    #print(tau , n)\n",
    "                    \n",
    "                    #tau_n_state = trajectory[tau + n] #indice_state_dict[tau + n]\n",
    "\n",
    "                    #if state_ii not in environment[4] and state_ii != environment[3]:\n",
    "\n",
    "                    G = G + (gamma ** n) * Q[str(stateTN)][str(actionTN)]\n",
    "\n",
    "                if state_tau not in environment[4] and state_tau != environment[3]:\n",
    "                    #print(type(tau_state))\n",
    "\n",
    "\n",
    "                    Q[str(state_tau)][str(action_tau)] = Q[str(state_tau)][str(action_tau)] + alpha * ratio * (G - Q[str(state_tau)][str(action_tau)])\n",
    "    \n",
    "    del Q[str(environment[3])]\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:04<00:00, 154.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -1.3281533443377147,\n",
       "  '[-1, 0]': -1.33332548919037,\n",
       "  '[0, 1]': -1.3333333308372837,\n",
       "  '[0, -1]': -1.3333333292649956},\n",
       " '[0, 2]': {'[1, 0]': -1.3333258121369658,\n",
       "  '[-1, 0]': -1.3333332933563438,\n",
       "  '[0, 1]': -1.3333333290985392,\n",
       "  '[0, -1]': -1.329834838676721},\n",
       " '[0, 3]': {'[1, 0]': -1.3324189631535748,\n",
       "  '[-1, 0]': -1.3333322862730463,\n",
       "  '[0, 1]': -1.333333332802156,\n",
       "  '[0, -1]': -1.3333333321259186},\n",
       " '[1, 0]': {'[1, 0]': -1.199999992476919,\n",
       "  '[-1, 0]': -1.199999955947541,\n",
       "  '[0, 1]': -1.3332622203766529,\n",
       "  '[0, -1]': -1.19889083959478},\n",
       " '[1, 1]': {'[1, 0]': -1.0704424625202302,\n",
       "  '[-1, 0]': -1.3333327817508671,\n",
       "  '[0, 1]': -1.3333322859254186,\n",
       "  '[0, -1]': -1.3333294334491763},\n",
       " '[1, 2]': {'[1, 0]': -1.246330414377891,\n",
       "  '[-1, 0]': -1.333333331383521,\n",
       "  '[0, 1]': -1.3285474499476326,\n",
       "  '[0, -1]': -1.3270157708485104},\n",
       " '[1, 3]': {'[1, 0]': -1.327755960018347,\n",
       "  '[-1, 0]': -1.3199999997155047,\n",
       "  '[0, 1]': -1.3290830175898678,\n",
       "  '[0, -1]': -1.3333333211414486},\n",
       " '[2, 1]': {'[1, 0]': -0.974793398236913,\n",
       "  '[-1, 0]': -1.3244851947385055,\n",
       "  '[0, 1]': -1.3178938236341171,\n",
       "  '[0, -1]': -1.333333317415266},\n",
       " '[2, 2]': {'[1, 0]': -1.3333099834376154,\n",
       "  '[-1, 0]': -1.3333294914782963,\n",
       "  '[0, 1]': -1.3326636676081387,\n",
       "  '[0, -1]': -1.250068055040547},\n",
       " '[2, 3]': {'[1, 0]': -1.3287597073982478,\n",
       "  '[-1, 0]': -1.332855738323574,\n",
       "  '[0, 1]': -1.3131147181692728,\n",
       "  '[0, -1]': -1.2659591405598376},\n",
       " '[3, 1]': {'[1, 0]': -1.2255885851293846,\n",
       "  '[-1, 0]': -1.2970582283206629,\n",
       "  '[0, 1]': -1.3331307744847756,\n",
       "  '[0, -1]': -1.3331238788306063},\n",
       " '[3, 3]': {'[1, 0]': 6.424108227121229e-09,\n",
       "  '[-1, 0]': -1.3135656298879688,\n",
       "  '[0, 1]': 8.359508171067833e-09,\n",
       "  '[0, -1]': 9.017450583798791e-09},\n",
       " '[4, 0]': {'[1, 0]': -1.331999898008628,\n",
       "  '[-1, 0]': -1.3333331166188551,\n",
       "  '[0, 1]': -1.3329714450341026,\n",
       "  '[0, -1]': -1.3158533412124842},\n",
       " '[4, 1]': {'[1, 0]': -1.3333255544792442,\n",
       "  '[-1, 0]': 0.0662243662699679,\n",
       "  '[0, 1]': 21.466668397236397,\n",
       "  '[0, -1]': -1.3332055176458628},\n",
       " '[4, 2]': {'[1, 0]': -1.3333333331858868,\n",
       "  '[-1, 0]': -1.3333326390187192,\n",
       "  '[0, 1]': 100.0,\n",
       "  '[0, -1]': -1.3333226318150273}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_b_0 = state_action_probs_policy(40)\n",
    "\n",
    "#since policy pi must be greedy, we set epsilon = 0\n",
    "off_policy_n_step_sarsa(10000, 2,  policy_b_0, 0.5, 0.9, 'deterministic', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:10<00:00, 142.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[0, 1]': {'[1, 0]': -1.3333332503393205,\n",
       "  '[-1, 0]': -1.333333267555271,\n",
       "  '[0, 1]': -1.3333330298719224,\n",
       "  '[0, -1]': -1.333333332477152},\n",
       " '[0, 2]': {'[1, 0]': -1.3333298757610192,\n",
       "  '[-1, 0]': -1.3333332295694422,\n",
       "  '[0, 1]': -1.3333285657425622,\n",
       "  '[0, -1]': -1.3333332383765757},\n",
       " '[0, 3]': {'[1, 0]': -1.3333140089654512,\n",
       "  '[-1, 0]': -1.333331010928405,\n",
       "  '[0, 1]': -1.3333328911411866,\n",
       "  '[0, -1]': -1.333332344330075},\n",
       " '[1, 0]': {'[1, 0]': -1.333327799204876,\n",
       "  '[-1, 0]': -1.3333333217659638,\n",
       "  '[0, 1]': -1.3333332927319261,\n",
       "  '[0, -1]': -1.3333332601525851},\n",
       " '[1, 1]': {'[1, 0]': -1.3328137616809135,\n",
       "  '[-1, 0]': -1.3333333298228511,\n",
       "  '[0, 1]': -1.3333331285843144,\n",
       "  '[0, -1]': -1.333333324635706},\n",
       " '[1, 2]': {'[1, 0]': -1.3333228256153098,\n",
       "  '[-1, 0]': -1.3333332129052402,\n",
       "  '[0, 1]': -1.3333315567297475,\n",
       "  '[0, -1]': -1.3333331496961094},\n",
       " '[1, 3]': {'[1, 0]': -1.3331658074257022,\n",
       "  '[-1, 0]': -1.3333332994742595,\n",
       "  '[0, 1]': -1.333333325252904,\n",
       "  '[0, -1]': -1.3333332944037952},\n",
       " '[2, 1]': {'[1, 0]': -1.3215585779786567,\n",
       "  '[-1, 0]': -1.3333292684245692,\n",
       "  '[0, 1]': -1.333332970800499,\n",
       "  '[0, -1]': -1.3331336221681773},\n",
       " '[2, 2]': {'[1, 0]': -1.33332766512005,\n",
       "  '[-1, 0]': -1.3333293374062478,\n",
       "  '[0, 1]': -1.3305430576649373,\n",
       "  '[0, -1]': -1.3332255018454753},\n",
       " '[2, 3]': {'[1, 0]': -1.333330426720805,\n",
       "  '[-1, 0]': -1.3333333313904803,\n",
       "  '[0, 1]': -1.33333332103813,\n",
       "  '[0, -1]': -1.3332969808461852},\n",
       " '[3, 1]': {'[1, 0]': 4.314854959740664,\n",
       "  '[-1, 0]': -1.3333008238815833,\n",
       "  '[0, 1]': -1.3333325490781756,\n",
       "  '[0, -1]': -1.333333319665136},\n",
       " '[3, 3]': {'[1, 0]': 100.0,\n",
       "  '[-1, 0]': -1.333325598630132,\n",
       "  '[0, 1]': -1.3332958266177715,\n",
       "  '[0, -1]': -1.3332468584292523},\n",
       " '[4, 0]': {'[1, 0]': -1.3332378147076174,\n",
       "  '[-1, 0]': -1.3333165375813916,\n",
       "  '[0, 1]': -1.3333275236707351,\n",
       "  '[0, -1]': -1.3332955656882473},\n",
       " '[4, 1]': {'[1, 0]': -1.333147064593945,\n",
       "  '[-1, 0]': -1.3333326808790757,\n",
       "  '[0, 1]': 23.974692303920683,\n",
       "  '[0, -1]': -1.3333220690977794},\n",
       " '[4, 2]': {'[1, 0]': -1.3333285631773246,\n",
       "  '[-1, 0]': -1.3333067633061424,\n",
       "  '[0, 1]': 100.0,\n",
       "  '[0, -1]': -1.3330323266205548}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_policy_n_step_sarsa(10000, 2,  policy_b_0, 0.5, 0.9, 'stochastic', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
